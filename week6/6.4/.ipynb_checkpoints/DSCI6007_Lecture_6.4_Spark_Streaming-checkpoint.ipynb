{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('http://asimjalis.github.io/ipyn-ext/js/ipyn-present.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('http://asimjalis.github.io/ipyn-ext/js/ipyn-present.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "This file was auto-generated from markdown using notedown.\n",
    "Instead of modifying the ipynb modify the markdown source. \n",
    "-->\n",
    "\n",
    "<h1 class=\"tocheading\">Spark Streaming</h1>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "Spark Streaming\n",
    "===============\n",
    "\n",
    "Why Spark Streaming\n",
    "-------------------\n",
    "\n",
    "Q: What problem does Spark Streaming solve?\n",
    "\n",
    "- Spark like MapReduce is designed to process data as a batch job.\n",
    "\n",
    "- Nightly batch jobs process large amounts of data and generate insights.\n",
    "\n",
    "- What if we want to react immediately instead of wait 24 hours.\n",
    "\n",
    "- Spark Streaming solves this problem.\n",
    "\n",
    "- It lets you process data immediately in near-realtime.\n",
    "\n",
    "Spark Streaming Applications\n",
    "----------------------------\n",
    "\n",
    "Q: What is an example scenario?\n",
    "\n",
    "- Suppose you have an intrusion detection system.\n",
    "\n",
    "- You process log files to determine if the system is under attack.\n",
    "\n",
    "- Batch processing will take 24 hours to raise an intrusion alert.\n",
    "\n",
    "- Spark Streaming can detect an intrustion in minutes or seconds.\n",
    "\n",
    "Micro-Batch Concept\n",
    "-------------------\n",
    "\n",
    "Q: How does Spark Streaming work?\n",
    "\n",
    "- Events are grouped into micro batched RDDs.\n",
    "\n",
    "- Each RDD contains events from the last few seconds.\n",
    "\n",
    "- Incoming event stream is turned into RDD stream.\n",
    "\n",
    "- These micro batched RDDs are joined with existing data to raise alerts.\n",
    "\n",
    "Spark Streaming RDDs\n",
    "--------------------\n",
    "\n",
    "Q: How does Spark Streaming integrate with Spark?\n",
    "\n",
    "- Spark Streaming converts incoming events into micro batched RDDs.\n",
    "\n",
    "- These are then processed by the regular Spark APIs.\n",
    "\n",
    "<img src=\"images/streaming-arch.png\">\n",
    "\n",
    "<img src=\"images/streaming-flow-micro-batches.png\">\n",
    "\n",
    "Spark Stack\n",
    "-----------\n",
    "\n",
    "Q: How does Spark Streaming fit into the rest of Spark?\n",
    "\n",
    "- Spark Streaming is a subsystem of Spark.\n",
    "\n",
    "- Spark Streaming enables handling realtime events.\n",
    "\n",
    "<img src=\"images/spark-stack.png\">\n",
    "\n",
    "Spark Streaming Big Picture\n",
    "---------------------------\n",
    "\n",
    "- Spark Streaming can consume events from multiple sources.\n",
    "\n",
    "- These are processed and written out to HDFS, databases, and other\n",
    "  systems.\n",
    "\n",
    "<img src=\"images/streaming-input-output-components.png\">\n",
    "\n",
    "\n",
    "DStream Concept\n",
    "---------------\n",
    "\n",
    "- A DStream is a stream of RDDs.\n",
    "\n",
    "- Think of a DStream as an infinite sequence of RDDs.\n",
    "\n",
    "<img src=\"images/streaming-dstream-as-rdds.png\">\n",
    "\n",
    "- The incoming events are batched together into RDDs.\n",
    "\n",
    "<img src=\"images/streaming-dstream-time-i.png\">\n",
    "\n",
    "\n",
    "Spark Streaming vs Storm\n",
    "------------------------\n",
    "\n",
    "Q: How does Spark Streaming compare with Storm?\n",
    "\n",
    "- Storm is another system for realtime processing of events.\n",
    "\n",
    "- Here is a comparison of Storm and Spark Streaming.\n",
    "\n",
    "Comparison           |Winner     |Spark Streaming      |Storm\n",
    "----------           |------     |---------------      |-----\n",
    "Processing Model     |  -        |Mini batches         |Record-at-a-time\n",
    "Latency              |Storm      |Few seconds          |Sub-second\n",
    "Fault tolerance      |Spark      |Exactly once         |At least once (may be duplicates)\n",
    "Batch integration    |Spark      |Spark                |Requires different framework\n",
    "API                  |Spark      |Simpler              |Complex\n",
    "Production use       |Storm      |2013                 |2011\n",
    "\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: What happens to an event that is half in batch `time=1` and half in\n",
    "batch `time=2`? Which batch does it go to?\n",
    "</summary>\n",
    "1. It goes to batch `time=2`.<br>\n",
    "2. Incomplete events are meaningless.<br>\n",
    "3. RDDs are formed from fully-formed events.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Streaming Code\n",
    "--------------------\n",
    "\n",
    "Q: How can I write a Spark Streaming app?\n",
    "\n",
    "- Here is an example of a Spark Streaming app.\n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create StreamingContext with 2 threads, and batch interval of 1 second\n",
    "sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Create DStream to listen to hostname:port\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Count words in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print first 10 elements of each RDD in DStream \n",
    "wordCounts.pprint()\n",
    "\n",
    "# Start computation\n",
    "ssc.start()\n",
    "\n",
    "# Wait streaming to terminate\n",
    "ssc.awaitTermination()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-fc3a2577df05>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-fc3a2577df05>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    $SPARK_HOME/bin/spark-submit \"/Users/Alexander/DSCI6007-student/week6/6.4/network_wordcount2 localhost 9999\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "$SPARK_HOME/bin/spark-submit /Users/Alexander/DSCI6007-student/week6/6.4/network_wordcount2.py localhost 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage: network_wordcount.py <hostname> <port>\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o17.awaitTermination.\n: java.net.ConnectException: Connection refused\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:579)\n\tat java.net.Socket.connect(Socket.java:528)\n\tat java.net.Socket.<init>(Socket.java:425)\n\tat java.net.Socket.<init>(Socket.java:241)\n\tat py4j.CallbackConnection.start(CallbackConnection.java:104)\n\tat py4j.CallbackClient.getConnection(CallbackClient.java:134)\n\tat py4j.CallbackClient.getConnectionLock(CallbackClient.java:146)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:229)\n\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:111)\n\tat com.sun.proxy.$Proxy14.call(Unknown Source)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:63)\n\tat org.apache.spark.streaming.api.python.PythonTransformedDStream.compute(PythonDStream.scala:197)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:350)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:350)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:349)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:349)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:344)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:342)\n\tat scala.Option.orElse(Option.scala:257)\n\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:339)\n\tat org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:38)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:120)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:120)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)\n\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)\n\tat org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:120)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$2.apply(JobGenerator.scala:243)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$2.apply(JobGenerator.scala:241)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:241)\n\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:177)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:83)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:82)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ad39e183846d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/pyspark/streaming/context.pyc\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \"\"\"\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 813\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    307\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o17.awaitTermination.\n: java.net.ConnectException: Connection refused\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:579)\n\tat java.net.Socket.connect(Socket.java:528)\n\tat java.net.Socket.<init>(Socket.java:425)\n\tat java.net.Socket.<init>(Socket.java:241)\n\tat py4j.CallbackConnection.start(CallbackConnection.java:104)\n\tat py4j.CallbackClient.getConnection(CallbackClient.java:134)\n\tat py4j.CallbackClient.getConnectionLock(CallbackClient.java:146)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:229)\n\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:111)\n\tat com.sun.proxy.$Proxy14.call(Unknown Source)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:63)\n\tat org.apache.spark.streaming.api.python.PythonTransformedDStream.compute(PythonDStream.scala:197)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:350)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:350)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:349)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:349)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:344)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:342)\n\tat scala.Option.orElse(Option.scala:257)\n\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:339)\n\tat org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:38)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:120)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:120)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)\n\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)\n\tat org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:120)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$2.apply(JobGenerator.scala:243)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$2.apply(JobGenerator.scala:241)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:241)\n\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:177)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:83)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:82)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n"
     ]
    }
   ],
   "source": [
    "# %load \"/Users/Alexander/DSCI6007-student/week6/6.4/network_wordcount2.py\"\n",
    "from __future__ import print_function\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: network_wordcount.py <hostname> <port>\", file=sys.stderr)\n",
    "        exit(-1)\n",
    "    sc = SparkContext(appName=\"PythonStreamingNetworkWordCount\")\n",
    "    ssc = StreamingContext(sc, 1)\n",
    "\n",
    "    lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "    counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "                  .map(lambda word: (word, 1))\\\n",
    "                  .reduceByKey(lambda a, b: a+b)\n",
    "    counts.pprint()\n",
    "\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o18.awaitTermination.\n: java.net.ConnectException: Connection refused\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:579)\n\tat java.net.Socket.connect(Socket.java:528)\n\tat java.net.Socket.<init>(Socket.java:425)\n\tat java.net.Socket.<init>(Socket.java:241)\n\tat py4j.CallbackConnection.start(CallbackConnection.java:104)\n\tat py4j.CallbackClient.getConnection(CallbackClient.java:134)\n\tat py4j.CallbackClient.getConnectionLock(CallbackClient.java:146)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:229)\n\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:111)\n\tat com.sun.proxy.$Proxy14.call(Unknown Source)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:63)\n\tat org.apache.spark.streaming.api.python.PythonTransformedDStream.compute(PythonDStream.scala:197)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:350)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:350)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:349)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:349)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:344)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:342)\n\tat scala.Option.orElse(Option.scala:257)\n\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:339)\n\tat org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:38)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:120)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:120)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)\n\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)\n\tat org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:120)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$2.apply(JobGenerator.scala:243)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$2.apply(JobGenerator.scala:241)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:241)\n\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:177)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:83)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:82)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8c4fa959e5f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Wait streaming to terminate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/pyspark/streaming/context.pyc\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \"\"\"\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 813\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    307\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o18.awaitTermination.\n: java.net.ConnectException: Connection refused\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:579)\n\tat java.net.Socket.connect(Socket.java:528)\n\tat java.net.Socket.<init>(Socket.java:425)\n\tat java.net.Socket.<init>(Socket.java:241)\n\tat py4j.CallbackConnection.start(CallbackConnection.java:104)\n\tat py4j.CallbackClient.getConnection(CallbackClient.java:134)\n\tat py4j.CallbackClient.getConnectionLock(CallbackClient.java:146)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:229)\n\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:111)\n\tat com.sun.proxy.$Proxy14.call(Unknown Source)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:63)\n\tat org.apache.spark.streaming.api.python.PythonTransformedDStream.compute(PythonDStream.scala:197)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:350)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:350)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:349)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:349)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:344)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:342)\n\tat scala.Option.orElse(Option.scala:257)\n\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:339)\n\tat org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:38)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:120)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:120)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)\n\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)\n\tat org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:120)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$2.apply(JobGenerator.scala:243)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$2.apply(JobGenerator.scala:241)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:241)\n\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:177)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:83)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:82)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create StreamingContext with 2 threads, and batch interval of 1 second\n",
    "sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Create DStream to listen to hostname:port\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Count words in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print first 10 elements of each RDD in DStream \n",
    "wordCounts.pprint()\n",
    "\n",
    "# Start computation\n",
    "ssc.start()\n",
    "\n",
    "# Wait streaming to terminate\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network Source\n",
    "--------------\n",
    "\n",
    "Q: How can I create data to feed into this stream?\n",
    "\n",
    "- You need to write to the network socket 9999 on localhost for this\n",
    "  streaming app to pick up the events.\n",
    "\n",
    "- Here is some shell code to do this.\n",
    "\n",
    "```sh\n",
    "nc -lk 9999\n",
    "```\n",
    "\n",
    "- You can type words into this or write a script that pipes data into\n",
    "  this periodically.\n",
    "\n",
    "\n",
    "Notes\n",
    "-----\n",
    "\n",
    "- The `StreamingContext` is stored in `ssc`.\n",
    "\n",
    "- `ssc.socketTextStream` creates a `DStream`.\n",
    "\n",
    "- DStreams transformations like `flatMap`, `map`, `reduceByKey` \n",
    "  create new DStreams.\n",
    "\n",
    "- DStreams output operations like `pprint` are like RDD actions.\n",
    "\n",
    "- Except DStream output operations do not cause execution.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: When you execute `pprint` on a DStream will anything be printed?\n",
    "</summary>\n",
    "1. Nothing is printed.<br>\n",
    "2. The printing happens when we call `ssc.start()` and when data flows in.\n",
    "</details>\n",
    "\n",
    "RDDs vs DStreams\n",
    "----------------\n",
    "\n",
    "Q: How are DStream different from RDDs?\n",
    "\n",
    "- DStream transformations and output operations define an assembly line.\n",
    "  \n",
    "- Nothing happens until data comes in.\n",
    "\n",
    "- When data comes in DStream output operations trigger the execution\n",
    "  of DStream transformations.\n",
    "\n",
    "<img src=\"images/donuts.jpg\">\n",
    "\n",
    "Transformations and Output Operations\n",
    "=====================================\n",
    "\n",
    "DStream Transformations\n",
    "-----------------------\n",
    "\n",
    "Q: How are DStream transformations different from RDD transformations?\n",
    "\n",
    "- DStream transformations define what will happen to RDDs when they\n",
    "  arrive.\n",
    "  \n",
    "- DStream transformations produce new DStreams that will contain \n",
    "  transformed RDDs.\n",
    "\n",
    "- Nothing happens until data arrives.\n",
    "\n",
    "<img src=\"images/streaming-dstream-ops.png\">\n",
    "\n",
    "Transforming DStreams\n",
    "---------------------\n",
    "\n",
    "Transformation                                 |For Each Incoming RDD\n",
    "--------------                                 |---------------------\n",
    "`ds.map(lambda line: line.upper())`            |Uppercase `line` \n",
    "`ds.flatMap(lambda line: line.split())`        |Split `line` into words\n",
    "`ds.filter(lambda line: line.strip() != '')`   |Exclude `line` if it is empty\n",
    "`ds.repartition(10)`                           |Repartition RDD into 10 partitions\n",
    "`ds.reduceByKey(lambda v1,v2: v1+v2)`          |For each key sum values \n",
    "`ds.groupByKey()`                              |For each key group values into iterable\n",
    "\n",
    "Generic Transformations\n",
    "-----------------------\n",
    "\n",
    "Q: How can I apply an arbitrary transformation on the incoming RDDs?\n",
    "\n",
    "- DStreams have some but not all of the transformations as RDDs.\n",
    "\n",
    "- For example, `sortByKey()` is not supported on DStreams.\n",
    "\n",
    "- Instead DStreams provide `transform()` \n",
    "\n",
    "- `transform()` lets you translate any RDD transformation to DStreams.\n",
    "\n",
    "- These two have the same effect.\n",
    "\n",
    "```python\n",
    "ds.transform(lambda rdd: rdd.flatMap(lambda line: line.split()))\n",
    "```\n",
    "\n",
    "```python\n",
    "ds.flatMap(lambda line: line.split())\n",
    "```\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: How can you write `sortByKey()` for DStreams?\n",
    "</summary>\n",
    "```python\n",
    "ds.transform(lambda rdd: rdd.sortByKey())\n",
    "```\n",
    "</details>\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "Consider this code:\n",
    "\n",
    "```python\n",
    "ds.transform(lambda rdd: rdd.flatMap(lambda line: line.split()))\n",
    "```\n",
    "\n",
    "<details><summary>\n",
    "Q: Where does `lambda line: ...` execute? \n",
    "</summary>\n",
    "On the executors.\n",
    "</details>\n",
    "\n",
    "\n",
    "<details><summary>\n",
    "Q: Where does `lambda rdd: ...` execute? \n",
    "</summary>\n",
    "On the driver.\n",
    "</details>\n",
    "\n",
    "\n",
    "DStream Output Operations\n",
    "-------------------------\n",
    "\n",
    "Expression                                     |Meaning\n",
    "----------                                     |-------\n",
    "`ds.foreachRDD(lambda rdd: func(rdd.first()))` |Call `func()` on `first()` of each incoming RDD\n",
    "`ds.pprint(num=10)`                            |Print first 10 elements of each incoming RDD\n",
    "`ds.saveAsTextFiles('foo',suffix=None)`        |Save each incoming RDD's partitions to disk\n",
    "\n",
    "Notes\n",
    "-----\n",
    "\n",
    "- These output operations only execute when RDDs start arriving.\n",
    "\n",
    "- `foreachRDD` is a generic output operation.\n",
    "\n",
    "- `foreachRDD` lets you define arbitrary output operations on incoming RDDs.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: Print the count of incoming RDDs.\n",
    "</summary>\n",
    "```python\n",
    "# Enable print as a function\n",
    "from __future__ import print_function\n",
    "\n",
    "# Define the output operation\n",
    "ds.foreachRDD(lambda rdd: print(rdd.count()))\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: Where will the lambda inside the `foreachRDD` execute?\n",
    "</summary>\n",
    "1. It will execute on the driver.<br>\n",
    "2. This is because RDDs are defined on the driver, not on the executors.<br>\n",
    "</details>\n",
    "\n",
    "\n",
    "Testing Streaming Apps Using QueueStream\n",
    "----------------------------------------\n",
    "\n",
    "Q: Manually testing apps using `nc` is quite tedious. Is there an\n",
    "easier more automatable way to do this?\n",
    "\n",
    "- *Queue streams* enable you to create preprogrammed streams perfect\n",
    "  for automated testing and test-driven development.\n",
    "\n",
    "Counting Event Types\n",
    "--------------------\n",
    "\n",
    "Q: Count how many events of different types are in incoming stream in\n",
    "each micro-batch.\n",
    "\n",
    "- Here is the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_queue_stream.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_queue_stream.py\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "print 'Initializing ssc'\n",
    "ssc = StreamingContext(SparkContext(), batchDuration=1) # batchDuration is 1 second\n",
    "\n",
    "print 'Initializing event_rdd_queue'\n",
    "event_rdd_queue = []\n",
    "for i in xrange(5):\n",
    "    # range(10) * 10 means \"output 0,1,2,...,9 ten times\n",
    "    events = range(10) * 10\n",
    "    event_rdd = ssc.sparkContext.parallelize(events)\n",
    "    event_rdd_queue.append(event_rdd)\n",
    "pprint(event_rdd_queue)\n",
    "\n",
    "print 'Building DStream pipeline'\n",
    "ds = ssc\\\n",
    "    .queueStream(event_rdd_queue) \\\n",
    "    .map(lambda event: (event, 1)) \\\n",
    "    .reduceByKey(lambda v1,v2: v1+v2)\n",
    "ds.pprint()\n",
    "\n",
    "print 'Starting ssc'\n",
    "ssc.start()\n",
    "time.sleep(6)\n",
    "\n",
    "print 'Stopping ssc'\n",
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets run this and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is terminated.\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "$SPARK_HOME/bin/spark-submit test_queue_stream.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating RDD\n",
    "===============\n",
    "\n",
    "Merging DStreams\n",
    "----------------\n",
    "\n",
    "Transformation      |Effect\n",
    "--------------      |------\n",
    "`ds1.union(ds2)`    |Combine RDD in `ds1` with RDD in same batch in `ds2`\n",
    "`ds1.join(ds2)`     |Join RDD in `ds1` with RDD in same batch in `ds2`\n",
    "\n",
    "Note\n",
    "----\n",
    "\n",
    "- For `union` or `join` the DStreams must have identical batch\n",
    "  durations.\n",
    "\n",
    "- The batches are matched up based on timestamps.\n",
    "\n",
    "\n",
    "Windowing Operations\n",
    "--------------------\n",
    "\n",
    "Q: How can I process multiple RDDs within a window of time?\n",
    "\n",
    "```python\n",
    "ds2 = ds1.window(windowDuration=30, slideDuration=10)\n",
    "```\n",
    "\n",
    "- Batches RDDs into 30-second windows \n",
    "\n",
    "- Produces new window every 10 seconds\n",
    "\n",
    "<img src=\"images/streaming-dstream-window.png\">\n",
    "\n",
    "Windowing Operations\n",
    "--------------------\n",
    "\n",
    "Q: Calculate the average of a series of heads and tails using a\n",
    "window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_window.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_window.py\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import time\n",
    "\n",
    "print 'Initializing ssc'\n",
    "ssc = StreamingContext(SparkContext(), batchDuration=1)\n",
    "\n",
    "print 'Initializing rdd_queue'\n",
    "rdd_queue = []\n",
    "for i in xrange(5): \n",
    "    rdd_data = xrange(1000)\n",
    "    rdd = ssc.sparkContext.parallelize(rdd_data)\n",
    "    rdd_queue.append(rdd)\n",
    "pprint(rdd_queue)\n",
    "\n",
    "print 'Creating queue stream'\n",
    "ds = ssc\\\n",
    "    .queueStream(rdd_queue)\\\n",
    "    .map(lambda x: (x % 10, 1))\\\n",
    "    .window(windowDuration=4,slideDuration=2)\\\n",
    "    .reduceByKey(lambda v1,v2:v1+v2)\n",
    "ds.pprint()\n",
    "\n",
    "print 'Starting ssc'\n",
    "ssc.start()\n",
    "time.sleep(20)\n",
    "\n",
    "print 'Stopping ssc'\n",
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets run this and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ssc\n",
      "Initializing rdd_queue\n",
      "[PythonRDD[5] at RDD at PythonRDD.scala:43,\n",
      " PythonRDD[6] at RDD at PythonRDD.scala:43,\n",
      " PythonRDD[7] at RDD at PythonRDD.scala:43,\n",
      " PythonRDD[8] at RDD at PythonRDD.scala:43,\n",
      " PythonRDD[9] at RDD at PythonRDD.scala:43]\n",
      "Creating queue stream\n",
      "Starting ssc\n",
      "-------------------------------------------\n",
      "Time: 2015-10-04 18:06:49\n",
      "-------------------------------------------\n",
      "(0, 200)\n",
      "(8, 200)\n",
      "(4, 200)\n",
      "(1, 200)\n",
      "(5, 200)\n",
      "(9, 200)\n",
      "(2, 200)\n",
      "(6, 200)\n",
      "(3, 200)\n",
      "(7, 200)\n",
      "()\n",
      "-------------------------------------------\n",
      "Time: 2015-10-04 18:06:51\n",
      "-------------------------------------------\n",
      "(0, 400)\n",
      "(8, 400)\n",
      "(4, 400)\n",
      "(1, 400)\n",
      "(5, 400)\n",
      "(9, 400)\n",
      "(2, 400)\n",
      "(6, 400)\n",
      "(3, 400)\n",
      "(7, 400)\n",
      "()\n",
      "-------------------------------------------\n",
      "Time: 2015-10-04 18:06:53\n",
      "-------------------------------------------\n",
      "(0, 300)\n",
      "(8, 300)\n",
      "(4, 300)\n",
      "(1, 300)\n",
      "(5, 300)\n",
      "(9, 300)\n",
      "(2, 300)\n",
      "(6, 300)\n",
      "(3, 300)\n",
      "(7, 300)\n",
      "()\n",
      "-------------------------------------------\n",
      "Time: 2015-10-04 18:06:55\n",
      "-------------------------------------------\n",
      "(0, 100)\n",
      "(8, 100)\n",
      "(4, 100)\n",
      "(1, 100)\n",
      "(5, 100)\n",
      "(9, 100)\n",
      "(2, 100)\n",
      "(6, 100)\n",
      "(3, 100)\n",
      "(7, 100)\n",
      "()\n",
      "-------------------------------------------\n",
      "Time: 2015-10-04 18:06:57\n",
      "-------------------------------------------\n",
      "()\n",
      "-------------------------------------------\n",
      "Time: 2015-10-04 18:06:59\n",
      "-------------------------------------------\n",
      "()\n",
      "-------------------------------------------\n",
      "Time: 2015-10-04 18:07:01\n",
      "-------------------------------------------\n",
      "()\n",
      "-------------------------------------------\n",
      "Time: 2015-10-04 18:07:03\n",
      "-------------------------------------------\n",
      "()\n",
      "-------------------------------------------\n",
      "Time: 2015-10-04 18:07:05\n",
      "-------------------------------------------\n",
      "()\n",
      "-------------------------------------------\n",
      "Time: 2015-10-04 18:07:07\n",
      "-------------------------------------------\n",
      "()\n",
      "Stopping ssc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "15/10/04 18:06:44 INFO SparkContext: Running Spark version 1.4.1\n",
      "15/10/04 18:06:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/10/04 18:06:45 INFO SecurityManager: Changing view acls to: Alexander\n",
      "15/10/04 18:06:45 INFO SecurityManager: Changing modify acls to: Alexander\n",
      "15/10/04 18:06:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(Alexander); users with modify permissions: Set(Alexander)\n",
      "15/10/04 18:06:46 INFO Slf4jLogger: Slf4jLogger started\n",
      "15/10/04 18:06:46 INFO Remoting: Starting remoting\n",
      "15/10/04 18:06:46 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.0.0.9:52443]\n",
      "15/10/04 18:06:46 INFO Utils: Successfully started service 'sparkDriver' on port 52443.\n",
      "15/10/04 18:06:46 INFO SparkEnv: Registering MapOutputTracker\n",
      "15/10/04 18:06:46 INFO SparkEnv: Registering BlockManagerMaster\n",
      "15/10/04 18:06:46 INFO DiskBlockManager: Created local directory at /private/var/folders/vx/9464rz3s72b0l50g6wwpr2hm0000gn/T/spark-a8890cd7-845e-489d-8c60-b14b2e729b6c/blockmgr-059ebf67-f1a1-4abe-81cf-40176bcb0ccb\n",
      "15/10/04 18:06:46 INFO MemoryStore: MemoryStore started with capacity 265.4 MB\n",
      "15/10/04 18:06:46 INFO HttpFileServer: HTTP File server directory is /private/var/folders/vx/9464rz3s72b0l50g6wwpr2hm0000gn/T/spark-a8890cd7-845e-489d-8c60-b14b2e729b6c/httpd-ac2497b1-09ef-4376-9eac-44876fb96863\n",
      "15/10/04 18:06:46 INFO HttpServer: Starting HTTP Server\n",
      "15/10/04 18:06:46 INFO Utils: Successfully started service 'HTTP file server' on port 52444.\n",
      "15/10/04 18:06:46 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "15/10/04 18:06:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "15/10/04 18:06:46 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "15/10/04 18:06:46 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "15/10/04 18:06:46 INFO Utils: Successfully started service 'SparkUI' on port 4043.\n",
      "15/10/04 18:06:46 INFO SparkUI: Started SparkUI at http://10.0.0.9:4043\n",
      "15/10/04 18:06:46 INFO Utils: Copying /Users/Alexander/DSCI6007-student/week6/6.4/test_window.py to /private/var/folders/vx/9464rz3s72b0l50g6wwpr2hm0000gn/T/spark-a8890cd7-845e-489d-8c60-b14b2e729b6c/userFiles-aa850f43-43d7-474c-ae5c-cd86b302b015/test_window.py\n",
      "15/10/04 18:06:47 INFO SparkContext: Added file file:/Users/Alexander/DSCI6007-student/week6/6.4/test_window.py at file:/Users/Alexander/DSCI6007-student/week6/6.4/test_window.py with timestamp 1444007206992\n",
      "15/10/04 18:06:47 INFO Executor: Starting executor ID driver on host localhost\n",
      "15/10/04 18:06:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52448.\n",
      "15/10/04 18:06:47 INFO NettyBlockTransferService: Server created on 52448\n",
      "15/10/04 18:06:47 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "15/10/04 18:06:47 INFO BlockManagerMasterEndpoint: Registering block manager localhost:52448 with 265.4 MB RAM, BlockManagerId(driver, localhost, 52448)\n",
      "15/10/04 18:06:47 INFO BlockManagerMaster: Registered BlockManager\n",
      "15/10/04 18:06:47 INFO PythonTransformedDStream: Duration for remembering RDDs set to 6000 ms for org.apache.spark.streaming.api.python.PythonTransformedDStream@614b284c\n",
      "15/10/04 18:06:47 INFO QueueInputDStream: Duration for remembering RDDs set to 6000 ms for org.apache.spark.streaming.dstream.QueueInputDStream@d42c77b\n",
      "15/10/04 18:06:47 INFO ForEachDStream: metadataCleanupDelay = -1\n",
      "15/10/04 18:06:47 INFO PythonTransformedDStream: metadataCleanupDelay = -1\n",
      "15/10/04 18:06:47 INFO WindowedDStream: metadataCleanupDelay = -1\n",
      "15/10/04 18:06:47 INFO PythonTransformedDStream: metadataCleanupDelay = -1\n",
      "15/10/04 18:06:47 INFO QueueInputDStream: metadataCleanupDelay = -1\n",
      "15/10/04 18:06:47 INFO QueueInputDStream: Slide time = 1000 ms\n",
      "15/10/04 18:06:47 INFO QueueInputDStream: Storage level = StorageLevel(false, false, false, false, 1)\n",
      "15/10/04 18:06:47 INFO QueueInputDStream: Checkpoint interval = null\n",
      "15/10/04 18:06:47 INFO QueueInputDStream: Remember duration = 6000 ms\n",
      "15/10/04 18:06:47 INFO QueueInputDStream: Initialized and validated org.apache.spark.streaming.dstream.QueueInputDStream@d42c77b\n",
      "15/10/04 18:06:47 INFO PythonTransformedDStream: Slide time = 1000 ms\n",
      "15/10/04 18:06:47 INFO PythonTransformedDStream: Storage level = StorageLevel(false, true, false, false, 1)\n",
      "15/10/04 18:06:47 INFO PythonTransformedDStream: Checkpoint interval = null\n",
      "15/10/04 18:06:47 INFO PythonTransformedDStream: Remember duration = 6000 ms\n",
      "15/10/04 18:06:47 INFO PythonTransformedDStream: Initialized and validated org.apache.spark.streaming.api.python.PythonTransformedDStream@614b284c\n",
      "15/10/04 18:06:47 INFO WindowedDStream: Slide time = 2000 ms\n",
      "15/10/04 18:06:47 INFO WindowedDStream: Storage level = StorageLevel(false, false, false, false, 1)\n",
      "15/10/04 18:06:47 INFO WindowedDStream: Checkpoint interval = null\n",
      "15/10/04 18:06:47 INFO WindowedDStream: Remember duration = 2000 ms\n",
      "15/10/04 18:06:47 INFO WindowedDStream: Initialized and validated org.apache.spark.streaming.dstream.WindowedDStream@5f121a29\n",
      "15/10/04 18:06:47 INFO PythonTransformedDStream: Slide time = 2000 ms\n",
      "15/10/04 18:06:47 INFO PythonTransformedDStream: Storage level = StorageLevel(false, false, false, false, 1)\n",
      "15/10/04 18:06:47 INFO PythonTransformedDStream: Checkpoint interval = null\n",
      "15/10/04 18:06:47 INFO PythonTransformedDStream: Remember duration = 2000 ms\n",
      "15/10/04 18:06:47 INFO PythonTransformedDStream: Initialized and validated org.apache.spark.streaming.api.python.PythonTransformedDStream@1d73aa82\n",
      "15/10/04 18:06:47 INFO ForEachDStream: Slide time = 2000 ms\n",
      "15/10/04 18:06:47 INFO ForEachDStream: Storage level = StorageLevel(false, false, false, false, 1)\n",
      "15/10/04 18:06:47 INFO ForEachDStream: Checkpoint interval = null\n",
      "15/10/04 18:06:47 INFO ForEachDStream: Remember duration = 2000 ms\n",
      "15/10/04 18:06:47 INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@14bf4be9\n",
      "15/10/04 18:06:47 INFO RecurringTimer: Started timer for JobGenerator at time 1444007208000\n",
      "15/10/04 18:06:47 INFO JobGenerator: Started JobGenerator at 1444007208000 ms\n",
      "15/10/04 18:06:47 INFO JobScheduler: Started JobScheduler\n",
      "15/10/04 18:06:47 INFO StreamingContext: StreamingContext started\n",
      "15/10/04 18:06:48 INFO PythonTransformedDStream: Time 1444007208000 ms is invalid as zeroTime is 1444007207000 ms and slideDuration is 2000 ms and difference is 1000 ms\n",
      "15/10/04 18:06:48 INFO JobScheduler: No jobs added for time 1444007208000 ms\n",
      "15/10/04 18:06:49 INFO PythonTransformedDStream: Slicing from 1444007206000 ms to 1444007209000 ms (aligned to 1444007206000 ms and 1444007209000 ms)\n",
      "15/10/04 18:06:49 INFO PythonTransformedDStream: Time 1444007207000 ms is invalid as zeroTime is 1444007207000 ms and slideDuration is 1000 ms and difference is 0 ms\n",
      "15/10/04 18:06:49 INFO JobScheduler: Added jobs for time 1444007209000 ms\n",
      "15/10/04 18:06:49 INFO JobScheduler: Starting job streaming job 1444007209000 ms.0 from job set of time 1444007209000 ms\n",
      "15/10/04 18:06:49 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/04 18:06:49 INFO DAGScheduler: Registering RDD 15 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/04 18:06:49 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:366) with 1 output partitions (allowLocal=true)\n",
      "15/10/04 18:06:49 INFO DAGScheduler: Final stage: ResultStage 1(runJob at PythonRDD.scala:366)\n",
      "15/10/04 18:06:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
      "15/10/04 18:06:49 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
      "15/10/04 18:06:49 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[15] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "15/10/04 18:06:49 INFO MemoryStore: ensureFreeSpace(11288) called with curMem=0, maxMem=278302556\n",
      "15/10/04 18:06:49 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 11.0 KB, free 265.4 MB)\n",
      "15/10/04 18:06:49 INFO MemoryStore: ensureFreeSpace(5430) called with curMem=11288, maxMem=278302556\n",
      "15/10/04 18:06:49 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.3 KB, free 265.4 MB)\n",
      "15/10/04 18:06:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:52448 (size: 5.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:06:49 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:06:49 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 0 (PairwiseRDD[15] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/04 18:06:49 INFO TaskSchedulerImpl: Adding task set 0.0 with 8 tasks\n",
      "15/10/04 18:06:49 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:49 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:49 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:49 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:49 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)\n",
      "15/10/04 18:06:49 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
      "15/10/04 18:06:49 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
      "15/10/04 18:06:49 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "15/10/04 18:06:49 INFO Executor: Fetching file:/Users/Alexander/DSCI6007-student/week6/6.4/test_window.py with timestamp 1444007206992\n",
      "15/10/04 18:06:49 INFO Utils: /Users/Alexander/DSCI6007-student/week6/6.4/test_window.py has been previously copied to /private/var/folders/vx/9464rz3s72b0l50g6wwpr2hm0000gn/T/spark-a8890cd7-845e-489d-8c60-b14b2e729b6c/userFiles-aa850f43-43d7-474c-ae5c-cd86b302b015/test_window.py\n",
      "15/10/04 18:06:50 INFO PythonTransformedDStream: Time 1444007210000 ms is invalid as zeroTime is 1444007207000 ms and slideDuration is 2000 ms and difference is 3000 ms\n",
      "15/10/04 18:06:50 INFO JobScheduler: No jobs added for time 1444007210000 ms\n",
      "15/10/04 18:06:51 INFO PythonTransformedDStream: Slicing from 1444007208000 ms to 1444007211000 ms (aligned to 1444007208000 ms and 1444007211000 ms)\n",
      "15/10/04 18:06:51 INFO JobScheduler: Added jobs for time 1444007211000 ms\n",
      "15/10/04 18:06:51 INFO CacheManager: Partition rdd_11_0 not found, computing it\n",
      "15/10/04 18:06:51 INFO CacheManager: Partition rdd_11_3 not found, computing it\n",
      "15/10/04 18:06:51 INFO CacheManager: Partition rdd_11_1 not found, computing it\n",
      "15/10/04 18:06:51 INFO CacheManager: Partition rdd_11_2 not found, computing it\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 8, boot = 5, init = 2, finish = 1\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 12, boot = 11, init = 1, finish = 0\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 17, boot = 15, init = 2, finish = 0\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 20, boot = 5, init = 14, finish = 1\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 20, boot = 8, init = 11, finish = 1\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 27, boot = 3, init = 23, finish = 1\n",
      "15/10/04 18:06:51 INFO MemoryStore: ensureFreeSpace(1090) called with curMem=16718, maxMem=278302556\n",
      "15/10/04 18:06:51 INFO MemoryStore: Block rdd_11_1 stored as bytes in memory (estimated size 1090.0 B, free 265.4 MB)\n",
      "15/10/04 18:06:51 INFO MemoryStore: ensureFreeSpace(1090) called with curMem=17808, maxMem=278302556\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 27, boot = 12, init = 14, finish = 1\n",
      "15/10/04 18:06:51 INFO MemoryStore: Block rdd_11_2 stored as bytes in memory (estimated size 1090.0 B, free 265.4 MB)\n",
      "15/10/04 18:06:51 INFO BlockManagerInfo: Added rdd_11_1 in memory on localhost:52448 (size: 1090.0 B, free: 265.4 MB)\n",
      "15/10/04 18:06:51 INFO MemoryStore: ensureFreeSpace(1090) called with curMem=18898, maxMem=278302556\n",
      "15/10/04 18:06:51 INFO BlockManagerInfo: Added rdd_11_2 in memory on localhost:52448 (size: 1090.0 B, free: 265.4 MB)\n",
      "15/10/04 18:06:51 INFO MemoryStore: Block rdd_11_3 stored as bytes in memory (estimated size 1090.0 B, free 265.4 MB)\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 42, boot = 10, init = 31, finish = 1\n",
      "15/10/04 18:06:51 INFO MemoryStore: ensureFreeSpace(1090) called with curMem=19988, maxMem=278302556\n",
      "15/10/04 18:06:51 INFO MemoryStore: Block rdd_11_0 stored as bytes in memory (estimated size 1090.0 B, free 265.4 MB)\n",
      "15/10/04 18:06:51 INFO BlockManagerInfo: Added rdd_11_3 in memory on localhost:52448 (size: 1090.0 B, free: 265.4 MB)\n",
      "15/10/04 18:06:51 INFO BlockManagerInfo: Added rdd_11_0 in memory on localhost:52448 (size: 1090.0 B, free: 265.4 MB)\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 2069, boot = 1926, init = 141, finish = 2\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 2003, boot = 1922, init = 79, finish = 2\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 2009, boot = 1922, init = 85, finish = 2\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 2012, boot = 1930, init = 70, finish = 12\n",
      "15/10/04 18:06:51 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1524 bytes result sent to driver\n",
      "15/10/04 18:06:51 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1524 bytes result sent to driver\n",
      "15/10/04 18:06:51 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1524 bytes result sent to driver\n",
      "15/10/04 18:06:51 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1524 bytes result sent to driver\n",
      "15/10/04 18:06:51 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:51 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)\n",
      "15/10/04 18:06:51 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:51 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)\n",
      "15/10/04 18:06:51 INFO CacheManager: Partition rdd_12_0 not found, computing it\n",
      "15/10/04 18:06:51 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 2267 ms on localhost (1/8)\n",
      "15/10/04 18:06:51 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2290 ms on localhost (2/8)\n",
      "15/10/04 18:06:51 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:51 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 2276 ms on localhost (3/8)\n",
      "15/10/04 18:06:51 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:51 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2282 ms on localhost (4/8)\n",
      "15/10/04 18:06:51 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 14, boot = -63, init = 77, finish = 0\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 17, boot = -151, init = 167, finish = 1\n",
      "15/10/04 18:06:51 INFO MemoryStore: ensureFreeSpace(1090) called with curMem=21078, maxMem=278302556\n",
      "15/10/04 18:06:51 INFO MemoryStore: Block rdd_12_0 stored as bytes in memory (estimated size 1090.0 B, free 265.4 MB)\n",
      "15/10/04 18:06:51 INFO BlockManagerInfo: Added rdd_12_0 in memory on localhost:52448 (size: 1090.0 B, free: 265.4 MB)\n",
      "15/10/04 18:06:51 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)\n",
      "15/10/04 18:06:51 INFO CacheManager: Partition rdd_12_3 not found, computing it\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 25, boot = -129, init = 154, finish = 0\n",
      "15/10/04 18:06:51 INFO CacheManager: Partition rdd_12_1 not found, computing it\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 14, boot = -81, init = 95, finish = 0\n",
      "15/10/04 18:06:51 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1524 bytes result sent to driver\n",
      "15/10/04 18:06:51 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 67 ms on localhost (5/8)\n",
      "15/10/04 18:06:51 INFO CacheManager: Partition rdd_12_2 not found, computing it\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 17, boot = 16, init = 0, finish = 1\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 6, boot = 4, init = 2, finish = 0\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 19, boot = -88, init = 106, finish = 1\n",
      "15/10/04 18:06:51 INFO MemoryStore: ensureFreeSpace(1090) called with curMem=22168, maxMem=278302556\n",
      "15/10/04 18:06:51 INFO MemoryStore: Block rdd_12_1 stored as bytes in memory (estimated size 1090.0 B, free 265.4 MB)\n",
      "15/10/04 18:06:51 INFO BlockManagerInfo: Added rdd_12_1 in memory on localhost:52448 (size: 1090.0 B, free: 265.4 MB)\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 13, boot = 3, init = 9, finish = 1\n",
      "15/10/04 18:06:51 INFO MemoryStore: ensureFreeSpace(1090) called with curMem=23258, maxMem=278302556\n",
      "15/10/04 18:06:51 INFO MemoryStore: Block rdd_12_2 stored as bytes in memory (estimated size 1090.0 B, free 265.4 MB)\n",
      "15/10/04 18:06:51 INFO BlockManagerInfo: Added rdd_12_2 in memory on localhost:52448 (size: 1090.0 B, free: 265.4 MB)\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 31, boot = -120, init = 150, finish = 1\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 26, boot = 0, init = 26, finish = 0\n",
      "15/10/04 18:06:51 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1524 bytes result sent to driver\n",
      "15/10/04 18:06:51 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 101 ms on localhost (6/8)\n",
      "15/10/04 18:06:51 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1524 bytes result sent to driver\n",
      "15/10/04 18:06:51 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 95 ms on localhost (7/8)\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 87, boot = 86, init = 1, finish = 0\n",
      "15/10/04 18:06:51 INFO MemoryStore: ensureFreeSpace(1090) called with curMem=24348, maxMem=278302556\n",
      "15/10/04 18:06:51 INFO MemoryStore: Block rdd_12_3 stored as bytes in memory (estimated size 1090.0 B, free 265.4 MB)\n",
      "15/10/04 18:06:51 INFO BlockManagerInfo: Added rdd_12_3 in memory on localhost:52448 (size: 1090.0 B, free: 265.4 MB)\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 97, boot = -83, init = 179, finish = 1\n",
      "15/10/04 18:06:51 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1524 bytes result sent to driver\n",
      "15/10/04 18:06:51 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 133 ms on localhost (8/8)\n",
      "15/10/04 18:06:51 INFO DAGScheduler: ShuffleMapStage 0 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 2.436 s\n",
      "15/10/04 18:06:51 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:06:51 INFO DAGScheduler: looking for newly runnable stages\n",
      "15/10/04 18:06:51 INFO DAGScheduler: running: Set()\n",
      "15/10/04 18:06:51 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
      "15/10/04 18:06:51 INFO DAGScheduler: failed: Set()\n",
      "15/10/04 18:06:51 INFO DAGScheduler: Missing parents for ResultStage 1: List()\n",
      "15/10/04 18:06:51 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[19] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "15/10/04 18:06:51 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=25438, maxMem=278302556\n",
      "15/10/04 18:06:51 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 5.9 KB, free 265.4 MB)\n",
      "15/10/04 18:06:51 INFO MemoryStore: ensureFreeSpace(3405) called with curMem=31478, maxMem=278302556\n",
      "15/10/04 18:06:51 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 265.4 MB)\n",
      "15/10/04 18:06:51 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:52448 (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:06:51 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:06:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[19] at RDD at PythonRDD.scala:43)\n",
      "15/10/04 18:06:51 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks\n",
      "15/10/04 18:06:51 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 8, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:06:51 INFO Executor: Running task 0.0 in stage 1.0 (TID 8)\n",
      "15/10/04 18:06:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/04 18:06:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 38, boot = -23, init = 61, finish = 0\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 39, boot = -60, init = 99, finish = 0\n",
      "15/10/04 18:06:51 INFO Executor: Finished task 0.0 in stage 1.0 (TID 8). 1001 bytes result sent to driver\n",
      "15/10/04 18:06:51 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 8) in 49 ms on localhost (1/1)\n",
      "15/10/04 18:06:51 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:06:51 INFO DAGScheduler: ResultStage 1 (runJob at PythonRDD.scala:366) finished in 0.050 s\n",
      "15/10/04 18:06:51 INFO DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:366, took 2.692489 s\n",
      "15/10/04 18:06:51 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/04 18:06:51 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 157 bytes\n",
      "15/10/04 18:06:51 INFO DAGScheduler: Got job 1 (runJob at PythonRDD.scala:366) with 3 output partitions (allowLocal=true)\n",
      "15/10/04 18:06:51 INFO DAGScheduler: Final stage: ResultStage 3(runJob at PythonRDD.scala:366)\n",
      "15/10/04 18:06:51 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
      "15/10/04 18:06:51 INFO DAGScheduler: Missing parents: List()\n",
      "15/10/04 18:06:51 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[28] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "15/10/04 18:06:51 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=34883, maxMem=278302556\n",
      "15/10/04 18:06:51 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 5.9 KB, free 265.4 MB)\n",
      "15/10/04 18:06:51 INFO MemoryStore: ensureFreeSpace(3405) called with curMem=40923, maxMem=278302556\n",
      "15/10/04 18:06:51 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.3 KB, free 265.4 MB)\n",
      "15/10/04 18:06:51 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:52448 (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:06:51 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:06:51 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 3 (PythonRDD[28] at RDD at PythonRDD.scala:43)\n",
      "15/10/04 18:06:51 INFO TaskSchedulerImpl: Adding task set 3.0 with 3 tasks\n",
      "15/10/04 18:06:51 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 9, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:06:51 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 10, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:06:51 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 11, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:06:51 INFO Executor: Running task 0.0 in stage 3.0 (TID 9)\n",
      "15/10/04 18:06:51 INFO Executor: Running task 1.0 in stage 3.0 (TID 10)\n",
      "15/10/04 18:06:51 INFO Executor: Running task 2.0 in stage 3.0 (TID 11)\n",
      "15/10/04 18:06:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/04 18:06:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/04 18:06:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:06:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:06:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/04 18:06:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 5, boot = -113, init = 118, finish = 0\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 5, boot = -108, init = 113, finish = 0\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 6, boot = -138, init = 144, finish = 0\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 7, boot = -143, init = 150, finish = 0\n",
      "15/10/04 18:06:51 INFO Executor: Finished task 1.0 in stage 3.0 (TID 10). 993 bytes result sent to driver\n",
      "15/10/04 18:06:51 INFO Executor: Finished task 2.0 in stage 3.0 (TID 11). 993 bytes result sent to driver\n",
      "15/10/04 18:06:51 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 10) in 15 ms on localhost (1/3)\n",
      "15/10/04 18:06:51 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 11) in 15 ms on localhost (2/3)\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 15, boot = -30, init = 45, finish = 0\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 33, boot = -31, init = 64, finish = 0\n",
      "15/10/04 18:06:51 INFO Executor: Finished task 0.0 in stage 3.0 (TID 9). 1001 bytes result sent to driver\n",
      "15/10/04 18:06:51 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 9) in 43 ms on localhost (3/3)\n",
      "15/10/04 18:06:51 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:06:51 INFO DAGScheduler: ResultStage 3 (runJob at PythonRDD.scala:366) finished in 0.044 s\n",
      "15/10/04 18:06:51 INFO DAGScheduler: Job 1 finished: runJob at PythonRDD.scala:366, took 0.061221 s\n",
      "15/10/04 18:06:51 INFO JobScheduler: Finished job streaming job 1444007209000 ms.0 from job set of time 1444007209000 ms\n",
      "15/10/04 18:06:51 INFO JobScheduler: Total delay: 2.906 s for time 1444007209000 ms (execution: 2.792 s)\n",
      "15/10/04 18:06:51 INFO JobScheduler: Starting job streaming job 1444007211000 ms.0 from job set of time 1444007211000 ms\n",
      "15/10/04 18:06:51 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "15/10/04 18:06:51 INFO InputInfoTracker: remove old batch metadata: \n",
      "15/10/04 18:06:51 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/04 18:06:51 INFO DAGScheduler: Registering RDD 24 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/04 18:06:51 INFO DAGScheduler: Got job 2 (runJob at PythonRDD.scala:366) with 1 output partitions (allowLocal=true)\n",
      "15/10/04 18:06:51 INFO DAGScheduler: Final stage: ResultStage 5(runJob at PythonRDD.scala:366)\n",
      "15/10/04 18:06:51 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
      "15/10/04 18:06:51 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 4)\n",
      "15/10/04 18:06:51 INFO DAGScheduler: Submitting ShuffleMapStage 4 (PairwiseRDD[24] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "15/10/04 18:06:51 INFO MemoryStore: ensureFreeSpace(15616) called with curMem=44328, maxMem=278302556\n",
      "15/10/04 18:06:51 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.3 KB, free 265.4 MB)\n",
      "15/10/04 18:06:51 INFO MemoryStore: ensureFreeSpace(5723) called with curMem=59944, maxMem=278302556\n",
      "15/10/04 18:06:51 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.6 KB, free 265.3 MB)\n",
      "15/10/04 18:06:51 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:52448 (size: 5.6 KB, free: 265.4 MB)\n",
      "15/10/04 18:06:51 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:06:51 INFO DAGScheduler: Submitting 16 missing tasks from ShuffleMapStage 4 (PairwiseRDD[24] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/04 18:06:51 INFO TaskSchedulerImpl: Adding task set 4.0 with 16 tasks\n",
      "15/10/04 18:06:51 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 12, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:51 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 13, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:51 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 14, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:51 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 15, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:51 INFO Executor: Running task 0.0 in stage 4.0 (TID 12)\n",
      "15/10/04 18:06:51 INFO Executor: Running task 3.0 in stage 4.0 (TID 15)\n",
      "15/10/04 18:06:51 INFO Executor: Running task 1.0 in stage 4.0 (TID 13)\n",
      "15/10/04 18:06:51 INFO Executor: Running task 2.0 in stage 4.0 (TID 14)\n",
      "15/10/04 18:06:51 INFO BlockManager: Found block rdd_11_3 locally\n",
      "15/10/04 18:06:51 INFO BlockManager: Found block rdd_11_0 locally\n",
      "15/10/04 18:06:51 INFO BlockManager: Found block rdd_11_1 locally\n",
      "15/10/04 18:06:51 INFO BlockManager: Found block rdd_11_2 locally\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 4, boot = -70, init = 74, finish = 0\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 14, boot = -72, init = 85, finish = 1\n",
      "15/10/04 18:06:51 INFO PythonRDD: Times: total = 23, boot = -71, init = 88, finish = 6\n",
      "15/10/04 18:06:52 INFO PythonTransformedDStream: Time 1444007212000 ms is invalid as zeroTime is 1444007207000 ms and slideDuration is 2000 ms and difference is 5000 ms\n",
      "15/10/04 18:06:52 INFO JobScheduler: No jobs added for time 1444007212000 ms\n",
      "15/10/04 18:06:52 INFO Executor: Finished task 0.0 in stage 4.0 (TID 12). 2066 bytes result sent to driver\n",
      "15/10/04 18:06:52 INFO Executor: Finished task 1.0 in stage 4.0 (TID 13). 2066 bytes result sent to driver\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 16, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:52 INFO Executor: Running task 4.0 in stage 4.0 (TID 16)\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 17, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 12) in 61 ms on localhost (1/16)\n",
      "15/10/04 18:06:52 INFO Executor: Running task 5.0 in stage 4.0 (TID 17)\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 13) in 60 ms on localhost (2/16)\n",
      "15/10/04 18:06:52 INFO BlockManager: Found block rdd_12_0 locally\n",
      "15/10/04 18:06:52 INFO Executor: Finished task 2.0 in stage 4.0 (TID 14). 2066 bytes result sent to driver\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 18, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 14) in 68 ms on localhost (3/16)\n",
      "15/10/04 18:06:52 INFO Executor: Running task 6.0 in stage 4.0 (TID 18)\n",
      "15/10/04 18:06:52 INFO BlockManager: Found block rdd_12_1 locally\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 10, boot = -101, init = 110, finish = 1\n",
      "15/10/04 18:06:52 INFO BlockManager: Found block rdd_12_2 locally\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 65, boot = -51, init = 106, finish = 10\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 3, boot = -8, init = 10, finish = 1\n",
      "15/10/04 18:06:52 INFO Executor: Finished task 3.0 in stage 4.0 (TID 15). 2066 bytes result sent to driver\n",
      "15/10/04 18:06:52 INFO Executor: Finished task 4.0 in stage 4.0 (TID 16). 2066 bytes result sent to driver\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 19, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:52 INFO Executor: Running task 7.0 in stage 4.0 (TID 19)\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 15) in 95 ms on localhost (4/16)\n",
      "15/10/04 18:06:52 INFO Executor: Finished task 6.0 in stage 4.0 (TID 18). 2066 bytes result sent to driver\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 20, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 16) in 42 ms on localhost (5/16)\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 21, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:52 INFO Executor: Running task 9.0 in stage 4.0 (TID 21)\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 18) in 35 ms on localhost (6/16)\n",
      "15/10/04 18:06:52 INFO BlockManager: Found block rdd_12_3 locally\n",
      "15/10/04 18:06:52 INFO Executor: Running task 8.0 in stage 4.0 (TID 20)\n",
      "15/10/04 18:06:52 INFO CacheManager: Partition rdd_20_1 not found, computing it\n",
      "15/10/04 18:06:52 INFO CacheManager: Partition rdd_20_0 not found, computing it\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 41, boot = -95, init = 135, finish = 1\n",
      "15/10/04 18:06:52 INFO Executor: Finished task 5.0 in stage 4.0 (TID 17). 2066 bytes result sent to driver\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 22, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:52 INFO Executor: Running task 10.0 in stage 4.0 (TID 22)\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 17) in 74 ms on localhost (7/16)\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 20, boot = 20, init = 0, finish = 0\n",
      "15/10/04 18:06:52 INFO CacheManager: Partition rdd_20_2 not found, computing it\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 28, boot = 5, init = 23, finish = 0\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 36, boot = 35, init = 0, finish = 1\n",
      "15/10/04 18:06:52 INFO MemoryStore: ensureFreeSpace(1090) called with curMem=65667, maxMem=278302556\n",
      "15/10/04 18:06:52 INFO MemoryStore: Block rdd_20_0 stored as bytes in memory (estimated size 1090.0 B, free 265.3 MB)\n",
      "15/10/04 18:06:52 INFO BlockManagerInfo: Added rdd_20_0 in memory on localhost:52448 (size: 1090.0 B, free: 265.4 MB)\n",
      "15/10/04 18:06:52 INFO Executor: Finished task 7.0 in stage 4.0 (TID 19). 2066 bytes result sent to driver\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 52, boot = 51, init = 0, finish = 1\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 23, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 19) in 78 ms on localhost (8/16)\n",
      "15/10/04 18:06:52 INFO Executor: Running task 11.0 in stage 4.0 (TID 23)\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 67, boot = 66, init = 1, finish = 0\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 69, boot = 53, init = 16, finish = 0\n",
      "15/10/04 18:06:52 INFO MemoryStore: ensureFreeSpace(1090) called with curMem=66757, maxMem=278302556\n",
      "15/10/04 18:06:52 INFO MemoryStore: Block rdd_20_1 stored as bytes in memory (estimated size 1090.0 B, free 265.3 MB)\n",
      "15/10/04 18:06:52 INFO Executor: Finished task 8.0 in stage 4.0 (TID 20). 1524 bytes result sent to driver\n",
      "15/10/04 18:06:52 INFO BlockManagerInfo: Added rdd_20_1 in memory on localhost:52448 (size: 1090.0 B, free: 265.4 MB)\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 24, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 20) in 91 ms on localhost (9/16)\n",
      "15/10/04 18:06:52 INFO Executor: Running task 12.0 in stage 4.0 (TID 24)\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 85, boot = 38, init = 46, finish = 1\n",
      "15/10/04 18:06:52 INFO CacheManager: Partition rdd_20_3 not found, computing it\n",
      "15/10/04 18:06:52 INFO CacheManager: Partition rdd_21_0 not found, computing it\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 11, boot = 5, init = 6, finish = 0\n",
      "15/10/04 18:06:52 INFO Executor: Finished task 9.0 in stage 4.0 (TID 21). 1524 bytes result sent to driver\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 16, boot = 13, init = 2, finish = 1\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 25, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:52 INFO Executor: Running task 13.0 in stage 4.0 (TID 25)\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 21) in 117 ms on localhost (10/16)\n",
      "15/10/04 18:06:52 INFO CacheManager: Partition rdd_21_1 not found, computing it\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 85, boot = 85, init = 0, finish = 0\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 93, boot = 5, init = 87, finish = 1\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 6, boot = 4, init = 2, finish = 0\n",
      "15/10/04 18:06:52 INFO MemoryStore: ensureFreeSpace(1090) called with curMem=67847, maxMem=278302556\n",
      "15/10/04 18:06:52 INFO MemoryStore: Block rdd_20_2 stored as bytes in memory (estimated size 1090.0 B, free 265.3 MB)\n",
      "15/10/04 18:06:52 INFO BlockManagerInfo: Added rdd_20_2 in memory on localhost:52448 (size: 1090.0 B, free: 265.4 MB)\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 14, boot = 5, init = 8, finish = 1\n",
      "15/10/04 18:06:52 INFO MemoryStore: ensureFreeSpace(1090) called with curMem=68937, maxMem=278302556\n",
      "15/10/04 18:06:52 INFO MemoryStore: Block rdd_21_1 stored as bytes in memory (estimated size 1090.0 B, free 265.3 MB)\n",
      "15/10/04 18:06:52 INFO BlockManagerInfo: Added rdd_21_1 in memory on localhost:52448 (size: 1090.0 B, free: 265.4 MB)\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 29, boot = 5, init = 22, finish = 2\n",
      "15/10/04 18:06:52 INFO Executor: Finished task 13.0 in stage 4.0 (TID 25). 1524 bytes result sent to driver\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 26, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:52 INFO Executor: Running task 14.0 in stage 4.0 (TID 26)\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 25) in 58 ms on localhost (11/16)\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 107, boot = 5, init = 101, finish = 1\n",
      "15/10/04 18:06:52 INFO CacheManager: Partition rdd_21_2 not found, computing it\n",
      "15/10/04 18:06:52 INFO Executor: Finished task 10.0 in stage 4.0 (TID 22). 1524 bytes result sent to driver\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Starting task 15.0 in stage 4.0 (TID 27, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:52 INFO Executor: Running task 15.0 in stage 4.0 (TID 27)\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 22) in 175 ms on localhost (12/16)\n",
      "15/10/04 18:06:52 INFO CacheManager: Partition rdd_21_3 not found, computing it\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 114, boot = 114, init = 0, finish = 0\n",
      "15/10/04 18:06:52 INFO MemoryStore: ensureFreeSpace(1090) called with curMem=70027, maxMem=278302556\n",
      "15/10/04 18:06:52 INFO MemoryStore: Block rdd_21_0 stored as bytes in memory (estimated size 1090.0 B, free 265.3 MB)\n",
      "15/10/04 18:06:52 INFO BlockManagerInfo: Added rdd_21_0 in memory on localhost:52448 (size: 1090.0 B, free: 265.4 MB)\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 141, boot = 140, init = 0, finish = 1\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 148, boot = 147, init = 1, finish = 0\n",
      "15/10/04 18:06:52 INFO MemoryStore: ensureFreeSpace(1090) called with curMem=71117, maxMem=278302556\n",
      "15/10/04 18:06:52 INFO MemoryStore: Block rdd_20_3 stored as bytes in memory (estimated size 1090.0 B, free 265.3 MB)\n",
      "15/10/04 18:06:52 INFO BlockManagerInfo: Added rdd_20_3 in memory on localhost:52448 (size: 1090.0 B, free: 265.4 MB)\n",
      "15/10/04 18:06:52 INFO Executor: Finished task 12.0 in stage 4.0 (TID 24). 1524 bytes result sent to driver\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 24) in 167 ms on localhost (13/16)\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 188, boot = 186, init = 1, finish = 1\n",
      "15/10/04 18:06:52 INFO Executor: Finished task 11.0 in stage 4.0 (TID 23). 1524 bytes result sent to driver\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 23) in 226 ms on localhost (14/16)\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 94, boot = 94, init = 0, finish = 0\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 101, boot = 5, init = 95, finish = 1\n",
      "15/10/04 18:06:52 INFO MemoryStore: ensureFreeSpace(1090) called with curMem=72207, maxMem=278302556\n",
      "15/10/04 18:06:52 INFO MemoryStore: Block rdd_21_3 stored as bytes in memory (estimated size 1090.0 B, free 265.3 MB)\n",
      "15/10/04 18:06:52 INFO BlockManagerInfo: Added rdd_21_3 in memory on localhost:52448 (size: 1090.0 B, free: 265.4 MB)\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 142, boot = 142, init = 0, finish = 0\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 144, boot = 123, init = 20, finish = 1\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 113, boot = 112, init = 1, finish = 0\n",
      "15/10/04 18:06:52 INFO MemoryStore: ensureFreeSpace(1090) called with curMem=73297, maxMem=278302556\n",
      "15/10/04 18:06:52 INFO MemoryStore: Block rdd_21_2 stored as bytes in memory (estimated size 1090.0 B, free 265.3 MB)\n",
      "15/10/04 18:06:52 INFO BlockManagerInfo: Added rdd_21_2 in memory on localhost:52448 (size: 1090.0 B, free: 265.4 MB)\n",
      "15/10/04 18:06:52 INFO Executor: Finished task 15.0 in stage 4.0 (TID 27). 1524 bytes result sent to driver\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Finished task 15.0 in stage 4.0 (TID 27) in 136 ms on localhost (15/16)\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 165, boot = 141, init = 12, finish = 12\n",
      "15/10/04 18:06:52 INFO Executor: Finished task 14.0 in stage 4.0 (TID 26). 1524 bytes result sent to driver\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 26) in 187 ms on localhost (16/16)\n",
      "15/10/04 18:06:52 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:06:52 INFO DAGScheduler: ShuffleMapStage 4 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.462 s\n",
      "15/10/04 18:06:52 INFO DAGScheduler: looking for newly runnable stages\n",
      "15/10/04 18:06:52 INFO DAGScheduler: running: Set()\n",
      "15/10/04 18:06:52 INFO DAGScheduler: waiting: Set(ResultStage 5)\n",
      "15/10/04 18:06:52 INFO DAGScheduler: failed: Set()\n",
      "15/10/04 18:06:52 INFO DAGScheduler: Missing parents for ResultStage 5: List()\n",
      "15/10/04 18:06:52 INFO DAGScheduler: Submitting ResultStage 5 (PythonRDD[29] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "15/10/04 18:06:52 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=74387, maxMem=278302556\n",
      "15/10/04 18:06:52 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 5.9 KB, free 265.3 MB)\n",
      "15/10/04 18:06:52 INFO MemoryStore: ensureFreeSpace(3412) called with curMem=80427, maxMem=278302556\n",
      "15/10/04 18:06:52 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.3 KB, free 265.3 MB)\n",
      "15/10/04 18:06:52 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:52448 (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:06:52 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:06:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (PythonRDD[29] at RDD at PythonRDD.scala:43)\n",
      "15/10/04 18:06:52 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 28, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:06:52 INFO Executor: Running task 0.0 in stage 5.0 (TID 28)\n",
      "15/10/04 18:06:52 INFO ShuffleBlockFetcherIterator: Getting 16 non-empty blocks out of 16 blocks\n",
      "15/10/04 18:06:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 36, boot = -57, init = 93, finish = 0\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 38, boot = -58, init = 96, finish = 0\n",
      "15/10/04 18:06:52 INFO Executor: Finished task 0.0 in stage 5.0 (TID 28). 1004 bytes result sent to driver\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 28) in 45 ms on localhost (1/1)\n",
      "15/10/04 18:06:52 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:06:52 INFO DAGScheduler: ResultStage 5 (runJob at PythonRDD.scala:366) finished in 0.046 s\n",
      "15/10/04 18:06:52 INFO DAGScheduler: Job 2 finished: runJob at PythonRDD.scala:366, took 0.552997 s\n",
      "15/10/04 18:06:52 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/04 18:06:52 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 159 bytes\n",
      "15/10/04 18:06:52 INFO DAGScheduler: Got job 3 (runJob at PythonRDD.scala:366) with 3 output partitions (allowLocal=true)\n",
      "15/10/04 18:06:52 INFO DAGScheduler: Final stage: ResultStage 7(runJob at PythonRDD.scala:366)\n",
      "15/10/04 18:06:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
      "15/10/04 18:06:52 INFO DAGScheduler: Missing parents: List()\n",
      "15/10/04 18:06:52 INFO DAGScheduler: Submitting ResultStage 7 (PythonRDD[30] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "15/10/04 18:06:52 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=83839, maxMem=278302556\n",
      "15/10/04 18:06:52 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 5.9 KB, free 265.3 MB)\n",
      "15/10/04 18:06:52 INFO MemoryStore: ensureFreeSpace(3412) called with curMem=89879, maxMem=278302556\n",
      "15/10/04 18:06:52 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.3 KB, free 265.3 MB)\n",
      "15/10/04 18:06:52 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:52448 (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:06:52 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:06:52 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 7 (PythonRDD[30] at RDD at PythonRDD.scala:43)\n",
      "15/10/04 18:06:52 INFO TaskSchedulerImpl: Adding task set 7.0 with 3 tasks\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 29, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 30, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 31, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:06:52 INFO Executor: Running task 2.0 in stage 7.0 (TID 31)\n",
      "15/10/04 18:06:52 INFO Executor: Running task 0.0 in stage 7.0 (TID 29)\n",
      "15/10/04 18:06:52 INFO Executor: Running task 1.0 in stage 7.0 (TID 30)\n",
      "15/10/04 18:06:52 INFO ShuffleBlockFetcherIterator: Getting 16 non-empty blocks out of 16 blocks\n",
      "15/10/04 18:06:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:06:52 INFO ShuffleBlockFetcherIterator: Getting 16 non-empty blocks out of 16 blocks\n",
      "15/10/04 18:06:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:06:52 INFO ShuffleBlockFetcherIterator: Getting 16 non-empty blocks out of 16 blocks\n",
      "15/10/04 18:06:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 8, boot = -15, init = 22, finish = 1\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 8, boot = -74, init = 82, finish = 0\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 8, boot = -71, init = 79, finish = 0\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 10, boot = -99, init = 109, finish = 0\n",
      "15/10/04 18:06:52 INFO Executor: Finished task 2.0 in stage 7.0 (TID 31). 995 bytes result sent to driver\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 31) in 15 ms on localhost (1/3)\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 20, boot = -75, init = 95, finish = 0\n",
      "15/10/04 18:06:52 INFO Executor: Finished task 0.0 in stage 7.0 (TID 29). 1004 bytes result sent to driver\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 29) in 27 ms on localhost (2/3)\n",
      "15/10/04 18:06:52 INFO PythonRDD: Times: total = 24, boot = -67, init = 91, finish = 0\n",
      "15/10/04 18:06:52 INFO Executor: Finished task 1.0 in stage 7.0 (TID 30). 995 bytes result sent to driver\n",
      "15/10/04 18:06:52 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 30) in 30 ms on localhost (3/3)\n",
      "15/10/04 18:06:52 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:06:52 INFO DAGScheduler: ResultStage 7 (runJob at PythonRDD.scala:366) finished in 0.031 s\n",
      "15/10/04 18:06:52 INFO DAGScheduler: Job 3 finished: runJob at PythonRDD.scala:366, took 0.045462 s\n",
      "15/10/04 18:06:52 INFO JobScheduler: Finished job streaming job 1444007211000 ms.0 from job set of time 1444007211000 ms\n",
      "15/10/04 18:06:52 INFO JobScheduler: Total delay: 1.540 s for time 1444007211000 ms (execution: 0.631 s)\n",
      "15/10/04 18:06:52 INFO PythonRDD: Removing RDD 18 from persistence list\n",
      "15/10/04 18:06:52 INFO UnionRDD: Removing RDD 13 from persistence list\n",
      "15/10/04 18:06:52 INFO BlockManager: Removing RDD 18\n",
      "15/10/04 18:06:52 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "15/10/04 18:06:52 INFO InputInfoTracker: remove old batch metadata: \n",
      "15/10/04 18:06:52 INFO BlockManager: Removing RDD 13\n",
      "15/10/04 18:06:53 INFO PythonTransformedDStream: Slicing from 1444007210000 ms to 1444007213000 ms (aligned to 1444007210000 ms and 1444007213000 ms)\n",
      "15/10/04 18:06:53 INFO JobScheduler: Added jobs for time 1444007213000 ms\n",
      "15/10/04 18:06:53 INFO JobScheduler: Starting job streaming job 1444007213000 ms.0 from job set of time 1444007213000 ms\n",
      "15/10/04 18:06:53 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/04 18:06:53 INFO DAGScheduler: Registering RDD 35 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/04 18:06:53 INFO DAGScheduler: Got job 4 (runJob at PythonRDD.scala:366) with 1 output partitions (allowLocal=true)\n",
      "15/10/04 18:06:53 INFO DAGScheduler: Final stage: ResultStage 9(runJob at PythonRDD.scala:366)\n",
      "15/10/04 18:06:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
      "15/10/04 18:06:53 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 8)\n",
      "15/10/04 18:06:53 INFO DAGScheduler: Submitting ShuffleMapStage 8 (PairwiseRDD[35] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "15/10/04 18:06:53 INFO MemoryStore: ensureFreeSpace(14888) called with curMem=93291, maxMem=278302556\n",
      "15/10/04 18:06:53 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 14.5 KB, free 265.3 MB)\n",
      "15/10/04 18:06:53 INFO MemoryStore: ensureFreeSpace(5852) called with curMem=108179, maxMem=278302556\n",
      "15/10/04 18:06:53 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.7 KB, free 265.3 MB)\n",
      "15/10/04 18:06:53 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:52448 (size: 5.7 KB, free: 265.4 MB)\n",
      "15/10/04 18:06:53 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:06:53 INFO DAGScheduler: Submitting 13 missing tasks from ShuffleMapStage 8 (PairwiseRDD[35] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/04 18:06:53 INFO TaskSchedulerImpl: Adding task set 8.0 with 13 tasks\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 32, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 33, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 34, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Starting task 3.0 in stage 8.0 (TID 35, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:53 INFO Executor: Running task 1.0 in stage 8.0 (TID 33)\n",
      "15/10/04 18:06:53 INFO Executor: Running task 2.0 in stage 8.0 (TID 34)\n",
      "15/10/04 18:06:53 INFO Executor: Running task 0.0 in stage 8.0 (TID 32)\n",
      "15/10/04 18:06:53 INFO Executor: Running task 3.0 in stage 8.0 (TID 35)\n",
      "15/10/04 18:06:53 INFO BlockManager: Found block rdd_20_1 locally\n",
      "15/10/04 18:06:53 INFO BlockManager: Found block rdd_20_3 locally\n",
      "15/10/04 18:06:53 INFO BlockManager: Found block rdd_20_2 locally\n",
      "15/10/04 18:06:53 INFO BlockManager: Found block rdd_20_0 locally\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 3, boot = -578, init = 580, finish = 1\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 3, boot = -507, init = 509, finish = 1\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 2, boot = -528, init = 529, finish = 1\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 6, boot = -503, init = 508, finish = 1\n",
      "15/10/04 18:06:53 INFO Executor: Finished task 3.0 in stage 8.0 (TID 35). 2066 bytes result sent to driver\n",
      "15/10/04 18:06:53 INFO Executor: Finished task 0.0 in stage 8.0 (TID 32). 2066 bytes result sent to driver\n",
      "15/10/04 18:06:53 INFO Executor: Finished task 2.0 in stage 8.0 (TID 34). 2066 bytes result sent to driver\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Starting task 4.0 in stage 8.0 (TID 36, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:53 INFO Executor: Running task 4.0 in stage 8.0 (TID 36)\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Starting task 5.0 in stage 8.0 (TID 37, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:53 INFO Executor: Running task 5.0 in stage 8.0 (TID 37)\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Starting task 6.0 in stage 8.0 (TID 38, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Finished task 3.0 in stage 8.0 (TID 35) in 34 ms on localhost (1/13)\n",
      "15/10/04 18:06:53 INFO Executor: Running task 6.0 in stage 8.0 (TID 38)\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 32) in 37 ms on localhost (2/13)\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 34) in 36 ms on localhost (3/13)\n",
      "15/10/04 18:06:53 INFO BlockManager: Found block rdd_21_1 locally\n",
      "15/10/04 18:06:53 INFO BlockManager: Found block rdd_21_0 locally\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 3, boot = -560, init = 563, finish = 0\n",
      "15/10/04 18:06:53 INFO BlockManager: Found block rdd_21_2 locally\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 4, boot = -536, init = 539, finish = 1\n",
      "15/10/04 18:06:53 INFO Executor: Finished task 1.0 in stage 8.0 (TID 33). 2066 bytes result sent to driver\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Starting task 7.0 in stage 8.0 (TID 39, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 33) in 59 ms on localhost (4/13)\n",
      "15/10/04 18:06:53 INFO Executor: Running task 7.0 in stage 8.0 (TID 39)\n",
      "15/10/04 18:06:53 INFO BlockManager: Found block rdd_21_3 locally\n",
      "15/10/04 18:06:53 INFO Executor: Finished task 5.0 in stage 8.0 (TID 37). 2066 bytes result sent to driver\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Starting task 8.0 in stage 8.0 (TID 40, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:53 INFO Executor: Running task 8.0 in stage 8.0 (TID 40)\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Finished task 5.0 in stage 8.0 (TID 37) in 47 ms on localhost (5/13)\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 8, boot = -540, init = 547, finish = 1\n",
      "15/10/04 18:06:53 INFO CacheManager: Partition rdd_31_0 not found, computing it\n",
      "15/10/04 18:06:53 INFO Executor: Finished task 4.0 in stage 8.0 (TID 36). 2066 bytes result sent to driver\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Starting task 9.0 in stage 8.0 (TID 41, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:53 INFO Executor: Running task 9.0 in stage 8.0 (TID 41)\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Finished task 4.0 in stage 8.0 (TID 36) in 64 ms on localhost (6/13)\n",
      "15/10/04 18:06:53 INFO Executor: Finished task 6.0 in stage 8.0 (TID 38). 2066 bytes result sent to driver\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Starting task 10.0 in stage 8.0 (TID 42, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:53 INFO Executor: Running task 10.0 in stage 8.0 (TID 42)\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Finished task 6.0 in stage 8.0 (TID 38) in 63 ms on localhost (7/13)\n",
      "15/10/04 18:06:53 INFO CacheManager: Partition rdd_31_1 not found, computing it\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 27, boot = 21, init = 1, finish = 5\n",
      "15/10/04 18:06:53 INFO CacheManager: Partition rdd_31_2 not found, computing it\n",
      "15/10/04 18:06:53 INFO Executor: Finished task 7.0 in stage 8.0 (TID 39). 2066 bytes result sent to driver\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Starting task 11.0 in stage 8.0 (TID 43, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:53 INFO Executor: Running task 11.0 in stage 8.0 (TID 43)\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Finished task 7.0 in stage 8.0 (TID 39) in 59 ms on localhost (8/13)\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 6, boot = 4, init = 2, finish = 0\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 14, boot = 4, init = 9, finish = 1\n",
      "15/10/04 18:06:53 INFO MemoryStore: ensureFreeSpace(1090) called with curMem=114031, maxMem=278302556\n",
      "15/10/04 18:06:53 INFO MemoryStore: Block rdd_31_2 stored as bytes in memory (estimated size 1090.0 B, free 265.3 MB)\n",
      "15/10/04 18:06:53 INFO BlockManagerInfo: Added rdd_31_2 in memory on localhost:52448 (size: 1090.0 B, free: 265.4 MB)\n",
      "15/10/04 18:06:53 INFO CacheManager: Partition rdd_31_3 not found, computing it\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 8, boot = 6, init = 2, finish = 0\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 17, boot = 6, init = 10, finish = 1\n",
      "15/10/04 18:06:53 INFO MemoryStore: ensureFreeSpace(1090) called with curMem=115121, maxMem=278302556\n",
      "15/10/04 18:06:53 INFO MemoryStore: Block rdd_31_3 stored as bytes in memory (estimated size 1090.0 B, free 265.3 MB)\n",
      "15/10/04 18:06:53 INFO BlockManagerInfo: Added rdd_31_3 in memory on localhost:52448 (size: 1090.0 B, free: 265.4 MB)\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 68, boot = 68, init = 0, finish = 0\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 69, boot = 22, init = 47, finish = 0\n",
      "15/10/04 18:06:53 INFO MemoryStore: ensureFreeSpace(1090) called with curMem=116211, maxMem=278302556\n",
      "15/10/04 18:06:53 INFO MemoryStore: Block rdd_31_0 stored as bytes in memory (estimated size 1090.0 B, free 265.3 MB)\n",
      "15/10/04 18:06:53 INFO BlockManagerInfo: Added rdd_31_0 in memory on localhost:52448 (size: 1090.0 B, free: 265.4 MB)\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 77, boot = 66, init = 10, finish = 1\n",
      "15/10/04 18:06:53 INFO Executor: Finished task 8.0 in stage 8.0 (TID 40). 1524 bytes result sent to driver\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Starting task 12.0 in stage 8.0 (TID 44, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Finished task 8.0 in stage 8.0 (TID 40) in 103 ms on localhost (9/13)\n",
      "15/10/04 18:06:53 INFO Executor: Running task 12.0 in stage 8.0 (TID 44)\n",
      "15/10/04 18:06:53 INFO CacheManager: Partition rdd_32_0 not found, computing it\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 93, boot = 92, init = 0, finish = 1\n",
      "15/10/04 18:06:53 INFO Executor: Finished task 10.0 in stage 8.0 (TID 42). 1524 bytes result sent to driver\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Finished task 10.0 in stage 8.0 (TID 42) in 112 ms on localhost (10/13)\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 126, boot = 126, init = 0, finish = 0\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 40, boot = 40, init = 0, finish = 0\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 129, boot = 36, init = 92, finish = 1\n",
      "15/10/04 18:06:53 INFO MemoryStore: ensureFreeSpace(26) called with curMem=117301, maxMem=278302556\n",
      "15/10/04 18:06:53 INFO MemoryStore: Block rdd_32_0 stored as bytes in memory (estimated size 26.0 B, free 265.3 MB)\n",
      "15/10/04 18:06:53 INFO MemoryStore: ensureFreeSpace(1090) called with curMem=117327, maxMem=278302556\n",
      "15/10/04 18:06:53 INFO MemoryStore: Block rdd_31_1 stored as bytes in memory (estimated size 1090.0 B, free 265.3 MB)\n",
      "15/10/04 18:06:53 INFO BlockManagerInfo: Added rdd_32_0 in memory on localhost:52448 (size: 26.0 B, free: 265.4 MB)\n",
      "15/10/04 18:06:53 INFO BlockManagerInfo: Added rdd_31_1 in memory on localhost:52448 (size: 1090.0 B, free: 265.4 MB)\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 136, boot = 46, init = 90, finish = 0\n",
      "15/10/04 18:06:53 INFO Executor: Finished task 9.0 in stage 8.0 (TID 41). 1524 bytes result sent to driver\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Finished task 9.0 in stage 8.0 (TID 41) in 160 ms on localhost (11/13)\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 110, boot = 109, init = 1, finish = 0\n",
      "15/10/04 18:06:53 INFO Executor: Finished task 12.0 in stage 8.0 (TID 44). 1524 bytes result sent to driver\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Finished task 12.0 in stage 8.0 (TID 44) in 128 ms on localhost (12/13)\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 185, boot = 184, init = 0, finish = 1\n",
      "15/10/04 18:06:53 INFO Executor: Finished task 11.0 in stage 8.0 (TID 43). 1524 bytes result sent to driver\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Finished task 11.0 in stage 8.0 (TID 43) in 213 ms on localhost (13/13)\n",
      "15/10/04 18:06:53 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:06:53 INFO DAGScheduler: ShuffleMapStage 8 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.330 s\n",
      "15/10/04 18:06:53 INFO DAGScheduler: looking for newly runnable stages\n",
      "15/10/04 18:06:53 INFO DAGScheduler: running: Set()\n",
      "15/10/04 18:06:53 INFO DAGScheduler: waiting: Set(ResultStage 9)\n",
      "15/10/04 18:06:53 INFO DAGScheduler: failed: Set()\n",
      "15/10/04 18:06:53 INFO DAGScheduler: Missing parents for ResultStage 9: List()\n",
      "15/10/04 18:06:53 INFO DAGScheduler: Submitting ResultStage 9 (PythonRDD[39] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "15/10/04 18:06:53 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=118417, maxMem=278302556\n",
      "15/10/04 18:06:53 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 5.9 KB, free 265.3 MB)\n",
      "15/10/04 18:06:53 INFO MemoryStore: ensureFreeSpace(3414) called with curMem=124457, maxMem=278302556\n",
      "15/10/04 18:06:53 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.3 KB, free 265.3 MB)\n",
      "15/10/04 18:06:53 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:52448 (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:06:53 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:06:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (PythonRDD[39] at RDD at PythonRDD.scala:43)\n",
      "15/10/04 18:06:53 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 45, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:06:53 INFO Executor: Running task 0.0 in stage 9.0 (TID 45)\n",
      "15/10/04 18:06:53 INFO ShuffleBlockFetcherIterator: Getting 12 non-empty blocks out of 13 blocks\n",
      "15/10/04 18:06:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 43, boot = -74, init = 117, finish = 0\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 44, boot = -121, init = 165, finish = 0\n",
      "15/10/04 18:06:53 INFO Executor: Finished task 0.0 in stage 9.0 (TID 45). 1004 bytes result sent to driver\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 45) in 51 ms on localhost (1/1)\n",
      "15/10/04 18:06:53 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:06:53 INFO DAGScheduler: ResultStage 9 (runJob at PythonRDD.scala:366) finished in 0.052 s\n",
      "15/10/04 18:06:53 INFO DAGScheduler: Job 4 finished: runJob at PythonRDD.scala:366, took 0.418176 s\n",
      "15/10/04 18:06:53 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/04 18:06:53 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 161 bytes\n",
      "15/10/04 18:06:53 INFO DAGScheduler: Got job 5 (runJob at PythonRDD.scala:366) with 3 output partitions (allowLocal=true)\n",
      "15/10/04 18:06:53 INFO DAGScheduler: Final stage: ResultStage 11(runJob at PythonRDD.scala:366)\n",
      "15/10/04 18:06:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)\n",
      "15/10/04 18:06:53 INFO DAGScheduler: Missing parents: List()\n",
      "15/10/04 18:06:53 INFO DAGScheduler: Submitting ResultStage 11 (PythonRDD[40] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "15/10/04 18:06:53 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=127871, maxMem=278302556\n",
      "15/10/04 18:06:53 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 5.9 KB, free 265.3 MB)\n",
      "15/10/04 18:06:53 INFO MemoryStore: ensureFreeSpace(3414) called with curMem=133911, maxMem=278302556\n",
      "15/10/04 18:06:53 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.3 KB, free 265.3 MB)\n",
      "15/10/04 18:06:53 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:52448 (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:06:53 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:06:53 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 11 (PythonRDD[40] at RDD at PythonRDD.scala:43)\n",
      "15/10/04 18:06:53 INFO TaskSchedulerImpl: Adding task set 11.0 with 3 tasks\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 46, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 47, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Starting task 2.0 in stage 11.0 (TID 48, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:06:53 INFO Executor: Running task 1.0 in stage 11.0 (TID 47)\n",
      "15/10/04 18:06:53 INFO Executor: Running task 0.0 in stage 11.0 (TID 46)\n",
      "15/10/04 18:06:53 INFO Executor: Running task 2.0 in stage 11.0 (TID 48)\n",
      "15/10/04 18:06:53 INFO ShuffleBlockFetcherIterator: Getting 12 non-empty blocks out of 13 blocks\n",
      "15/10/04 18:06:53 INFO ShuffleBlockFetcherIterator: Getting 12 non-empty blocks out of 13 blocks\n",
      "15/10/04 18:06:53 INFO ShuffleBlockFetcherIterator: Getting 12 non-empty blocks out of 13 blocks\n",
      "15/10/04 18:06:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "15/10/04 18:06:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:06:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 10, boot = -98, init = 107, finish = 1\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 9, boot = -14, init = 23, finish = 0\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 11, boot = -131, init = 142, finish = 0\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 11, boot = -156, init = 166, finish = 1\n",
      "15/10/04 18:06:53 INFO Executor: Finished task 2.0 in stage 11.0 (TID 48). 995 bytes result sent to driver\n",
      "15/10/04 18:06:53 INFO Executor: Finished task 1.0 in stage 11.0 (TID 47). 995 bytes result sent to driver\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Finished task 2.0 in stage 11.0 (TID 48) in 16 ms on localhost (1/3)\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 47) in 18 ms on localhost (2/3)\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 21, boot = -91, init = 112, finish = 0\n",
      "15/10/04 18:06:53 INFO PythonRDD: Times: total = 22, boot = -154, init = 176, finish = 0\n",
      "15/10/04 18:06:53 INFO Executor: Finished task 0.0 in stage 11.0 (TID 46). 1004 bytes result sent to driver\n",
      "15/10/04 18:06:53 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 46) in 28 ms on localhost (3/3)\n",
      "15/10/04 18:06:53 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:06:53 INFO DAGScheduler: ResultStage 11 (runJob at PythonRDD.scala:366) finished in 0.030 s\n",
      "15/10/04 18:06:53 INFO DAGScheduler: Job 5 finished: runJob at PythonRDD.scala:366, took 0.042332 s\n",
      "15/10/04 18:06:53 INFO JobScheduler: Finished job streaming job 1444007213000 ms.0 from job set of time 1444007213000 ms\n",
      "15/10/04 18:06:53 INFO JobScheduler: Total delay: 0.522 s for time 1444007213000 ms (execution: 0.489 s)\n",
      "15/10/04 18:06:53 INFO PythonRDD: Removing RDD 27 from persistence list\n",
      "15/10/04 18:06:53 INFO BlockManager: Removing RDD 27\n",
      "15/10/04 18:06:53 INFO UnionRDD: Removing RDD 22 from persistence list\n",
      "15/10/04 18:06:53 INFO BlockManager: Removing RDD 22\n",
      "15/10/04 18:06:53 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "15/10/04 18:06:53 INFO InputInfoTracker: remove old batch metadata: \n",
      "15/10/04 18:06:54 INFO PythonTransformedDStream: Time 1444007214000 ms is invalid as zeroTime is 1444007207000 ms and slideDuration is 2000 ms and difference is 7000 ms\n",
      "15/10/04 18:06:54 INFO JobScheduler: No jobs added for time 1444007214000 ms\n",
      "15/10/04 18:06:55 INFO PythonTransformedDStream: Slicing from 1444007212000 ms to 1444007215000 ms (aligned to 1444007212000 ms and 1444007215000 ms)\n",
      "15/10/04 18:06:55 INFO JobScheduler: Added jobs for time 1444007215000 ms\n",
      "15/10/04 18:06:55 INFO JobScheduler: Starting job streaming job 1444007215000 ms.0 from job set of time 1444007215000 ms\n",
      "15/10/04 18:06:55 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/04 18:06:55 INFO DAGScheduler: Registering RDD 45 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/04 18:06:55 INFO DAGScheduler: Got job 6 (runJob at PythonRDD.scala:366) with 1 output partitions (allowLocal=true)\n",
      "15/10/04 18:06:55 INFO DAGScheduler: Final stage: ResultStage 13(runJob at PythonRDD.scala:366)\n",
      "15/10/04 18:06:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\n",
      "15/10/04 18:06:55 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 12)\n",
      "15/10/04 18:06:55 INFO DAGScheduler: Submitting ShuffleMapStage 12 (PairwiseRDD[45] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "15/10/04 18:06:55 INFO MemoryStore: ensureFreeSpace(12728) called with curMem=137325, maxMem=278302556\n",
      "15/10/04 18:06:55 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 12.4 KB, free 265.3 MB)\n",
      "15/10/04 18:06:55 INFO MemoryStore: ensureFreeSpace(5674) called with curMem=150053, maxMem=278302556\n",
      "15/10/04 18:06:55 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 5.5 KB, free 265.3 MB)\n",
      "15/10/04 18:06:55 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:52448 (size: 5.5 KB, free: 265.3 MB)\n",
      "15/10/04 18:06:55 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:06:55 INFO DAGScheduler: Submitting 7 missing tasks from ShuffleMapStage 12 (PairwiseRDD[45] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/04 18:06:55 INFO TaskSchedulerImpl: Adding task set 12.0 with 7 tasks\n",
      "15/10/04 18:06:55 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 49, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:55 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 50, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:55 INFO TaskSetManager: Starting task 2.0 in stage 12.0 (TID 51, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:55 INFO TaskSetManager: Starting task 3.0 in stage 12.0 (TID 52, localhost, PROCESS_LOCAL, 1526 bytes)\n",
      "15/10/04 18:06:55 INFO Executor: Running task 0.0 in stage 12.0 (TID 49)\n",
      "15/10/04 18:06:55 INFO Executor: Running task 1.0 in stage 12.0 (TID 50)\n",
      "15/10/04 18:06:55 INFO Executor: Running task 3.0 in stage 12.0 (TID 52)\n",
      "15/10/04 18:06:55 INFO Executor: Running task 2.0 in stage 12.0 (TID 51)\n",
      "15/10/04 18:06:55 INFO BlockManager: Found block rdd_31_3 locally\n",
      "15/10/04 18:06:55 INFO BlockManager: Found block rdd_31_0 locally\n",
      "15/10/04 18:06:55 INFO PythonRDD: Times: total = 3, boot = -1590, init = 1593, finish = 0\n",
      "15/10/04 18:06:55 INFO BlockManager: Found block rdd_31_1 locally\n",
      "15/10/04 18:06:55 INFO PythonRDD: Times: total = 9, boot = -1525, init = 1533, finish = 1\n",
      "15/10/04 18:06:55 INFO BlockManager: Found block rdd_31_2 locally\n",
      "15/10/04 18:06:55 INFO PythonRDD: Times: total = 17, boot = -1513, init = 1529, finish = 1\n",
      "15/10/04 18:06:55 INFO PythonRDD: Times: total = 22, boot = -1522, init = 1543, finish = 1\n",
      "15/10/04 18:06:55 INFO Executor: Finished task 3.0 in stage 12.0 (TID 52). 2066 bytes result sent to driver\n",
      "15/10/04 18:06:55 INFO TaskSetManager: Starting task 4.0 in stage 12.0 (TID 53, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:06:55 INFO Executor: Running task 4.0 in stage 12.0 (TID 53)\n",
      "15/10/04 18:06:55 INFO TaskSetManager: Finished task 3.0 in stage 12.0 (TID 52) in 40 ms on localhost (1/7)\n",
      "15/10/04 18:06:55 INFO BlockManager: Found block rdd_32_0 locally\n",
      "15/10/04 18:06:55 INFO PythonRDD: Times: total = 2, boot = -1575, init = 1577, finish = 0\n",
      "15/10/04 18:06:55 INFO Executor: Finished task 0.0 in stage 12.0 (TID 49). 2066 bytes result sent to driver\n",
      "15/10/04 18:06:55 INFO Executor: Finished task 4.0 in stage 12.0 (TID 53). 2066 bytes result sent to driver\n",
      "15/10/04 18:06:55 INFO TaskSetManager: Starting task 5.0 in stage 12.0 (TID 54, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:06:55 INFO TaskSetManager: Starting task 6.0 in stage 12.0 (TID 55, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:06:55 INFO Executor: Running task 5.0 in stage 12.0 (TID 54)\n",
      "15/10/04 18:06:55 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 49) in 58 ms on localhost (2/7)\n",
      "15/10/04 18:06:55 INFO TaskSetManager: Finished task 4.0 in stage 12.0 (TID 53) in 19 ms on localhost (3/7)\n",
      "15/10/04 18:06:55 INFO Executor: Running task 6.0 in stage 12.0 (TID 55)\n",
      "15/10/04 18:06:55 INFO Executor: Finished task 1.0 in stage 12.0 (TID 50). 2066 bytes result sent to driver\n",
      "15/10/04 18:06:55 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 50) in 67 ms on localhost (4/7)\n",
      "15/10/04 18:06:55 INFO Executor: Finished task 2.0 in stage 12.0 (TID 51). 2066 bytes result sent to driver\n",
      "15/10/04 18:06:55 INFO TaskSetManager: Finished task 2.0 in stage 12.0 (TID 51) in 70 ms on localhost (5/7)\n",
      "15/10/04 18:06:55 INFO CacheManager: Partition rdd_42_0 not found, computing it\n",
      "15/10/04 18:06:55 INFO CacheManager: Partition rdd_41_0 not found, computing it\n",
      "15/10/04 18:06:55 INFO PythonRDD: Times: total = 1, boot = -36, init = 37, finish = 0\n",
      "15/10/04 18:06:55 INFO MemoryStore: ensureFreeSpace(26) called with curMem=155727, maxMem=278302556\n",
      "15/10/04 18:06:55 INFO PythonRDD: Times: total = 1, boot = -24, init = 25, finish = 0\n",
      "15/10/04 18:06:55 INFO MemoryStore: Block rdd_42_0 stored as bytes in memory (estimated size 26.0 B, free 265.3 MB)\n",
      "15/10/04 18:06:55 INFO MemoryStore: ensureFreeSpace(26) called with curMem=155753, maxMem=278302556\n",
      "15/10/04 18:06:55 INFO BlockManagerInfo: Added rdd_42_0 in memory on localhost:52448 (size: 26.0 B, free: 265.3 MB)\n",
      "15/10/04 18:06:55 INFO MemoryStore: Block rdd_41_0 stored as bytes in memory (estimated size 26.0 B, free 265.3 MB)\n",
      "15/10/04 18:06:55 INFO BlockManagerInfo: Added rdd_41_0 in memory on localhost:52448 (size: 26.0 B, free: 265.3 MB)\n",
      "15/10/04 18:06:55 INFO PythonRDD: Times: total = 6, boot = -1574, init = 1579, finish = 1\n",
      "15/10/04 18:06:55 INFO Executor: Finished task 6.0 in stage 12.0 (TID 55). 1524 bytes result sent to driver\n",
      "15/10/04 18:06:55 INFO TaskSetManager: Finished task 6.0 in stage 12.0 (TID 55) in 28 ms on localhost (6/7)\n",
      "15/10/04 18:06:55 INFO PythonRDD: Times: total = 6, boot = -1597, init = 1603, finish = 0\n",
      "15/10/04 18:06:55 INFO Executor: Finished task 5.0 in stage 12.0 (TID 54). 1524 bytes result sent to driver\n",
      "15/10/04 18:06:55 INFO TaskSetManager: Finished task 5.0 in stage 12.0 (TID 54) in 35 ms on localhost (7/7)\n",
      "15/10/04 18:06:55 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:06:55 INFO DAGScheduler: ShuffleMapStage 12 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.094 s\n",
      "15/10/04 18:06:55 INFO DAGScheduler: looking for newly runnable stages\n",
      "15/10/04 18:06:55 INFO DAGScheduler: running: Set()\n",
      "15/10/04 18:06:55 INFO DAGScheduler: waiting: Set(ResultStage 13)\n",
      "15/10/04 18:06:55 INFO DAGScheduler: failed: Set()\n",
      "15/10/04 18:06:55 INFO DAGScheduler: Missing parents for ResultStage 13: List()\n",
      "15/10/04 18:06:55 INFO DAGScheduler: Submitting ResultStage 13 (PythonRDD[49] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "15/10/04 18:06:55 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=155779, maxMem=278302556\n",
      "15/10/04 18:06:55 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.9 KB, free 265.3 MB)\n",
      "15/10/04 18:06:55 INFO MemoryStore: ensureFreeSpace(3412) called with curMem=161819, maxMem=278302556\n",
      "15/10/04 18:06:55 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.3 KB, free 265.3 MB)\n",
      "15/10/04 18:06:55 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:52448 (size: 3.3 KB, free: 265.3 MB)\n",
      "15/10/04 18:06:55 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:06:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (PythonRDD[49] at RDD at PythonRDD.scala:43)\n",
      "15/10/04 18:06:55 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks\n",
      "15/10/04 18:06:55 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 56, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:06:55 INFO Executor: Running task 0.0 in stage 13.0 (TID 56)\n",
      "15/10/04 18:06:55 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 7 blocks\n",
      "15/10/04 18:06:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "15/10/04 18:06:55 INFO PythonRDD: Times: total = 18, boot = -54, init = 72, finish = 0\n",
      "15/10/04 18:06:55 INFO PythonRDD: Times: total = 19, boot = -56, init = 75, finish = 0\n",
      "15/10/04 18:06:55 INFO Executor: Finished task 0.0 in stage 13.0 (TID 56). 1001 bytes result sent to driver\n",
      "15/10/04 18:06:55 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 56) in 24 ms on localhost (1/1)\n",
      "15/10/04 18:06:55 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:06:55 INFO DAGScheduler: ResultStage 13 (runJob at PythonRDD.scala:366) finished in 0.026 s\n",
      "15/10/04 18:06:55 INFO DAGScheduler: Job 6 finished: runJob at PythonRDD.scala:366, took 0.147229 s\n",
      "15/10/04 18:06:55 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/04 18:06:55 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 3 is 160 bytes\n",
      "15/10/04 18:06:55 INFO DAGScheduler: Got job 7 (runJob at PythonRDD.scala:366) with 3 output partitions (allowLocal=true)\n",
      "15/10/04 18:06:55 INFO DAGScheduler: Final stage: ResultStage 15(runJob at PythonRDD.scala:366)\n",
      "15/10/04 18:06:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)\n",
      "15/10/04 18:06:55 INFO DAGScheduler: Missing parents: List()\n",
      "15/10/04 18:06:55 INFO DAGScheduler: Submitting ResultStage 15 (PythonRDD[50] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "15/10/04 18:06:55 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=165231, maxMem=278302556\n",
      "15/10/04 18:06:55 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 5.9 KB, free 265.2 MB)\n",
      "15/10/04 18:06:55 INFO MemoryStore: ensureFreeSpace(3412) called with curMem=171271, maxMem=278302556\n",
      "15/10/04 18:06:55 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 3.3 KB, free 265.2 MB)\n",
      "15/10/04 18:06:55 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:52448 (size: 3.3 KB, free: 265.3 MB)\n",
      "15/10/04 18:06:55 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:06:55 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 15 (PythonRDD[50] at RDD at PythonRDD.scala:43)\n",
      "15/10/04 18:06:55 INFO TaskSchedulerImpl: Adding task set 15.0 with 3 tasks\n",
      "15/10/04 18:06:55 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 57, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:06:55 INFO TaskSetManager: Starting task 1.0 in stage 15.0 (TID 58, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:06:55 INFO TaskSetManager: Starting task 2.0 in stage 15.0 (TID 59, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:06:55 INFO Executor: Running task 2.0 in stage 15.0 (TID 59)\n",
      "15/10/04 18:06:55 INFO Executor: Running task 0.0 in stage 15.0 (TID 57)\n",
      "15/10/04 18:06:55 INFO Executor: Running task 1.0 in stage 15.0 (TID 58)\n",
      "15/10/04 18:06:55 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 7 blocks\n",
      "15/10/04 18:06:55 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 7 blocks\n",
      "15/10/04 18:06:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "15/10/04 18:06:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "15/10/04 18:06:55 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 7 blocks\n",
      "15/10/04 18:06:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "15/10/04 18:06:55 INFO PythonRDD: Times: total = 4, boot = -42, init = 46, finish = 0\n",
      "15/10/04 18:06:55 INFO PythonRDD: Times: total = 4, boot = -43, init = 47, finish = 0\n",
      "15/10/04 18:06:55 INFO PythonRDD: Times: total = 4, boot = -11, init = 15, finish = 0\n",
      "15/10/04 18:06:55 INFO PythonRDD: Times: total = 5, boot = -84, init = 89, finish = 0\n",
      "15/10/04 18:06:55 INFO Executor: Finished task 1.0 in stage 15.0 (TID 58). 993 bytes result sent to driver\n",
      "15/10/04 18:06:55 INFO TaskSetManager: Finished task 1.0 in stage 15.0 (TID 58) in 10 ms on localhost (1/3)\n",
      "15/10/04 18:06:55 INFO PythonRDD: Times: total = 15, boot = -38, init = 53, finish = 0\n",
      "15/10/04 18:06:55 INFO Executor: Finished task 0.0 in stage 15.0 (TID 57). 1001 bytes result sent to driver\n",
      "15/10/04 18:06:55 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 57) in 21 ms on localhost (2/3)\n",
      "15/10/04 18:06:55 INFO PythonRDD: Times: total = 19, boot = -37, init = 56, finish = 0\n",
      "15/10/04 18:06:55 INFO Executor: Finished task 2.0 in stage 15.0 (TID 59). 993 bytes result sent to driver\n",
      "15/10/04 18:06:55 INFO TaskSetManager: Finished task 2.0 in stage 15.0 (TID 59) in 23 ms on localhost (3/3)\n",
      "15/10/04 18:06:55 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:06:55 INFO DAGScheduler: ResultStage 15 (runJob at PythonRDD.scala:366) finished in 0.025 s\n",
      "15/10/04 18:06:55 INFO DAGScheduler: Job 7 finished: runJob at PythonRDD.scala:366, took 0.036981 s\n",
      "15/10/04 18:06:55 INFO JobScheduler: Finished job streaming job 1444007215000 ms.0 from job set of time 1444007215000 ms\n",
      "15/10/04 18:06:55 INFO JobScheduler: Total delay: 0.252 s for time 1444007215000 ms (execution: 0.211 s)\n",
      "15/10/04 18:06:55 INFO PythonRDD: Removing RDD 38 from persistence list\n",
      "15/10/04 18:06:55 INFO UnionRDD: Removing RDD 33 from persistence list\n",
      "15/10/04 18:06:55 INFO BlockManager: Removing RDD 38\n",
      "15/10/04 18:06:55 INFO PythonRDD: Removing RDD 12 from persistence list\n",
      "15/10/04 18:06:55 INFO BlockManager: Removing RDD 33\n",
      "15/10/04 18:06:55 INFO PythonRDD: Removing RDD 11 from persistence list\n",
      "15/10/04 18:06:55 INFO BlockManager: Removing RDD 12\n",
      "15/10/04 18:06:55 INFO BlockManager: Removing RDD 11\n",
      "15/10/04 18:06:55 INFO PythonRDD: Removing RDD 6 from persistence list\n",
      "15/10/04 18:06:55 INFO PythonRDD: Removing RDD 5 from persistence list\n",
      "15/10/04 18:06:55 INFO BlockManager: Removing RDD 6\n",
      "15/10/04 18:06:55 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "15/10/04 18:06:55 INFO BlockManager: Removing RDD 5\n",
      "15/10/04 18:06:55 INFO InputInfoTracker: remove old batch metadata: \n",
      "15/10/04 18:06:56 INFO PythonTransformedDStream: Time 1444007216000 ms is invalid as zeroTime is 1444007207000 ms and slideDuration is 2000 ms and difference is 9000 ms\n",
      "15/10/04 18:06:56 INFO JobScheduler: No jobs added for time 1444007216000 ms\n",
      "15/10/04 18:06:57 INFO PythonTransformedDStream: Slicing from 1444007214000 ms to 1444007217000 ms (aligned to 1444007214000 ms and 1444007217000 ms)\n",
      "15/10/04 18:06:57 INFO JobScheduler: Added jobs for time 1444007217000 ms\n",
      "15/10/04 18:06:57 INFO JobScheduler: Starting job streaming job 1444007217000 ms.0 from job set of time 1444007217000 ms\n",
      "15/10/04 18:06:57 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/04 18:06:57 INFO DAGScheduler: Registering RDD 55 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/04 18:06:57 INFO DAGScheduler: Got job 8 (runJob at PythonRDD.scala:366) with 1 output partitions (allowLocal=true)\n",
      "15/10/04 18:06:57 INFO DAGScheduler: Final stage: ResultStage 17(runJob at PythonRDD.scala:366)\n",
      "15/10/04 18:06:57 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\n",
      "15/10/04 18:06:57 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 16)\n",
      "15/10/04 18:06:57 INFO ContextCleaner: Cleaned shuffle 2\n",
      "15/10/04 18:06:57 INFO DAGScheduler: Submitting ShuffleMapStage 16 (PairwiseRDD[55] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "15/10/04 18:06:57 INFO MemoryStore: ensureFreeSpace(11648) called with curMem=156511, maxMem=278302556\n",
      "15/10/04 18:06:57 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 11.4 KB, free 265.2 MB)\n",
      "15/10/04 18:06:57 INFO BlockManagerInfo: Removed broadcast_11_piece0 on localhost:52448 in memory (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:06:57 INFO MemoryStore: ensureFreeSpace(5222) called with curMem=168159, maxMem=278302556\n",
      "15/10/04 18:06:57 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 5.1 KB, free 265.2 MB)\n",
      "15/10/04 18:06:57 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:52448 (size: 5.1 KB, free: 265.3 MB)\n",
      "15/10/04 18:06:57 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:06:57 INFO BlockManagerInfo: Removed broadcast_10_piece0 on localhost:52448 in memory (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:06:57 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 16 (PairwiseRDD[55] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/04 18:06:57 INFO TaskSchedulerImpl: Adding task set 16.0 with 4 tasks\n",
      "15/10/04 18:06:57 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:52448 in memory (size: 5.5 KB, free: 265.4 MB)\n",
      "15/10/04 18:06:57 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 60, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:06:57 INFO TaskSetManager: Starting task 1.0 in stage 16.0 (TID 61, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:06:57 INFO TaskSetManager: Starting task 2.0 in stage 16.0 (TID 62, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:06:57 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:52448 in memory (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:06:57 INFO TaskSetManager: Starting task 3.0 in stage 16.0 (TID 63, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:06:57 INFO Executor: Running task 0.0 in stage 16.0 (TID 60)\n",
      "15/10/04 18:06:57 INFO Executor: Running task 1.0 in stage 16.0 (TID 61)\n",
      "15/10/04 18:06:57 INFO Executor: Running task 2.0 in stage 16.0 (TID 62)\n",
      "15/10/04 18:06:57 INFO Executor: Running task 3.0 in stage 16.0 (TID 63)\n",
      "15/10/04 18:06:57 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:52448 in memory (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:06:57 INFO BlockManager: Found block rdd_42_0 locally\n",
      "15/10/04 18:06:57 INFO PythonRDD: Times: total = 2, boot = -1873, init = 1875, finish = 0\n",
      "15/10/04 18:06:57 INFO BlockManager: Found block rdd_41_0 locally\n",
      "15/10/04 18:06:57 INFO CacheManager: Partition rdd_51_0 not found, computing it\n",
      "15/10/04 18:06:57 INFO CacheManager: Partition rdd_52_0 not found, computing it\n",
      "15/10/04 18:06:57 INFO PythonRDD: Times: total = 3, boot = -1808, init = 1811, finish = 0\n",
      "15/10/04 18:06:57 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:52448 in memory (size: 5.7 KB, free: 265.4 MB)\n",
      "15/10/04 18:06:57 INFO PythonRDD: Times: total = 3, boot = -1836, init = 1839, finish = 0\n",
      "15/10/04 18:06:57 INFO MemoryStore: ensureFreeSpace(26) called with curMem=105879, maxMem=278302556\n",
      "15/10/04 18:06:57 INFO MemoryStore: Block rdd_51_0 stored as bytes in memory (estimated size 26.0 B, free 265.3 MB)\n",
      "15/10/04 18:06:57 INFO BlockManagerInfo: Added rdd_51_0 in memory on localhost:52448 (size: 26.0 B, free: 265.4 MB)\n",
      "15/10/04 18:06:57 INFO Executor: Finished task 1.0 in stage 16.0 (TID 61). 2066 bytes result sent to driver\n",
      "15/10/04 18:06:57 INFO Executor: Finished task 0.0 in stage 16.0 (TID 60). 2066 bytes result sent to driver\n",
      "15/10/04 18:06:57 INFO BlockManagerInfo: Removed broadcast_5_piece0 on localhost:52448 in memory (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:06:57 INFO TaskSetManager: Finished task 1.0 in stage 16.0 (TID 61) in 20 ms on localhost (1/4)\n",
      "15/10/04 18:06:57 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 60) in 21 ms on localhost (2/4)\n",
      "15/10/04 18:06:57 INFO PythonRDD: Times: total = 14, boot = -1806, init = 1820, finish = 0\n",
      "15/10/04 18:06:57 INFO PythonRDD: Times: total = 19, boot = -1805, init = 1824, finish = 0\n",
      "15/10/04 18:06:57 INFO MemoryStore: ensureFreeSpace(26) called with curMem=96453, maxMem=278302556\n",
      "15/10/04 18:06:57 INFO MemoryStore: Block rdd_52_0 stored as bytes in memory (estimated size 26.0 B, free 265.3 MB)\n",
      "15/10/04 18:06:57 INFO BlockManagerInfo: Added rdd_52_0 in memory on localhost:52448 (size: 26.0 B, free: 265.4 MB)\n",
      "15/10/04 18:06:57 INFO Executor: Finished task 2.0 in stage 16.0 (TID 62). 1524 bytes result sent to driver\n",
      "15/10/04 18:06:57 INFO TaskSetManager: Finished task 2.0 in stage 16.0 (TID 62) in 47 ms on localhost (3/4)\n",
      "15/10/04 18:06:57 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:52448 in memory (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:06:57 INFO PythonRDD: Times: total = 43, boot = -1806, init = 1849, finish = 0\n",
      "15/10/04 18:06:57 INFO Executor: Finished task 3.0 in stage 16.0 (TID 63). 1524 bytes result sent to driver\n",
      "15/10/04 18:06:57 INFO TaskSetManager: Finished task 3.0 in stage 16.0 (TID 63) in 52 ms on localhost (4/4)\n",
      "15/10/04 18:06:57 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:06:57 INFO DAGScheduler: ShuffleMapStage 16 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.058 s\n",
      "15/10/04 18:06:57 INFO DAGScheduler: looking for newly runnable stages\n",
      "15/10/04 18:06:57 INFO DAGScheduler: running: Set()\n",
      "15/10/04 18:06:57 INFO DAGScheduler: waiting: Set(ResultStage 17)\n",
      "15/10/04 18:06:57 INFO DAGScheduler: failed: Set()\n",
      "15/10/04 18:06:57 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:52448 in memory (size: 5.6 KB, free: 265.4 MB)\n",
      "15/10/04 18:06:57 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:52448 in memory (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:06:57 INFO DAGScheduler: Missing parents for ResultStage 17: List()\n",
      "15/10/04 18:06:57 INFO DAGScheduler: Submitting ResultStage 17 (PythonRDD[59] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "15/10/04 18:06:57 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:52448 in memory (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:06:57 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=46798, maxMem=278302556\n",
      "15/10/04 18:06:57 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 5.9 KB, free 265.4 MB)\n",
      "15/10/04 18:06:57 INFO MemoryStore: ensureFreeSpace(3406) called with curMem=36120, maxMem=278302556\n",
      "15/10/04 18:06:57 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 3.3 KB, free 265.4 MB)\n",
      "15/10/04 18:06:57 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:52448 in memory (size: 5.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:06:57 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:52448 (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:06:57 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:06:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (PythonRDD[59] at RDD at PythonRDD.scala:43)\n",
      "15/10/04 18:06:57 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks\n",
      "15/10/04 18:06:57 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 64, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:06:57 INFO Executor: Running task 0.0 in stage 17.0 (TID 64)\n",
      "15/10/04 18:06:57 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 4 blocks\n",
      "15/10/04 18:06:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:06:57 INFO PythonRDD: Times: total = 1, boot = -32, init = 33, finish = 0\n",
      "15/10/04 18:06:57 INFO PythonRDD: Times: total = 2, boot = -1863, init = 1865, finish = 0\n",
      "15/10/04 18:06:57 INFO Executor: Finished task 0.0 in stage 17.0 (TID 64). 932 bytes result sent to driver\n",
      "15/10/04 18:06:57 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 64) in 6 ms on localhost (1/1)\n",
      "15/10/04 18:06:57 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:06:57 INFO DAGScheduler: ResultStage 17 (runJob at PythonRDD.scala:366) finished in 0.007 s\n",
      "15/10/04 18:06:57 INFO DAGScheduler: Job 8 finished: runJob at PythonRDD.scala:366, took 0.110508 s\n",
      "15/10/04 18:06:57 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/04 18:06:57 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 4 is 153 bytes\n",
      "15/10/04 18:06:57 INFO DAGScheduler: Got job 9 (runJob at PythonRDD.scala:366) with 3 output partitions (allowLocal=true)\n",
      "15/10/04 18:06:57 INFO DAGScheduler: Final stage: ResultStage 19(runJob at PythonRDD.scala:366)\n",
      "15/10/04 18:06:57 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\n",
      "15/10/04 18:06:57 INFO DAGScheduler: Missing parents: List()\n",
      "15/10/04 18:06:57 INFO DAGScheduler: Submitting ResultStage 19 (PythonRDD[60] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "15/10/04 18:06:57 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=39526, maxMem=278302556\n",
      "15/10/04 18:06:57 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 5.9 KB, free 265.4 MB)\n",
      "15/10/04 18:06:57 INFO MemoryStore: ensureFreeSpace(3406) called with curMem=45566, maxMem=278302556\n",
      "15/10/04 18:06:57 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 3.3 KB, free 265.4 MB)\n",
      "15/10/04 18:06:57 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:52448 (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:06:57 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:06:57 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 19 (PythonRDD[60] at RDD at PythonRDD.scala:43)\n",
      "15/10/04 18:06:57 INFO TaskSchedulerImpl: Adding task set 19.0 with 3 tasks\n",
      "15/10/04 18:06:57 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 65, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:06:57 INFO TaskSetManager: Starting task 1.0 in stage 19.0 (TID 66, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:06:57 INFO TaskSetManager: Starting task 2.0 in stage 19.0 (TID 67, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:06:57 INFO Executor: Running task 2.0 in stage 19.0 (TID 67)\n",
      "15/10/04 18:06:57 INFO Executor: Running task 1.0 in stage 19.0 (TID 66)\n",
      "15/10/04 18:06:57 INFO Executor: Running task 0.0 in stage 19.0 (TID 65)\n",
      "15/10/04 18:06:57 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 4 blocks\n",
      "15/10/04 18:06:57 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 4 blocks\n",
      "15/10/04 18:06:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:06:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:06:57 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 4 blocks\n",
      "15/10/04 18:06:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:06:57 INFO PythonRDD: Times: total = 1, boot = -33, init = 34, finish = 0\n",
      "15/10/04 18:06:57 INFO PythonRDD: Times: total = 2, boot = -50, init = 51, finish = 1\n",
      "15/10/04 18:06:57 INFO PythonRDD: Times: total = 3, boot = -12, init = 15, finish = 0\n",
      "15/10/04 18:06:57 INFO PythonRDD: Times: total = 5, boot = -60, init = 65, finish = 0\n",
      "15/10/04 18:06:57 INFO PythonRDD: Times: total = 12, boot = -58, init = 70, finish = 0\n",
      "15/10/04 18:06:57 INFO Executor: Finished task 0.0 in stage 19.0 (TID 65). 932 bytes result sent to driver\n",
      "15/10/04 18:06:57 INFO Executor: Finished task 2.0 in stage 19.0 (TID 67). 932 bytes result sent to driver\n",
      "15/10/04 18:06:57 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 65) in 22 ms on localhost (1/3)\n",
      "15/10/04 18:06:57 INFO TaskSetManager: Finished task 2.0 in stage 19.0 (TID 67) in 21 ms on localhost (2/3)\n",
      "15/10/04 18:06:57 INFO PythonRDD: Times: total = 20, boot = -56, init = 76, finish = 0\n",
      "15/10/04 18:06:57 INFO Executor: Finished task 1.0 in stage 19.0 (TID 66). 932 bytes result sent to driver\n",
      "15/10/04 18:06:57 INFO TaskSetManager: Finished task 1.0 in stage 19.0 (TID 66) in 38 ms on localhost (3/3)\n",
      "15/10/04 18:06:57 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:06:57 INFO DAGScheduler: ResultStage 19 (runJob at PythonRDD.scala:366) finished in 0.044 s\n",
      "15/10/04 18:06:57 INFO DAGScheduler: Job 9 finished: runJob at PythonRDD.scala:366, took 0.054994 s\n",
      "15/10/04 18:06:57 INFO JobScheduler: Finished job streaming job 1444007217000 ms.0 from job set of time 1444007217000 ms\n",
      "15/10/04 18:06:57 INFO JobScheduler: Total delay: 0.231 s for time 1444007217000 ms (execution: 0.191 s)\n",
      "15/10/04 18:06:57 INFO PythonRDD: Removing RDD 48 from persistence list\n",
      "15/10/04 18:06:57 INFO BlockManager: Removing RDD 48\n",
      "15/10/04 18:06:57 INFO UnionRDD: Removing RDD 43 from persistence list\n",
      "15/10/04 18:06:57 INFO BlockManager: Removing RDD 43\n",
      "15/10/04 18:06:57 INFO PythonRDD: Removing RDD 20 from persistence list\n",
      "15/10/04 18:06:57 INFO BlockManager: Removing RDD 20\n",
      "15/10/04 18:06:57 INFO PythonRDD: Removing RDD 21 from persistence list\n",
      "15/10/04 18:06:57 INFO PythonRDD: Removing RDD 7 from persistence list\n",
      "15/10/04 18:06:57 INFO BlockManager: Removing RDD 21\n",
      "15/10/04 18:06:57 INFO BlockManager: Removing RDD 7\n",
      "15/10/04 18:06:57 INFO PythonRDD: Removing RDD 8 from persistence list\n",
      "15/10/04 18:06:57 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "15/10/04 18:06:57 INFO BlockManager: Removing RDD 8\n",
      "15/10/04 18:06:57 INFO InputInfoTracker: remove old batch metadata: \n",
      "15/10/04 18:06:58 INFO PythonTransformedDStream: Time 1444007218000 ms is invalid as zeroTime is 1444007207000 ms and slideDuration is 2000 ms and difference is 11000 ms\n",
      "15/10/04 18:06:58 INFO JobScheduler: No jobs added for time 1444007218000 ms\n",
      "15/10/04 18:06:59 INFO PythonTransformedDStream: Slicing from 1444007216000 ms to 1444007219000 ms (aligned to 1444007216000 ms and 1444007219000 ms)\n",
      "15/10/04 18:06:59 INFO JobScheduler: Added jobs for time 1444007219000 ms\n",
      "15/10/04 18:06:59 INFO JobScheduler: Starting job streaming job 1444007219000 ms.0 from job set of time 1444007219000 ms\n",
      "15/10/04 18:06:59 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/04 18:06:59 INFO DAGScheduler: Registering RDD 65 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/04 18:06:59 INFO DAGScheduler: Got job 10 (runJob at PythonRDD.scala:366) with 1 output partitions (allowLocal=true)\n",
      "15/10/04 18:06:59 INFO DAGScheduler: Final stage: ResultStage 21(runJob at PythonRDD.scala:366)\n",
      "15/10/04 18:06:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)\n",
      "15/10/04 18:06:59 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 20)\n",
      "15/10/04 18:06:59 INFO DAGScheduler: Submitting ShuffleMapStage 20 (PairwiseRDD[65] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "15/10/04 18:06:59 INFO MemoryStore: ensureFreeSpace(11648) called with curMem=40252, maxMem=278302556\n",
      "15/10/04 18:06:59 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 11.4 KB, free 265.4 MB)\n",
      "15/10/04 18:06:59 INFO MemoryStore: ensureFreeSpace(5220) called with curMem=51900, maxMem=278302556\n",
      "15/10/04 18:06:59 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 5.1 KB, free 265.4 MB)\n",
      "15/10/04 18:06:59 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:52448 (size: 5.1 KB, free: 265.4 MB)\n",
      "15/10/04 18:06:59 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:06:59 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 20 (PairwiseRDD[65] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/04 18:06:59 INFO TaskSchedulerImpl: Adding task set 20.0 with 4 tasks\n",
      "15/10/04 18:06:59 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 68, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:06:59 INFO TaskSetManager: Starting task 1.0 in stage 20.0 (TID 69, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:06:59 INFO TaskSetManager: Starting task 2.0 in stage 20.0 (TID 70, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:06:59 INFO TaskSetManager: Starting task 3.0 in stage 20.0 (TID 71, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:06:59 INFO Executor: Running task 0.0 in stage 20.0 (TID 68)\n",
      "15/10/04 18:06:59 INFO Executor: Running task 1.0 in stage 20.0 (TID 69)\n",
      "15/10/04 18:06:59 INFO Executor: Running task 3.0 in stage 20.0 (TID 71)\n",
      "15/10/04 18:06:59 INFO Executor: Running task 2.0 in stage 20.0 (TID 70)\n",
      "15/10/04 18:06:59 INFO BlockManager: Found block rdd_52_0 locally\n",
      "15/10/04 18:06:59 INFO BlockManager: Found block rdd_51_0 locally\n",
      "15/10/04 18:06:59 INFO PythonRDD: Times: total = 1, boot = -1834, init = 1835, finish = 0\n",
      "15/10/04 18:06:59 INFO CacheManager: Partition rdd_62_0 not found, computing it\n",
      "15/10/04 18:06:59 INFO PythonRDD: Times: total = 2, boot = -1832, init = 1833, finish = 1\n",
      "15/10/04 18:06:59 INFO Executor: Finished task 1.0 in stage 20.0 (TID 69). 2066 bytes result sent to driver\n",
      "15/10/04 18:06:59 INFO CacheManager: Partition rdd_61_0 not found, computing it\n",
      "15/10/04 18:06:59 INFO TaskSetManager: Finished task 1.0 in stage 20.0 (TID 69) in 9 ms on localhost (1/4)\n",
      "15/10/04 18:06:59 INFO PythonRDD: Times: total = 3, boot = -1831, init = 1834, finish = 0\n",
      "15/10/04 18:06:59 INFO MemoryStore: ensureFreeSpace(26) called with curMem=57120, maxMem=278302556\n",
      "15/10/04 18:06:59 INFO MemoryStore: Block rdd_62_0 stored as bytes in memory (estimated size 26.0 B, free 265.4 MB)\n",
      "15/10/04 18:06:59 INFO BlockManagerInfo: Added rdd_62_0 in memory on localhost:52448 (size: 26.0 B, free: 265.4 MB)\n",
      "15/10/04 18:06:59 INFO Executor: Finished task 0.0 in stage 20.0 (TID 68). 2066 bytes result sent to driver\n",
      "15/10/04 18:06:59 INFO PythonRDD: Times: total = 7, boot = -1831, init = 1838, finish = 0\n",
      "15/10/04 18:06:59 INFO MemoryStore: ensureFreeSpace(26) called with curMem=57146, maxMem=278302556\n",
      "15/10/04 18:06:59 INFO MemoryStore: Block rdd_61_0 stored as bytes in memory (estimated size 26.0 B, free 265.4 MB)\n",
      "15/10/04 18:06:59 INFO PythonRDD: Times: total = 13, boot = -1880, init = 1892, finish = 1\n",
      "15/10/04 18:06:59 INFO BlockManagerInfo: Added rdd_61_0 in memory on localhost:52448 (size: 26.0 B, free: 265.4 MB)\n",
      "15/10/04 18:06:59 INFO Executor: Finished task 3.0 in stage 20.0 (TID 71). 1524 bytes result sent to driver\n",
      "15/10/04 18:06:59 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 68) in 37 ms on localhost (2/4)\n",
      "15/10/04 18:06:59 INFO TaskSetManager: Finished task 3.0 in stage 20.0 (TID 71) in 45 ms on localhost (3/4)\n",
      "15/10/04 18:06:59 INFO PythonRDD: Times: total = 35, boot = -1829, init = 1864, finish = 0\n",
      "15/10/04 18:06:59 INFO Executor: Finished task 2.0 in stage 20.0 (TID 70). 1524 bytes result sent to driver\n",
      "15/10/04 18:06:59 INFO TaskSetManager: Finished task 2.0 in stage 20.0 (TID 70) in 52 ms on localhost (4/4)\n",
      "15/10/04 18:06:59 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:06:59 INFO DAGScheduler: ShuffleMapStage 20 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.055 s\n",
      "15/10/04 18:06:59 INFO DAGScheduler: looking for newly runnable stages\n",
      "15/10/04 18:06:59 INFO DAGScheduler: running: Set()\n",
      "15/10/04 18:06:59 INFO DAGScheduler: waiting: Set(ResultStage 21)\n",
      "15/10/04 18:06:59 INFO DAGScheduler: failed: Set()\n",
      "15/10/04 18:06:59 INFO DAGScheduler: Missing parents for ResultStage 21: List()\n",
      "15/10/04 18:06:59 INFO DAGScheduler: Submitting ResultStage 21 (PythonRDD[69] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "15/10/04 18:06:59 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=57172, maxMem=278302556\n",
      "15/10/04 18:06:59 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 5.9 KB, free 265.3 MB)\n",
      "15/10/04 18:06:59 INFO MemoryStore: ensureFreeSpace(3412) called with curMem=63212, maxMem=278302556\n",
      "15/10/04 18:06:59 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 3.3 KB, free 265.3 MB)\n",
      "15/10/04 18:06:59 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:52448 (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:06:59 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:06:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (PythonRDD[69] at RDD at PythonRDD.scala:43)\n",
      "15/10/04 18:06:59 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks\n",
      "15/10/04 18:06:59 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 72, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:06:59 INFO Executor: Running task 0.0 in stage 21.0 (TID 72)\n",
      "15/10/04 18:06:59 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 4 blocks\n",
      "15/10/04 18:06:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:06:59 INFO PythonRDD: Times: total = 1, boot = -28, init = 29, finish = 0\n",
      "15/10/04 18:06:59 INFO PythonRDD: Times: total = 2, boot = -1881, init = 1883, finish = 0\n",
      "15/10/04 18:06:59 INFO Executor: Finished task 0.0 in stage 21.0 (TID 72). 932 bytes result sent to driver\n",
      "15/10/04 18:06:59 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 72) in 6 ms on localhost (1/1)\n",
      "15/10/04 18:06:59 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:06:59 INFO DAGScheduler: ResultStage 21 (runJob at PythonRDD.scala:366) finished in 0.006 s\n",
      "15/10/04 18:06:59 INFO DAGScheduler: Job 10 finished: runJob at PythonRDD.scala:366, took 0.083800 s\n",
      "15/10/04 18:06:59 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/04 18:06:59 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 5 is 153 bytes\n",
      "15/10/04 18:06:59 INFO DAGScheduler: Got job 11 (runJob at PythonRDD.scala:366) with 3 output partitions (allowLocal=true)\n",
      "15/10/04 18:06:59 INFO DAGScheduler: Final stage: ResultStage 23(runJob at PythonRDD.scala:366)\n",
      "15/10/04 18:06:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)\n",
      "15/10/04 18:06:59 INFO DAGScheduler: Missing parents: List()\n",
      "15/10/04 18:06:59 INFO DAGScheduler: Submitting ResultStage 23 (PythonRDD[70] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "15/10/04 18:06:59 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=66624, maxMem=278302556\n",
      "15/10/04 18:06:59 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.9 KB, free 265.3 MB)\n",
      "15/10/04 18:06:59 INFO MemoryStore: ensureFreeSpace(3412) called with curMem=72664, maxMem=278302556\n",
      "15/10/04 18:06:59 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.3 KB, free 265.3 MB)\n",
      "15/10/04 18:06:59 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on localhost:52448 (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:06:59 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:06:59 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 23 (PythonRDD[70] at RDD at PythonRDD.scala:43)\n",
      "15/10/04 18:06:59 INFO TaskSchedulerImpl: Adding task set 23.0 with 3 tasks\n",
      "15/10/04 18:06:59 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 73, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:06:59 INFO TaskSetManager: Starting task 1.0 in stage 23.0 (TID 74, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:06:59 INFO TaskSetManager: Starting task 2.0 in stage 23.0 (TID 75, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:06:59 INFO Executor: Running task 2.0 in stage 23.0 (TID 75)\n",
      "15/10/04 18:06:59 INFO Executor: Running task 1.0 in stage 23.0 (TID 74)\n",
      "15/10/04 18:06:59 INFO Executor: Running task 0.0 in stage 23.0 (TID 73)\n",
      "15/10/04 18:06:59 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 4 blocks\n",
      "15/10/04 18:06:59 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 4 blocks\n",
      "15/10/04 18:06:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:06:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:06:59 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 4 blocks\n",
      "15/10/04 18:06:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:06:59 INFO PythonRDD: Times: total = 2, boot = -50, init = 51, finish = 1\n",
      "15/10/04 18:06:59 INFO PythonRDD: Times: total = 2, boot = -39, init = 41, finish = 0\n",
      "15/10/04 18:06:59 INFO PythonRDD: Times: total = 3, boot = -12, init = 14, finish = 1\n",
      "15/10/04 18:06:59 INFO PythonRDD: Times: total = 4, boot = -56, init = 60, finish = 0\n",
      "15/10/04 18:06:59 INFO Executor: Finished task 2.0 in stage 23.0 (TID 75). 932 bytes result sent to driver\n",
      "15/10/04 18:06:59 INFO PythonRDD: Times: total = 13, boot = -55, init = 68, finish = 0\n",
      "15/10/04 18:06:59 INFO PythonRDD: Times: total = 23, boot = -56, init = 79, finish = 0\n",
      "15/10/04 18:06:59 INFO Executor: Finished task 1.0 in stage 23.0 (TID 74). 932 bytes result sent to driver\n",
      "15/10/04 18:06:59 INFO Executor: Finished task 0.0 in stage 23.0 (TID 73). 932 bytes result sent to driver\n",
      "15/10/04 18:06:59 INFO TaskSetManager: Finished task 2.0 in stage 23.0 (TID 75) in 28 ms on localhost (1/3)\n",
      "15/10/04 18:06:59 INFO TaskSetManager: Finished task 1.0 in stage 23.0 (TID 74) in 29 ms on localhost (2/3)\n",
      "15/10/04 18:06:59 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 73) in 31 ms on localhost (3/3)\n",
      "15/10/04 18:06:59 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:06:59 INFO DAGScheduler: ResultStage 23 (runJob at PythonRDD.scala:366) finished in 0.039 s\n",
      "15/10/04 18:06:59 INFO DAGScheduler: Job 11 finished: runJob at PythonRDD.scala:366, took 0.050409 s\n",
      "15/10/04 18:06:59 INFO JobScheduler: Finished job streaming job 1444007219000 ms.0 from job set of time 1444007219000 ms\n",
      "15/10/04 18:06:59 INFO JobScheduler: Total delay: 0.191 s for time 1444007219000 ms (execution: 0.161 s)\n",
      "15/10/04 18:06:59 INFO PythonRDD: Removing RDD 58 from persistence list\n",
      "15/10/04 18:06:59 INFO UnionRDD: Removing RDD 53 from persistence list\n",
      "15/10/04 18:06:59 INFO BlockManager: Removing RDD 58\n",
      "15/10/04 18:06:59 INFO PythonRDD: Removing RDD 32 from persistence list\n",
      "15/10/04 18:06:59 INFO BlockManager: Removing RDD 53\n",
      "15/10/04 18:06:59 INFO PythonRDD: Removing RDD 31 from persistence list\n",
      "15/10/04 18:06:59 INFO BlockManager: Removing RDD 32\n",
      "15/10/04 18:06:59 INFO ParallelCollectionRDD: Removing RDD 10 from persistence list\n",
      "15/10/04 18:06:59 INFO BlockManager: Removing RDD 31\n",
      "15/10/04 18:06:59 INFO PythonRDD: Removing RDD 9 from persistence list\n",
      "15/10/04 18:06:59 INFO BlockManager: Removing RDD 10\n",
      "15/10/04 18:06:59 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "15/10/04 18:06:59 INFO BlockManager: Removing RDD 9\n",
      "15/10/04 18:06:59 INFO InputInfoTracker: remove old batch metadata: \n",
      "15/10/04 18:07:00 INFO PythonTransformedDStream: Time 1444007220000 ms is invalid as zeroTime is 1444007207000 ms and slideDuration is 2000 ms and difference is 13000 ms\n",
      "15/10/04 18:07:00 INFO JobScheduler: No jobs added for time 1444007220000 ms\n",
      "15/10/04 18:07:01 INFO PythonTransformedDStream: Slicing from 1444007218000 ms to 1444007221000 ms (aligned to 1444007218000 ms and 1444007221000 ms)\n",
      "15/10/04 18:07:01 INFO JobScheduler: Added jobs for time 1444007221000 ms\n",
      "15/10/04 18:07:01 INFO JobScheduler: Starting job streaming job 1444007221000 ms.0 from job set of time 1444007221000 ms\n",
      "15/10/04 18:07:01 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/04 18:07:01 INFO DAGScheduler: Registering RDD 75 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/04 18:07:01 INFO DAGScheduler: Got job 12 (runJob at PythonRDD.scala:366) with 1 output partitions (allowLocal=true)\n",
      "15/10/04 18:07:01 INFO DAGScheduler: Final stage: ResultStage 25(runJob at PythonRDD.scala:366)\n",
      "15/10/04 18:07:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 24)\n",
      "15/10/04 18:07:01 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 24)\n",
      "15/10/04 18:07:01 INFO DAGScheduler: Submitting ShuffleMapStage 24 (PairwiseRDD[75] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "15/10/04 18:07:01 INFO MemoryStore: ensureFreeSpace(11648) called with curMem=71690, maxMem=278302556\n",
      "15/10/04 18:07:01 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 11.4 KB, free 265.3 MB)\n",
      "15/10/04 18:07:01 INFO MemoryStore: ensureFreeSpace(5222) called with curMem=83338, maxMem=278302556\n",
      "15/10/04 18:07:01 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 5.1 KB, free 265.3 MB)\n",
      "15/10/04 18:07:01 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on localhost:52448 (size: 5.1 KB, free: 265.4 MB)\n",
      "15/10/04 18:07:01 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:07:01 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 24 (PairwiseRDD[75] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/04 18:07:01 INFO TaskSchedulerImpl: Adding task set 24.0 with 4 tasks\n",
      "15/10/04 18:07:01 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 76, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:07:01 INFO TaskSetManager: Starting task 1.0 in stage 24.0 (TID 77, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:07:01 INFO TaskSetManager: Starting task 2.0 in stage 24.0 (TID 78, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:07:01 INFO TaskSetManager: Starting task 3.0 in stage 24.0 (TID 79, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:07:01 INFO Executor: Running task 0.0 in stage 24.0 (TID 76)\n",
      "15/10/04 18:07:01 INFO Executor: Running task 1.0 in stage 24.0 (TID 77)\n",
      "15/10/04 18:07:01 INFO Executor: Running task 2.0 in stage 24.0 (TID 78)\n",
      "15/10/04 18:07:01 INFO Executor: Running task 3.0 in stage 24.0 (TID 79)\n",
      "15/10/04 18:07:01 INFO BlockManager: Found block rdd_61_0 locally\n",
      "15/10/04 18:07:01 INFO BlockManager: Found block rdd_62_0 locally\n",
      "15/10/04 18:07:01 INFO PythonRDD: Times: total = 1, boot = -1876, init = 1877, finish = 0\n",
      "15/10/04 18:07:01 INFO CacheManager: Partition rdd_72_0 not found, computing it\n",
      "15/10/04 18:07:01 INFO CacheManager: Partition rdd_71_0 not found, computing it\n",
      "15/10/04 18:07:01 INFO Executor: Finished task 0.0 in stage 24.0 (TID 76). 2066 bytes result sent to driver\n",
      "15/10/04 18:07:01 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 76) in 16 ms on localhost (1/4)\n",
      "15/10/04 18:07:01 INFO PythonRDD: Times: total = 3, boot = -1924, init = 1927, finish = 0\n",
      "15/10/04 18:07:01 INFO PythonRDD: Times: total = 5, boot = -1881, init = 1886, finish = 0\n",
      "15/10/04 18:07:01 INFO Executor: Finished task 1.0 in stage 24.0 (TID 77). 2066 bytes result sent to driver\n",
      "15/10/04 18:07:01 INFO MemoryStore: ensureFreeSpace(26) called with curMem=88560, maxMem=278302556\n",
      "15/10/04 18:07:01 INFO PythonRDD: Times: total = 6, boot = -1869, init = 1875, finish = 0\n",
      "15/10/04 18:07:01 INFO MemoryStore: Block rdd_72_0 stored as bytes in memory (estimated size 26.0 B, free 265.3 MB)\n",
      "15/10/04 18:07:01 INFO MemoryStore: ensureFreeSpace(26) called with curMem=88586, maxMem=278302556\n",
      "15/10/04 18:07:01 INFO MemoryStore: Block rdd_71_0 stored as bytes in memory (estimated size 26.0 B, free 265.3 MB)\n",
      "15/10/04 18:07:01 INFO BlockManagerInfo: Added rdd_72_0 in memory on localhost:52448 (size: 26.0 B, free: 265.4 MB)\n",
      "15/10/04 18:07:01 INFO TaskSetManager: Finished task 1.0 in stage 24.0 (TID 77) in 22 ms on localhost (2/4)\n",
      "15/10/04 18:07:01 INFO BlockManagerInfo: Added rdd_71_0 in memory on localhost:52448 (size: 26.0 B, free: 265.4 MB)\n",
      "15/10/04 18:07:01 INFO PythonRDD: Times: total = 18, boot = -1880, init = 1898, finish = 0\n",
      "15/10/04 18:07:01 INFO PythonRDD: Times: total = 16, boot = -1882, init = 1898, finish = 0\n",
      "15/10/04 18:07:01 INFO Executor: Finished task 2.0 in stage 24.0 (TID 78). 1524 bytes result sent to driver\n",
      "15/10/04 18:07:01 INFO Executor: Finished task 3.0 in stage 24.0 (TID 79). 1524 bytes result sent to driver\n",
      "15/10/04 18:07:01 INFO TaskSetManager: Finished task 2.0 in stage 24.0 (TID 78) in 30 ms on localhost (3/4)\n",
      "15/10/04 18:07:01 INFO TaskSetManager: Finished task 3.0 in stage 24.0 (TID 79) in 30 ms on localhost (4/4)\n",
      "15/10/04 18:07:01 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:07:01 INFO DAGScheduler: ShuffleMapStage 24 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.050 s\n",
      "15/10/04 18:07:01 INFO DAGScheduler: looking for newly runnable stages\n",
      "15/10/04 18:07:01 INFO DAGScheduler: running: Set()\n",
      "15/10/04 18:07:01 INFO DAGScheduler: waiting: Set(ResultStage 25)\n",
      "15/10/04 18:07:01 INFO DAGScheduler: failed: Set()\n",
      "15/10/04 18:07:01 INFO DAGScheduler: Missing parents for ResultStage 25: List()\n",
      "15/10/04 18:07:01 INFO DAGScheduler: Submitting ResultStage 25 (PythonRDD[79] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "15/10/04 18:07:01 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=88612, maxMem=278302556\n",
      "15/10/04 18:07:01 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 5.9 KB, free 265.3 MB)\n",
      "15/10/04 18:07:01 INFO MemoryStore: ensureFreeSpace(3412) called with curMem=94652, maxMem=278302556\n",
      "15/10/04 18:07:01 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 3.3 KB, free 265.3 MB)\n",
      "15/10/04 18:07:01 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on localhost:52448 (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:07:01 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:07:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (PythonRDD[79] at RDD at PythonRDD.scala:43)\n",
      "15/10/04 18:07:01 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks\n",
      "15/10/04 18:07:01 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 80, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:07:01 INFO Executor: Running task 0.0 in stage 25.0 (TID 80)\n",
      "15/10/04 18:07:01 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 4 blocks\n",
      "15/10/04 18:07:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:07:01 INFO PythonRDD: Times: total = 2, boot = 2, init = 0, finish = 0\n",
      "15/10/04 18:07:01 INFO PythonRDD: Times: total = 4, boot = -1916, init = 1920, finish = 0\n",
      "15/10/04 18:07:01 INFO Executor: Finished task 0.0 in stage 25.0 (TID 80). 932 bytes result sent to driver\n",
      "15/10/04 18:07:01 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 80) in 8 ms on localhost (1/1)\n",
      "15/10/04 18:07:01 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:07:01 INFO DAGScheduler: ResultStage 25 (runJob at PythonRDD.scala:366) finished in 0.008 s\n",
      "15/10/04 18:07:01 INFO DAGScheduler: Job 12 finished: runJob at PythonRDD.scala:366, took 0.086733 s\n",
      "15/10/04 18:07:01 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/04 18:07:01 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 6 is 153 bytes\n",
      "15/10/04 18:07:01 INFO DAGScheduler: Got job 13 (runJob at PythonRDD.scala:366) with 3 output partitions (allowLocal=true)\n",
      "15/10/04 18:07:01 INFO DAGScheduler: Final stage: ResultStage 27(runJob at PythonRDD.scala:366)\n",
      "15/10/04 18:07:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)\n",
      "15/10/04 18:07:01 INFO DAGScheduler: Missing parents: List()\n",
      "15/10/04 18:07:01 INFO DAGScheduler: Submitting ResultStage 27 (PythonRDD[80] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "15/10/04 18:07:01 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=98064, maxMem=278302556\n",
      "15/10/04 18:07:01 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 5.9 KB, free 265.3 MB)\n",
      "15/10/04 18:07:01 INFO MemoryStore: ensureFreeSpace(3412) called with curMem=104104, maxMem=278302556\n",
      "15/10/04 18:07:01 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 3.3 KB, free 265.3 MB)\n",
      "15/10/04 18:07:01 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on localhost:52448 (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:07:01 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:07:01 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 27 (PythonRDD[80] at RDD at PythonRDD.scala:43)\n",
      "15/10/04 18:07:01 INFO TaskSchedulerImpl: Adding task set 27.0 with 3 tasks\n",
      "15/10/04 18:07:01 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 81, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:07:01 INFO TaskSetManager: Starting task 1.0 in stage 27.0 (TID 82, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:07:01 INFO TaskSetManager: Starting task 2.0 in stage 27.0 (TID 83, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:07:01 INFO Executor: Running task 1.0 in stage 27.0 (TID 82)\n",
      "15/10/04 18:07:01 INFO Executor: Running task 2.0 in stage 27.0 (TID 83)\n",
      "15/10/04 18:07:01 INFO Executor: Running task 0.0 in stage 27.0 (TID 81)\n",
      "15/10/04 18:07:01 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 4 blocks\n",
      "15/10/04 18:07:01 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 4 blocks\n",
      "15/10/04 18:07:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:07:01 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 4 blocks\n",
      "15/10/04 18:07:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:07:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:07:01 INFO PythonRDD: Times: total = 2, boot = -29, init = 31, finish = 0\n",
      "15/10/04 18:07:01 INFO PythonRDD: Times: total = 2, boot = -21, init = 23, finish = 0\n",
      "15/10/04 18:07:01 INFO PythonRDD: Times: total = 3, boot = -46, init = 49, finish = 0\n",
      "15/10/04 18:07:01 INFO Executor: Finished task 1.0 in stage 27.0 (TID 82). 932 bytes result sent to driver\n",
      "15/10/04 18:07:01 INFO TaskSetManager: Finished task 1.0 in stage 27.0 (TID 82) in 7 ms on localhost (1/3)\n",
      "15/10/04 18:07:01 INFO PythonRDD: Times: total = 12, boot = -42, init = 54, finish = 0\n",
      "15/10/04 18:07:01 INFO Executor: Finished task 2.0 in stage 27.0 (TID 83). 932 bytes result sent to driver\n",
      "15/10/04 18:07:01 INFO TaskSetManager: Finished task 2.0 in stage 27.0 (TID 83) in 16 ms on localhost (2/3)\n",
      "15/10/04 18:07:01 INFO PythonRDD: Times: total = 18, boot = 17, init = 1, finish = 0\n",
      "15/10/04 18:07:01 INFO PythonRDD: Times: total = 18, boot = -19, init = 37, finish = 0\n",
      "15/10/04 18:07:01 INFO Executor: Finished task 0.0 in stage 27.0 (TID 81). 932 bytes result sent to driver\n",
      "15/10/04 18:07:01 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 81) in 24 ms on localhost (3/3)\n",
      "15/10/04 18:07:01 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:07:01 INFO DAGScheduler: ResultStage 27 (runJob at PythonRDD.scala:366) finished in 0.025 s\n",
      "15/10/04 18:07:01 INFO DAGScheduler: Job 13 finished: runJob at PythonRDD.scala:366, took 0.039623 s\n",
      "15/10/04 18:07:01 INFO JobScheduler: Finished job streaming job 1444007221000 ms.0 from job set of time 1444007221000 ms\n",
      "15/10/04 18:07:01 INFO JobScheduler: Total delay: 0.188 s for time 1444007221000 ms (execution: 0.152 s)\n",
      "15/10/04 18:07:01 INFO PythonRDD: Removing RDD 68 from persistence list\n",
      "15/10/04 18:07:01 INFO BlockManager: Removing RDD 68\n",
      "15/10/04 18:07:01 INFO UnionRDD: Removing RDD 63 from persistence list\n",
      "15/10/04 18:07:01 INFO BlockManager: Removing RDD 63\n",
      "15/10/04 18:07:01 INFO PythonRDD: Removing RDD 41 from persistence list\n",
      "15/10/04 18:07:01 INFO PythonRDD: Removing RDD 42 from persistence list\n",
      "15/10/04 18:07:01 INFO BlockManager: Removing RDD 41\n",
      "15/10/04 18:07:01 INFO ParallelCollectionRDD: Removing RDD 10 from persistence list\n",
      "15/10/04 18:07:01 INFO BlockManager: Removing RDD 42\n",
      "15/10/04 18:07:01 INFO ParallelCollectionRDD: Removing RDD 10 from persistence list\n",
      "15/10/04 18:07:01 INFO BlockManager: Removing RDD 10\n",
      "15/10/04 18:07:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "15/10/04 18:07:01 INFO BlockManager: Removing RDD 10\n",
      "15/10/04 18:07:01 INFO InputInfoTracker: remove old batch metadata: \n",
      "15/10/04 18:07:02 INFO PythonTransformedDStream: Time 1444007222000 ms is invalid as zeroTime is 1444007207000 ms and slideDuration is 2000 ms and difference is 15000 ms\n",
      "15/10/04 18:07:02 INFO JobScheduler: No jobs added for time 1444007222000 ms\n",
      "15/10/04 18:07:03 INFO PythonTransformedDStream: Slicing from 1444007220000 ms to 1444007223000 ms (aligned to 1444007220000 ms and 1444007223000 ms)\n",
      "15/10/04 18:07:03 INFO JobScheduler: Added jobs for time 1444007223000 ms\n",
      "15/10/04 18:07:03 INFO JobScheduler: Starting job streaming job 1444007223000 ms.0 from job set of time 1444007223000 ms\n",
      "15/10/04 18:07:03 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/04 18:07:03 INFO DAGScheduler: Registering RDD 85 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/04 18:07:03 INFO DAGScheduler: Got job 14 (runJob at PythonRDD.scala:366) with 1 output partitions (allowLocal=true)\n",
      "15/10/04 18:07:03 INFO DAGScheduler: Final stage: ResultStage 29(runJob at PythonRDD.scala:366)\n",
      "15/10/04 18:07:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 28)\n",
      "15/10/04 18:07:03 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 28)\n",
      "15/10/04 18:07:03 INFO DAGScheduler: Submitting ShuffleMapStage 28 (PairwiseRDD[85] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "15/10/04 18:07:03 INFO MemoryStore: ensureFreeSpace(11648) called with curMem=107464, maxMem=278302556\n",
      "15/10/04 18:07:03 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 11.4 KB, free 265.3 MB)\n",
      "15/10/04 18:07:03 INFO MemoryStore: ensureFreeSpace(5222) called with curMem=119112, maxMem=278302556\n",
      "15/10/04 18:07:03 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 5.1 KB, free 265.3 MB)\n",
      "15/10/04 18:07:03 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on localhost:52448 (size: 5.1 KB, free: 265.4 MB)\n",
      "15/10/04 18:07:03 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:07:03 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 28 (PairwiseRDD[85] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/04 18:07:03 INFO TaskSchedulerImpl: Adding task set 28.0 with 4 tasks\n",
      "15/10/04 18:07:03 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 84, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:07:03 INFO TaskSetManager: Starting task 1.0 in stage 28.0 (TID 85, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:07:03 INFO TaskSetManager: Starting task 2.0 in stage 28.0 (TID 86, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:07:03 INFO TaskSetManager: Starting task 3.0 in stage 28.0 (TID 87, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:07:03 INFO Executor: Running task 0.0 in stage 28.0 (TID 84)\n",
      "15/10/04 18:07:03 INFO Executor: Running task 1.0 in stage 28.0 (TID 85)\n",
      "15/10/04 18:07:03 INFO Executor: Running task 2.0 in stage 28.0 (TID 86)\n",
      "15/10/04 18:07:03 INFO Executor: Running task 3.0 in stage 28.0 (TID 87)\n",
      "15/10/04 18:07:03 INFO BlockManager: Found block rdd_71_0 locally\n",
      "15/10/04 18:07:03 INFO PythonRDD: Times: total = 5, boot = -1930, init = 1935, finish = 0\n",
      "15/10/04 18:07:03 INFO Executor: Finished task 0.0 in stage 28.0 (TID 84). 2066 bytes result sent to driver\n",
      "15/10/04 18:07:03 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 84) in 17 ms on localhost (1/4)\n",
      "15/10/04 18:07:03 INFO CacheManager: Partition rdd_82_0 not found, computing it\n",
      "15/10/04 18:07:03 INFO PythonRDD: Times: total = 2, boot = -1882, init = 1884, finish = 0\n",
      "15/10/04 18:07:03 INFO MemoryStore: ensureFreeSpace(26) called with curMem=124334, maxMem=278302556\n",
      "15/10/04 18:07:03 INFO MemoryStore: Block rdd_82_0 stored as bytes in memory (estimated size 26.0 B, free 265.3 MB)\n",
      "15/10/04 18:07:03 INFO BlockManagerInfo: Added rdd_82_0 in memory on localhost:52448 (size: 26.0 B, free: 265.4 MB)\n",
      "15/10/04 18:07:03 INFO BlockManager: Found block rdd_72_0 locally\n",
      "15/10/04 18:07:03 INFO PythonRDD: Times: total = 20, boot = -1871, init = 1891, finish = 0\n",
      "15/10/04 18:07:03 INFO PythonRDD: Times: total = 24, boot = -1872, init = 1895, finish = 1\n",
      "15/10/04 18:07:03 INFO Executor: Finished task 1.0 in stage 28.0 (TID 85). 2066 bytes result sent to driver\n",
      "15/10/04 18:07:03 INFO CacheManager: Partition rdd_81_0 not found, computing it\n",
      "15/10/04 18:07:03 INFO TaskSetManager: Finished task 1.0 in stage 28.0 (TID 85) in 53 ms on localhost (2/4)\n",
      "15/10/04 18:07:03 INFO PythonRDD: Times: total = 4, boot = -1900, init = 1904, finish = 0\n",
      "15/10/04 18:07:03 INFO MemoryStore: ensureFreeSpace(26) called with curMem=124360, maxMem=278302556\n",
      "15/10/04 18:07:03 INFO MemoryStore: Block rdd_81_0 stored as bytes in memory (estimated size 26.0 B, free 265.3 MB)\n",
      "15/10/04 18:07:03 INFO BlockManagerInfo: Added rdd_81_0 in memory on localhost:52448 (size: 26.0 B, free: 265.4 MB)\n",
      "15/10/04 18:07:03 INFO Executor: Finished task 3.0 in stage 28.0 (TID 87). 1524 bytes result sent to driver\n",
      "15/10/04 18:07:03 INFO TaskSetManager: Finished task 3.0 in stage 28.0 (TID 87) in 62 ms on localhost (3/4)\n",
      "15/10/04 18:07:03 INFO PythonRDD: Times: total = 55, boot = -1875, init = 1930, finish = 0\n",
      "15/10/04 18:07:03 INFO Executor: Finished task 2.0 in stage 28.0 (TID 86). 1524 bytes result sent to driver\n",
      "15/10/04 18:07:03 INFO TaskSetManager: Finished task 2.0 in stage 28.0 (TID 86) in 67 ms on localhost (4/4)\n",
      "15/10/04 18:07:03 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:07:03 INFO DAGScheduler: ShuffleMapStage 28 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.070 s\n",
      "15/10/04 18:07:03 INFO DAGScheduler: looking for newly runnable stages\n",
      "15/10/04 18:07:03 INFO DAGScheduler: running: Set()\n",
      "15/10/04 18:07:03 INFO DAGScheduler: waiting: Set(ResultStage 29)\n",
      "15/10/04 18:07:03 INFO DAGScheduler: failed: Set()\n",
      "15/10/04 18:07:03 INFO DAGScheduler: Missing parents for ResultStage 29: List()\n",
      "15/10/04 18:07:03 INFO DAGScheduler: Submitting ResultStage 29 (PythonRDD[89] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "15/10/04 18:07:03 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=124386, maxMem=278302556\n",
      "15/10/04 18:07:03 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 5.9 KB, free 265.3 MB)\n",
      "15/10/04 18:07:03 INFO MemoryStore: ensureFreeSpace(3412) called with curMem=130426, maxMem=278302556\n",
      "15/10/04 18:07:03 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 3.3 KB, free 265.3 MB)\n",
      "15/10/04 18:07:03 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on localhost:52448 (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:07:03 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:07:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (PythonRDD[89] at RDD at PythonRDD.scala:43)\n",
      "15/10/04 18:07:03 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks\n",
      "15/10/04 18:07:03 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 88, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:07:03 INFO Executor: Running task 0.0 in stage 29.0 (TID 88)\n",
      "15/10/04 18:07:03 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 4 blocks\n",
      "15/10/04 18:07:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:07:03 INFO PythonRDD: Times: total = 2, boot = -44, init = 45, finish = 1\n",
      "15/10/04 18:07:03 INFO PythonRDD: Times: total = 4, boot = -1943, init = 1947, finish = 0\n",
      "15/10/04 18:07:03 INFO Executor: Finished task 0.0 in stage 29.0 (TID 88). 932 bytes result sent to driver\n",
      "15/10/04 18:07:03 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 88) in 13 ms on localhost (1/1)\n",
      "15/10/04 18:07:03 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:07:03 INFO DAGScheduler: ResultStage 29 (runJob at PythonRDD.scala:366) finished in 0.015 s\n",
      "15/10/04 18:07:03 INFO DAGScheduler: Job 14 finished: runJob at PythonRDD.scala:366, took 0.113676 s\n",
      "15/10/04 18:07:03 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/04 18:07:03 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 7 is 153 bytes\n",
      "15/10/04 18:07:03 INFO DAGScheduler: Got job 15 (runJob at PythonRDD.scala:366) with 3 output partitions (allowLocal=true)\n",
      "15/10/04 18:07:03 INFO DAGScheduler: Final stage: ResultStage 31(runJob at PythonRDD.scala:366)\n",
      "15/10/04 18:07:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\n",
      "15/10/04 18:07:03 INFO DAGScheduler: Missing parents: List()\n",
      "15/10/04 18:07:03 INFO DAGScheduler: Submitting ResultStage 31 (PythonRDD[90] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "15/10/04 18:07:03 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=133838, maxMem=278302556\n",
      "15/10/04 18:07:03 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 5.9 KB, free 265.3 MB)\n",
      "15/10/04 18:07:03 INFO MemoryStore: ensureFreeSpace(3412) called with curMem=139878, maxMem=278302556\n",
      "15/10/04 18:07:03 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 3.3 KB, free 265.3 MB)\n",
      "15/10/04 18:07:03 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on localhost:52448 (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:07:03 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:07:03 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 31 (PythonRDD[90] at RDD at PythonRDD.scala:43)\n",
      "15/10/04 18:07:03 INFO TaskSchedulerImpl: Adding task set 31.0 with 3 tasks\n",
      "15/10/04 18:07:03 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 89, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:07:03 INFO TaskSetManager: Starting task 1.0 in stage 31.0 (TID 90, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:07:03 INFO TaskSetManager: Starting task 2.0 in stage 31.0 (TID 91, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:07:03 INFO Executor: Running task 0.0 in stage 31.0 (TID 89)\n",
      "15/10/04 18:07:03 INFO Executor: Running task 1.0 in stage 31.0 (TID 90)\n",
      "15/10/04 18:07:03 INFO Executor: Running task 2.0 in stage 31.0 (TID 91)\n",
      "15/10/04 18:07:03 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 4 blocks\n",
      "15/10/04 18:07:03 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 4 blocks\n",
      "15/10/04 18:07:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:07:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:07:03 INFO PythonRDD: Times: total = 1, boot = -36, init = 37, finish = 0\n",
      "15/10/04 18:07:03 INFO PythonRDD: Times: total = 2, boot = -66, init = 68, finish = 0\n",
      "15/10/04 18:07:03 INFO PythonRDD: Times: total = 5, boot = -69, init = 74, finish = 0\n",
      "15/10/04 18:07:03 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 4 blocks\n",
      "15/10/04 18:07:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:07:03 INFO PythonRDD: Times: total = 8, boot = -73, init = 81, finish = 0\n",
      "15/10/04 18:07:03 INFO PythonRDD: Times: total = 10, boot = -18, init = 28, finish = 0\n",
      "15/10/04 18:07:03 INFO Executor: Finished task 2.0 in stage 31.0 (TID 91). 932 bytes result sent to driver\n",
      "15/10/04 18:07:03 INFO Executor: Finished task 1.0 in stage 31.0 (TID 90). 932 bytes result sent to driver\n",
      "15/10/04 18:07:03 INFO TaskSetManager: Finished task 2.0 in stage 31.0 (TID 91) in 23 ms on localhost (1/3)\n",
      "15/10/04 18:07:03 INFO TaskSetManager: Finished task 1.0 in stage 31.0 (TID 90) in 25 ms on localhost (2/3)\n",
      "15/10/04 18:07:03 INFO PythonRDD: Times: total = 24, boot = -43, init = 66, finish = 1\n",
      "15/10/04 18:07:03 INFO Executor: Finished task 0.0 in stage 31.0 (TID 89). 932 bytes result sent to driver\n",
      "15/10/04 18:07:03 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 89) in 47 ms on localhost (3/3)\n",
      "15/10/04 18:07:03 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:07:03 INFO DAGScheduler: ResultStage 31 (runJob at PythonRDD.scala:366) finished in 0.048 s\n",
      "15/10/04 18:07:03 INFO DAGScheduler: Job 15 finished: runJob at PythonRDD.scala:366, took 0.058771 s\n",
      "15/10/04 18:07:03 INFO JobScheduler: Finished job streaming job 1444007223000 ms.0 from job set of time 1444007223000 ms\n",
      "15/10/04 18:07:03 INFO JobScheduler: Total delay: 0.251 s for time 1444007223000 ms (execution: 0.203 s)\n",
      "15/10/04 18:07:03 INFO PythonRDD: Removing RDD 78 from persistence list\n",
      "15/10/04 18:07:03 INFO BlockManager: Removing RDD 78\n",
      "15/10/04 18:07:03 INFO UnionRDD: Removing RDD 73 from persistence list\n",
      "15/10/04 18:07:03 INFO PythonRDD: Removing RDD 52 from persistence list\n",
      "15/10/04 18:07:03 INFO BlockManager: Removing RDD 73\n",
      "15/10/04 18:07:03 INFO PythonRDD: Removing RDD 51 from persistence list\n",
      "15/10/04 18:07:03 INFO BlockManager: Removing RDD 52\n",
      "15/10/04 18:07:03 INFO ParallelCollectionRDD: Removing RDD 10 from persistence list\n",
      "15/10/04 18:07:03 INFO BlockManager: Removing RDD 51\n",
      "15/10/04 18:07:03 INFO ParallelCollectionRDD: Removing RDD 10 from persistence list\n",
      "15/10/04 18:07:03 INFO BlockManager: Removing RDD 10\n",
      "15/10/04 18:07:03 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "15/10/04 18:07:03 INFO BlockManager: Removing RDD 10\n",
      "15/10/04 18:07:03 INFO InputInfoTracker: remove old batch metadata: \n",
      "15/10/04 18:07:04 INFO PythonTransformedDStream: Time 1444007224000 ms is invalid as zeroTime is 1444007207000 ms and slideDuration is 2000 ms and difference is 17000 ms\n",
      "15/10/04 18:07:04 INFO JobScheduler: No jobs added for time 1444007224000 ms\n",
      "15/10/04 18:07:05 INFO PythonTransformedDStream: Slicing from 1444007222000 ms to 1444007225000 ms (aligned to 1444007222000 ms and 1444007225000 ms)\n",
      "15/10/04 18:07:05 INFO JobScheduler: Added jobs for time 1444007225000 ms\n",
      "15/10/04 18:07:05 INFO JobScheduler: Starting job streaming job 1444007225000 ms.0 from job set of time 1444007225000 ms\n",
      "15/10/04 18:07:05 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/04 18:07:05 INFO DAGScheduler: Registering RDD 95 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/04 18:07:05 INFO DAGScheduler: Got job 16 (runJob at PythonRDD.scala:366) with 1 output partitions (allowLocal=true)\n",
      "15/10/04 18:07:05 INFO DAGScheduler: Final stage: ResultStage 33(runJob at PythonRDD.scala:366)\n",
      "15/10/04 18:07:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)\n",
      "15/10/04 18:07:05 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 32)\n",
      "15/10/04 18:07:05 INFO DAGScheduler: Submitting ShuffleMapStage 32 (PairwiseRDD[95] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "15/10/04 18:07:05 INFO MemoryStore: ensureFreeSpace(11648) called with curMem=143238, maxMem=278302556\n",
      "15/10/04 18:07:05 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 11.4 KB, free 265.3 MB)\n",
      "15/10/04 18:07:05 INFO MemoryStore: ensureFreeSpace(5222) called with curMem=154886, maxMem=278302556\n",
      "15/10/04 18:07:05 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 5.1 KB, free 265.3 MB)\n",
      "15/10/04 18:07:05 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on localhost:52448 (size: 5.1 KB, free: 265.4 MB)\n",
      "15/10/04 18:07:05 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:07:05 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 32 (PairwiseRDD[95] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/04 18:07:05 INFO TaskSchedulerImpl: Adding task set 32.0 with 4 tasks\n",
      "15/10/04 18:07:05 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 92, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:07:05 INFO TaskSetManager: Starting task 1.0 in stage 32.0 (TID 93, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:07:05 INFO TaskSetManager: Starting task 2.0 in stage 32.0 (TID 94, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:07:05 INFO TaskSetManager: Starting task 3.0 in stage 32.0 (TID 95, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:07:05 INFO Executor: Running task 0.0 in stage 32.0 (TID 92)\n",
      "15/10/04 18:07:05 INFO Executor: Running task 1.0 in stage 32.0 (TID 93)\n",
      "15/10/04 18:07:05 INFO Executor: Running task 2.0 in stage 32.0 (TID 94)\n",
      "15/10/04 18:07:05 INFO Executor: Running task 3.0 in stage 32.0 (TID 95)\n",
      "15/10/04 18:07:05 INFO BlockManager: Found block rdd_81_0 locally\n",
      "15/10/04 18:07:05 INFO PythonRDD: Times: total = 2, boot = -1876, init = 1878, finish = 0\n",
      "15/10/04 18:07:05 INFO BlockManager: Found block rdd_82_0 locally\n",
      "15/10/04 18:07:05 INFO Executor: Finished task 0.0 in stage 32.0 (TID 92). 2066 bytes result sent to driver\n",
      "15/10/04 18:07:05 INFO CacheManager: Partition rdd_91_0 not found, computing it\n",
      "15/10/04 18:07:05 INFO CacheManager: Partition rdd_92_0 not found, computing it\n",
      "15/10/04 18:07:05 INFO PythonRDD: Times: total = 6, boot = -1827, init = 1833, finish = 0\n",
      "15/10/04 18:07:05 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 92) in 17 ms on localhost (1/4)\n",
      "15/10/04 18:07:05 INFO PythonRDD: Times: total = 5, boot = -1830, init = 1835, finish = 0\n",
      "15/10/04 18:07:05 INFO MemoryStore: ensureFreeSpace(26) called with curMem=160108, maxMem=278302556\n",
      "15/10/04 18:07:05 INFO MemoryStore: Block rdd_91_0 stored as bytes in memory (estimated size 26.0 B, free 265.3 MB)\n",
      "15/10/04 18:07:05 INFO Executor: Finished task 1.0 in stage 32.0 (TID 93). 2066 bytes result sent to driver\n",
      "15/10/04 18:07:05 INFO BlockManagerInfo: Added rdd_91_0 in memory on localhost:52448 (size: 26.0 B, free: 265.4 MB)\n",
      "15/10/04 18:07:05 INFO TaskSetManager: Finished task 1.0 in stage 32.0 (TID 93) in 29 ms on localhost (2/4)\n",
      "15/10/04 18:07:05 INFO PythonRDD: Times: total = 25, boot = -1825, init = 1850, finish = 0\n",
      "15/10/04 18:07:05 INFO PythonRDD: Times: total = 19, boot = -1826, init = 1844, finish = 1\n",
      "15/10/04 18:07:05 INFO MemoryStore: ensureFreeSpace(26) called with curMem=160134, maxMem=278302556\n",
      "15/10/04 18:07:05 INFO MemoryStore: Block rdd_92_0 stored as bytes in memory (estimated size 26.0 B, free 265.3 MB)\n",
      "15/10/04 18:07:05 INFO Executor: Finished task 2.0 in stage 32.0 (TID 94). 1524 bytes result sent to driver\n",
      "15/10/04 18:07:05 INFO BlockManagerInfo: Added rdd_92_0 in memory on localhost:52448 (size: 26.0 B, free: 265.4 MB)\n",
      "15/10/04 18:07:05 INFO PythonRDD: Times: total = 34, boot = -1824, init = 1858, finish = 0\n",
      "15/10/04 18:07:05 INFO TaskSetManager: Finished task 2.0 in stage 32.0 (TID 94) in 42 ms on localhost (3/4)\n",
      "15/10/04 18:07:05 INFO Executor: Finished task 3.0 in stage 32.0 (TID 95). 1524 bytes result sent to driver\n",
      "15/10/04 18:07:05 INFO TaskSetManager: Finished task 3.0 in stage 32.0 (TID 95) in 45 ms on localhost (4/4)\n",
      "15/10/04 18:07:05 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:07:05 INFO DAGScheduler: ShuffleMapStage 32 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.048 s\n",
      "15/10/04 18:07:05 INFO DAGScheduler: looking for newly runnable stages\n",
      "15/10/04 18:07:05 INFO DAGScheduler: running: Set()\n",
      "15/10/04 18:07:05 INFO DAGScheduler: waiting: Set(ResultStage 33)\n",
      "15/10/04 18:07:05 INFO DAGScheduler: failed: Set()\n",
      "15/10/04 18:07:05 INFO DAGScheduler: Missing parents for ResultStage 33: List()\n",
      "15/10/04 18:07:05 INFO DAGScheduler: Submitting ResultStage 33 (PythonRDD[99] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "15/10/04 18:07:05 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=160160, maxMem=278302556\n",
      "15/10/04 18:07:05 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 5.9 KB, free 265.3 MB)\n",
      "15/10/04 18:07:05 INFO MemoryStore: ensureFreeSpace(3411) called with curMem=166200, maxMem=278302556\n",
      "15/10/04 18:07:05 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 3.3 KB, free 265.2 MB)\n",
      "15/10/04 18:07:05 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on localhost:52448 (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:07:05 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:07:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (PythonRDD[99] at RDD at PythonRDD.scala:43)\n",
      "15/10/04 18:07:05 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks\n",
      "15/10/04 18:07:05 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 96, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:07:05 INFO Executor: Running task 0.0 in stage 33.0 (TID 96)\n",
      "15/10/04 18:07:05 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 4 blocks\n",
      "15/10/04 18:07:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:07:05 INFO PythonRDD: Times: total = 1, boot = -30, init = 31, finish = 0\n",
      "15/10/04 18:07:05 INFO PythonRDD: Times: total = 3, boot = -1868, init = 1871, finish = 0\n",
      "15/10/04 18:07:05 INFO Executor: Finished task 0.0 in stage 33.0 (TID 96). 932 bytes result sent to driver\n",
      "15/10/04 18:07:05 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 96) in 7 ms on localhost (1/1)\n",
      "15/10/04 18:07:05 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:07:05 INFO DAGScheduler: ResultStage 33 (runJob at PythonRDD.scala:366) finished in 0.009 s\n",
      "15/10/04 18:07:05 INFO DAGScheduler: Job 16 finished: runJob at PythonRDD.scala:366, took 0.081902 s\n",
      "15/10/04 18:07:05 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/04 18:07:05 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 8 is 153 bytes\n",
      "15/10/04 18:07:05 INFO DAGScheduler: Got job 17 (runJob at PythonRDD.scala:366) with 3 output partitions (allowLocal=true)\n",
      "15/10/04 18:07:05 INFO DAGScheduler: Final stage: ResultStage 35(runJob at PythonRDD.scala:366)\n",
      "15/10/04 18:07:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)\n",
      "15/10/04 18:07:05 INFO DAGScheduler: Missing parents: List()\n",
      "15/10/04 18:07:05 INFO DAGScheduler: Submitting ResultStage 35 (PythonRDD[100] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "15/10/04 18:07:05 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=169611, maxMem=278302556\n",
      "15/10/04 18:07:05 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 5.9 KB, free 265.2 MB)\n",
      "15/10/04 18:07:05 INFO MemoryStore: ensureFreeSpace(3411) called with curMem=175651, maxMem=278302556\n",
      "15/10/04 18:07:05 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 3.3 KB, free 265.2 MB)\n",
      "15/10/04 18:07:05 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on localhost:52448 (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:07:05 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:07:05 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 35 (PythonRDD[100] at RDD at PythonRDD.scala:43)\n",
      "15/10/04 18:07:05 INFO TaskSchedulerImpl: Adding task set 35.0 with 3 tasks\n",
      "15/10/04 18:07:05 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 97, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:07:05 INFO TaskSetManager: Starting task 1.0 in stage 35.0 (TID 98, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:07:05 INFO TaskSetManager: Starting task 2.0 in stage 35.0 (TID 99, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:07:05 INFO Executor: Running task 0.0 in stage 35.0 (TID 97)\n",
      "15/10/04 18:07:05 INFO Executor: Running task 2.0 in stage 35.0 (TID 99)\n",
      "15/10/04 18:07:05 INFO Executor: Running task 1.0 in stage 35.0 (TID 98)\n",
      "15/10/04 18:07:05 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 4 blocks\n",
      "15/10/04 18:07:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:07:05 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 4 blocks\n",
      "15/10/04 18:07:05 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 4 blocks\n",
      "15/10/04 18:07:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:07:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:07:05 INFO PythonRDD: Times: total = 1, boot = -36, init = 37, finish = 0\n",
      "15/10/04 18:07:05 INFO PythonRDD: Times: total = 2, boot = -25, init = 26, finish = 1\n",
      "15/10/04 18:07:05 INFO PythonRDD: Times: total = 3, boot = -54, init = 57, finish = 0\n",
      "15/10/04 18:07:05 INFO PythonRDD: Times: total = 2, boot = -45, init = 47, finish = 0\n",
      "15/10/04 18:07:05 INFO Executor: Finished task 1.0 in stage 35.0 (TID 98). 932 bytes result sent to driver\n",
      "15/10/04 18:07:05 INFO Executor: Finished task 0.0 in stage 35.0 (TID 97). 932 bytes result sent to driver\n",
      "15/10/04 18:07:05 INFO TaskSetManager: Finished task 1.0 in stage 35.0 (TID 98) in 7 ms on localhost (1/3)\n",
      "15/10/04 18:07:05 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 97) in 7 ms on localhost (2/3)\n",
      "15/10/04 18:07:05 INFO PythonRDD: Times: total = 12, boot = -8, init = 10, finish = 10\n",
      "15/10/04 18:07:05 INFO PythonRDD: Times: total = 13, boot = -27, init = 40, finish = 0\n",
      "15/10/04 18:07:05 INFO Executor: Finished task 2.0 in stage 35.0 (TID 99). 932 bytes result sent to driver\n",
      "15/10/04 18:07:05 INFO TaskSetManager: Finished task 2.0 in stage 35.0 (TID 99) in 17 ms on localhost (3/3)\n",
      "15/10/04 18:07:05 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:07:05 INFO DAGScheduler: ResultStage 35 (runJob at PythonRDD.scala:366) finished in 0.018 s\n",
      "15/10/04 18:07:05 INFO DAGScheduler: Job 17 finished: runJob at PythonRDD.scala:366, took 0.031204 s\n",
      "15/10/04 18:07:05 INFO JobScheduler: Finished job streaming job 1444007225000 ms.0 from job set of time 1444007225000 ms\n",
      "15/10/04 18:07:05 INFO JobScheduler: Total delay: 0.176 s for time 1444007225000 ms (execution: 0.137 s)\n",
      "15/10/04 18:07:05 INFO PythonRDD: Removing RDD 88 from persistence list\n",
      "15/10/04 18:07:05 INFO BlockManager: Removing RDD 88\n",
      "15/10/04 18:07:05 INFO UnionRDD: Removing RDD 83 from persistence list\n",
      "15/10/04 18:07:05 INFO PythonRDD: Removing RDD 62 from persistence list\n",
      "15/10/04 18:07:05 INFO BlockManager: Removing RDD 83\n",
      "15/10/04 18:07:05 INFO PythonRDD: Removing RDD 61 from persistence list\n",
      "15/10/04 18:07:05 INFO BlockManager: Removing RDD 62\n",
      "15/10/04 18:07:05 INFO BlockManager: Removing RDD 61\n",
      "15/10/04 18:07:05 INFO ParallelCollectionRDD: Removing RDD 10 from persistence list\n",
      "15/10/04 18:07:05 INFO ParallelCollectionRDD: Removing RDD 10 from persistence list\n",
      "15/10/04 18:07:05 INFO BlockManager: Removing RDD 10\n",
      "15/10/04 18:07:05 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "15/10/04 18:07:05 INFO BlockManager: Removing RDD 10\n",
      "15/10/04 18:07:05 INFO InputInfoTracker: remove old batch metadata: \n",
      "15/10/04 18:07:06 INFO PythonTransformedDStream: Time 1444007226000 ms is invalid as zeroTime is 1444007207000 ms and slideDuration is 2000 ms and difference is 19000 ms\n",
      "15/10/04 18:07:06 INFO JobScheduler: No jobs added for time 1444007226000 ms\n",
      "15/10/04 18:07:07 INFO PythonTransformedDStream: Slicing from 1444007224000 ms to 1444007227000 ms (aligned to 1444007224000 ms and 1444007227000 ms)\n",
      "15/10/04 18:07:07 INFO JobScheduler: Added jobs for time 1444007227000 ms\n",
      "15/10/04 18:07:07 INFO JobScheduler: Starting job streaming job 1444007227000 ms.0 from job set of time 1444007227000 ms\n",
      "15/10/04 18:07:07 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/04 18:07:07 INFO DAGScheduler: Registering RDD 105 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/04 18:07:07 INFO DAGScheduler: Got job 18 (runJob at PythonRDD.scala:366) with 1 output partitions (allowLocal=true)\n",
      "15/10/04 18:07:07 INFO DAGScheduler: Final stage: ResultStage 37(runJob at PythonRDD.scala:366)\n",
      "15/10/04 18:07:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)\n",
      "15/10/04 18:07:07 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 36)\n",
      "15/10/04 18:07:07 INFO DAGScheduler: Submitting ShuffleMapStage 36 (PairwiseRDD[105] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "15/10/04 18:07:07 INFO MemoryStore: ensureFreeSpace(11648) called with curMem=179010, maxMem=278302556\n",
      "15/10/04 18:07:07 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 11.4 KB, free 265.2 MB)\n",
      "15/10/04 18:07:07 INFO MemoryStore: ensureFreeSpace(5222) called with curMem=190658, maxMem=278302556\n",
      "15/10/04 18:07:07 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 5.1 KB, free 265.2 MB)\n",
      "15/10/04 18:07:07 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on localhost:52448 (size: 5.1 KB, free: 265.3 MB)\n",
      "15/10/04 18:07:07 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:07:07 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 36 (PairwiseRDD[105] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/04 18:07:07 INFO TaskSchedulerImpl: Adding task set 36.0 with 4 tasks\n",
      "15/10/04 18:07:07 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 100, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:07:07 INFO TaskSetManager: Starting task 1.0 in stage 36.0 (TID 101, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:07:07 INFO TaskSetManager: Starting task 2.0 in stage 36.0 (TID 102, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:07:07 INFO TaskSetManager: Starting task 3.0 in stage 36.0 (TID 103, localhost, PROCESS_LOCAL, 1542 bytes)\n",
      "15/10/04 18:07:07 INFO Executor: Running task 0.0 in stage 36.0 (TID 100)\n",
      "15/10/04 18:07:07 INFO Executor: Running task 1.0 in stage 36.0 (TID 101)\n",
      "15/10/04 18:07:07 INFO Executor: Running task 2.0 in stage 36.0 (TID 102)\n",
      "15/10/04 18:07:07 INFO Executor: Running task 3.0 in stage 36.0 (TID 103)\n",
      "15/10/04 18:07:07 INFO BlockManager: Found block rdd_91_0 locally\n",
      "15/10/04 18:07:07 INFO PythonRDD: Times: total = 2, boot = -1919, init = 1921, finish = 0\n",
      "15/10/04 18:07:07 INFO BlockManager: Found block rdd_92_0 locally\n",
      "15/10/04 18:07:07 INFO PythonRDD: Times: total = 3, boot = -1865, init = 1868, finish = 0\n",
      "15/10/04 18:07:07 INFO Executor: Finished task 0.0 in stage 36.0 (TID 100). 2066 bytes result sent to driver\n",
      "15/10/04 18:07:07 INFO CacheManager: Partition rdd_101_0 not found, computing it\n",
      "15/10/04 18:07:07 INFO CacheManager: Partition rdd_102_0 not found, computing it\n",
      "15/10/04 18:07:07 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 100) in 12 ms on localhost (1/4)\n",
      "15/10/04 18:07:07 INFO Executor: Finished task 1.0 in stage 36.0 (TID 101). 2066 bytes result sent to driver\n",
      "15/10/04 18:07:07 INFO TaskSetManager: Finished task 1.0 in stage 36.0 (TID 101) in 15 ms on localhost (2/4)\n",
      "15/10/04 18:07:07 INFO PythonRDD: Times: total = 6, boot = -1877, init = 1883, finish = 0\n",
      "15/10/04 18:07:07 INFO MemoryStore: ensureFreeSpace(26) called with curMem=195880, maxMem=278302556\n",
      "15/10/04 18:07:07 INFO MemoryStore: Block rdd_101_0 stored as bytes in memory (estimated size 26.0 B, free 265.2 MB)\n",
      "15/10/04 18:07:07 INFO BlockManagerInfo: Added rdd_101_0 in memory on localhost:52448 (size: 26.0 B, free: 265.3 MB)\n",
      "15/10/04 18:07:07 INFO PythonRDD: Times: total = 23, boot = -1867, init = 1890, finish = 0\n",
      "15/10/04 18:07:07 INFO Executor: Finished task 2.0 in stage 36.0 (TID 102). 1524 bytes result sent to driver\n",
      "15/10/04 18:07:07 INFO PythonRDD: Times: total = 25, boot = -1869, init = 1894, finish = 0\n",
      "15/10/04 18:07:07 INFO MemoryStore: ensureFreeSpace(26) called with curMem=195906, maxMem=278302556\n",
      "15/10/04 18:07:07 INFO MemoryStore: Block rdd_102_0 stored as bytes in memory (estimated size 26.0 B, free 265.2 MB)\n",
      "15/10/04 18:07:07 INFO BlockManagerInfo: Added rdd_102_0 in memory on localhost:52448 (size: 26.0 B, free: 265.3 MB)\n",
      "15/10/04 18:07:07 INFO TaskSetManager: Finished task 2.0 in stage 36.0 (TID 102) in 42 ms on localhost (3/4)\n",
      "15/10/04 18:07:07 INFO PythonRDD: Times: total = 41, boot = -1880, init = 1921, finish = 0\n",
      "15/10/04 18:07:07 INFO Executor: Finished task 3.0 in stage 36.0 (TID 103). 1524 bytes result sent to driver\n",
      "15/10/04 18:07:07 INFO TaskSetManager: Finished task 3.0 in stage 36.0 (TID 103) in 53 ms on localhost (4/4)\n",
      "15/10/04 18:07:07 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:07:07 INFO DAGScheduler: ShuffleMapStage 36 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.060 s\n",
      "15/10/04 18:07:07 INFO DAGScheduler: looking for newly runnable stages\n",
      "15/10/04 18:07:07 INFO DAGScheduler: running: Set()\n",
      "15/10/04 18:07:07 INFO DAGScheduler: waiting: Set(ResultStage 37)\n",
      "15/10/04 18:07:07 INFO DAGScheduler: failed: Set()\n",
      "15/10/04 18:07:07 INFO DAGScheduler: Missing parents for ResultStage 37: List()\n",
      "15/10/04 18:07:07 INFO DAGScheduler: Submitting ResultStage 37 (PythonRDD[109] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "15/10/04 18:07:07 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=195932, maxMem=278302556\n",
      "15/10/04 18:07:07 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 5.9 KB, free 265.2 MB)\n",
      "15/10/04 18:07:07 INFO MemoryStore: ensureFreeSpace(3412) called with curMem=201972, maxMem=278302556\n",
      "15/10/04 18:07:07 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 3.3 KB, free 265.2 MB)\n",
      "15/10/04 18:07:07 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on localhost:52448 (size: 3.3 KB, free: 265.3 MB)\n",
      "15/10/04 18:07:07 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:07:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (PythonRDD[109] at RDD at PythonRDD.scala:43)\n",
      "15/10/04 18:07:07 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks\n",
      "15/10/04 18:07:07 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 104, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:07:07 INFO Executor: Running task 0.0 in stage 37.0 (TID 104)\n",
      "15/10/04 18:07:07 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 4 blocks\n",
      "15/10/04 18:07:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:07:07 INFO PythonRDD: Times: total = 2, boot = -46, init = 48, finish = 0\n",
      "15/10/04 18:07:07 INFO PythonRDD: Times: total = 3, boot = -1940, init = 1943, finish = 0\n",
      "15/10/04 18:07:07 INFO Executor: Finished task 0.0 in stage 37.0 (TID 104). 932 bytes result sent to driver\n",
      "15/10/04 18:07:07 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 104) in 7 ms on localhost (1/1)\n",
      "15/10/04 18:07:07 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:07:07 INFO DAGScheduler: ResultStage 37 (runJob at PythonRDD.scala:366) finished in 0.009 s\n",
      "15/10/04 18:07:07 INFO DAGScheduler: Job 18 finished: runJob at PythonRDD.scala:366, took 0.101802 s\n",
      "15/10/04 18:07:07 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/04 18:07:07 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 9 is 153 bytes\n",
      "15/10/04 18:07:07 INFO DAGScheduler: Got job 19 (runJob at PythonRDD.scala:366) with 3 output partitions (allowLocal=true)\n",
      "15/10/04 18:07:07 INFO DAGScheduler: Final stage: ResultStage 39(runJob at PythonRDD.scala:366)\n",
      "15/10/04 18:07:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 38)\n",
      "15/10/04 18:07:07 INFO DAGScheduler: Missing parents: List()\n",
      "15/10/04 18:07:07 INFO DAGScheduler: Submitting ResultStage 39 (PythonRDD[110] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "15/10/04 18:07:07 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=205384, maxMem=278302556\n",
      "15/10/04 18:07:07 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 5.9 KB, free 265.2 MB)\n",
      "15/10/04 18:07:07 INFO MemoryStore: ensureFreeSpace(3412) called with curMem=211424, maxMem=278302556\n",
      "15/10/04 18:07:07 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 3.3 KB, free 265.2 MB)\n",
      "15/10/04 18:07:07 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on localhost:52448 (size: 3.3 KB, free: 265.3 MB)\n",
      "15/10/04 18:07:07 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:874\n",
      "15/10/04 18:07:07 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 39 (PythonRDD[110] at RDD at PythonRDD.scala:43)\n",
      "15/10/04 18:07:07 INFO TaskSchedulerImpl: Adding task set 39.0 with 3 tasks\n",
      "15/10/04 18:07:07 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 105, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:07:07 INFO TaskSetManager: Starting task 1.0 in stage 39.0 (TID 106, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:07:07 INFO TaskSetManager: Starting task 2.0 in stage 39.0 (TID 107, localhost, PROCESS_LOCAL, 1238 bytes)\n",
      "15/10/04 18:07:07 INFO Executor: Running task 0.0 in stage 39.0 (TID 105)\n",
      "15/10/04 18:07:07 INFO Executor: Running task 1.0 in stage 39.0 (TID 106)\n",
      "15/10/04 18:07:07 INFO Executor: Running task 2.0 in stage 39.0 (TID 107)\n",
      "15/10/04 18:07:07 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 4 blocks\n",
      "15/10/04 18:07:07 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 4 blocks\n",
      "15/10/04 18:07:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/04 18:07:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "15/10/04 18:07:07 INFO PythonRDD: Times: total = 17, boot = -57, init = 74, finish = 0\n",
      "15/10/04 18:07:07 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 4 blocks\n",
      "15/10/04 18:07:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "15/10/04 18:07:07 INFO PythonRDD: Times: total = 19, boot = -67, init = 86, finish = 0\n",
      "15/10/04 18:07:07 INFO BlockManagerInfo: Removed broadcast_28_piece0 on localhost:52448 in memory (size: 3.3 KB, free: 265.3 MB)\n",
      "15/10/04 18:07:07 INFO Executor: Finished task 1.0 in stage 39.0 (TID 106). 932 bytes result sent to driver\n",
      "15/10/04 18:07:07 INFO TaskSetManager: Finished task 1.0 in stage 39.0 (TID 106) in 23 ms on localhost (1/3)\n",
      "15/10/04 18:07:07 INFO BlockManagerInfo: Removed broadcast_27_piece0 on localhost:52448 in memory (size: 5.1 KB, free: 265.3 MB)\n",
      "15/10/04 18:07:07 INFO BlockManagerInfo: Removed broadcast_26_piece0 on localhost:52448 in memory (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:07:07 INFO BlockManagerInfo: Removed broadcast_25_piece0 on localhost:52448 in memory (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:07:07 INFO PythonRDD: Times: total = 28, boot = -41, init = 68, finish = 1\n",
      "15/10/04 18:07:07 INFO PythonRDD: Times: total = 29, boot = -76, init = 105, finish = 0\n",
      "15/10/04 18:07:07 INFO BlockManagerInfo: Removed broadcast_24_piece0 on localhost:52448 in memory (size: 5.1 KB, free: 265.4 MB)\n",
      "15/10/04 18:07:07 INFO Executor: Finished task 0.0 in stage 39.0 (TID 105). 932 bytes result sent to driver\n",
      "15/10/04 18:07:07 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 105) in 34 ms on localhost (2/3)\n",
      "15/10/04 18:07:07 INFO BlockManagerInfo: Removed broadcast_23_piece0 on localhost:52448 in memory (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:07:07 INFO BlockManagerInfo: Removed broadcast_22_piece0 on localhost:52448 in memory (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:07:07 INFO BlockManagerInfo: Removed broadcast_21_piece0 on localhost:52448 in memory (size: 5.1 KB, free: 265.4 MB)\n",
      "15/10/04 18:07:07 INFO ContextCleaner: Cleaned shuffle 7\n",
      "15/10/04 18:07:07 INFO BlockManagerInfo: Removed broadcast_20_piece0 on localhost:52448 in memory (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:07:07 INFO BlockManagerInfo: Removed broadcast_19_piece0 on localhost:52448 in memory (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:07:07 INFO BlockManagerInfo: Removed broadcast_18_piece0 on localhost:52448 in memory (size: 5.1 KB, free: 265.4 MB)\n",
      "15/10/04 18:07:07 INFO ContextCleaner: Cleaned shuffle 6\n",
      "15/10/04 18:07:07 INFO BlockManagerInfo: Removed broadcast_17_piece0 on localhost:52448 in memory (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:07:07 INFO BlockManagerInfo: Removed broadcast_16_piece0 on localhost:52448 in memory (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:07:07 INFO BlockManagerInfo: Removed broadcast_15_piece0 on localhost:52448 in memory (size: 5.1 KB, free: 265.4 MB)\n",
      "15/10/04 18:07:07 INFO ContextCleaner: Cleaned shuffle 5\n",
      "15/10/04 18:07:07 INFO BlockManager: Removing RDD 62\n",
      "15/10/04 18:07:07 INFO ContextCleaner: Cleaned RDD 62\n",
      "15/10/04 18:07:07 INFO BlockManager: Removing RDD 61\n",
      "15/10/04 18:07:07 INFO ContextCleaner: Cleaned RDD 61\n",
      "15/10/04 18:07:07 INFO PythonRDD: Times: total = 50, boot = -27, init = 77, finish = 0\n",
      "15/10/04 18:07:07 INFO PythonRDD: Times: total = 51, boot = -46, init = 97, finish = 0\n",
      "15/10/04 18:07:07 INFO Executor: Finished task 2.0 in stage 39.0 (TID 107). 932 bytes result sent to driver\n",
      "15/10/04 18:07:07 INFO TaskSetManager: Finished task 2.0 in stage 39.0 (TID 107) in 73 ms on localhost (3/3)\n",
      "15/10/04 18:07:07 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool \n",
      "15/10/04 18:07:07 INFO DAGScheduler: ResultStage 39 (runJob at PythonRDD.scala:366) finished in 0.075 s\n",
      "15/10/04 18:07:07 INFO DAGScheduler: Job 19 finished: runJob at PythonRDD.scala:366, took 0.086649 s\n",
      "15/10/04 18:07:07 INFO BlockManagerInfo: Removed broadcast_14_piece0 on localhost:52448 in memory (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:07:07 INFO JobScheduler: Finished job streaming job 1444007227000 ms.0 from job set of time 1444007227000 ms\n",
      "15/10/04 18:07:07 INFO JobScheduler: Total delay: 0.259 s for time 1444007227000 ms (execution: 0.229 s)\n",
      "15/10/04 18:07:07 INFO PythonRDD: Removing RDD 98 from persistence list\n",
      "15/10/04 18:07:07 INFO BlockManager: Removing RDD 98\n",
      "15/10/04 18:07:07 INFO UnionRDD: Removing RDD 93 from persistence list\n",
      "15/10/04 18:07:07 INFO BlockManagerInfo: Removed broadcast_13_piece0 on localhost:52448 in memory (size: 3.3 KB, free: 265.4 MB)\n",
      "15/10/04 18:07:07 INFO BlockManager: Removing RDD 93\n",
      "15/10/04 18:07:07 INFO PythonRDD: Removing RDD 72 from persistence list\n",
      "15/10/04 18:07:07 INFO BlockManager: Removing RDD 72\n",
      "15/10/04 18:07:07 INFO PythonRDD: Removing RDD 71 from persistence list\n",
      "15/10/04 18:07:07 INFO BlockManagerInfo: Removed broadcast_12_piece0 on localhost:52448 in memory (size: 5.1 KB, free: 265.4 MB)\n",
      "15/10/04 18:07:07 INFO BlockManager: Removing RDD 71\n",
      "15/10/04 18:07:07 INFO ParallelCollectionRDD: Removing RDD 10 from persistence list\n",
      "15/10/04 18:07:07 INFO ContextCleaner: Cleaned shuffle 4\n",
      "15/10/04 18:07:07 INFO ParallelCollectionRDD: Removing RDD 10 from persistence list\n",
      "15/10/04 18:07:07 INFO BlockManager: Removing RDD 10\n",
      "15/10/04 18:07:07 INFO BlockManager: Removing RDD 52\n",
      "15/10/04 18:07:07 INFO ContextCleaner: Cleaned RDD 52\n",
      "15/10/04 18:07:07 INFO BlockManager: Removing RDD 10\n",
      "15/10/04 18:07:07 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "15/10/04 18:07:07 INFO InputInfoTracker: remove old batch metadata: \n",
      "15/10/04 18:07:07 INFO BlockManager: Removing RDD 51\n",
      "15/10/04 18:07:07 INFO ContextCleaner: Cleaned RDD 51\n",
      "15/10/04 18:07:07 INFO ContextCleaner: Cleaned shuffle 3\n",
      "15/10/04 18:07:07 INFO BlockManager: Removing RDD 42\n",
      "15/10/04 18:07:07 INFO ContextCleaner: Cleaned RDD 42\n",
      "15/10/04 18:07:07 INFO BlockManager: Removing RDD 41\n",
      "15/10/04 18:07:07 INFO ContextCleaner: Cleaned RDD 41\n",
      "15/10/04 18:07:07 INFO BlockManager: Removing RDD 32\n",
      "15/10/04 18:07:07 INFO ContextCleaner: Cleaned RDD 32\n",
      "15/10/04 18:07:07 INFO BlockManager: Removing RDD 31\n",
      "15/10/04 18:07:07 INFO ContextCleaner: Cleaned RDD 31\n",
      "15/10/04 18:07:07 INFO ContextCleaner: Cleaned shuffle 1\n",
      "15/10/04 18:07:07 INFO BlockManager: Removing RDD 21\n",
      "15/10/04 18:07:07 INFO ContextCleaner: Cleaned RDD 21\n",
      "15/10/04 18:07:07 INFO BlockManager: Removing RDD 20\n",
      "15/10/04 18:07:07 INFO ContextCleaner: Cleaned RDD 20\n",
      "15/10/04 18:07:07 INFO ContextCleaner: Cleaned shuffle 0\n",
      "15/10/04 18:07:07 INFO BlockManager: Removing RDD 12\n",
      "15/10/04 18:07:07 INFO ContextCleaner: Cleaned RDD 12\n",
      "15/10/04 18:07:07 INFO BlockManager: Removing RDD 11\n",
      "15/10/04 18:07:07 INFO ContextCleaner: Cleaned RDD 11\n",
      "15/10/04 18:07:07 INFO JobGenerator: Stopping JobGenerator gracefully\n",
      "15/10/04 18:07:07 INFO JobGenerator: Waiting for all received blocks to be consumed for job generation\n",
      "15/10/04 18:07:07 INFO JobGenerator: Waited for all received blocks to be consumed for job generation\n",
      "15/10/04 18:07:08 INFO PythonTransformedDStream: Time 1444007228000 ms is invalid as zeroTime is 1444007207000 ms and slideDuration is 2000 ms and difference is 21000 ms\n",
      "15/10/04 18:07:08 INFO JobScheduler: No jobs added for time 1444007228000 ms\n",
      "15/10/04 18:07:08 INFO RecurringTimer: Stopped timer for JobGenerator after time 1444007228000\n",
      "15/10/04 18:07:08 INFO JobGenerator: Stopped generation timer\n",
      "15/10/04 18:07:08 INFO JobGenerator: Waiting for jobs to be processed and checkpoints to be written\n",
      "15/10/04 18:07:17 WARN JobGenerator: Timed out while stopping the job generator (timeout = 10000)\n",
      "15/10/04 18:07:17 INFO JobGenerator: Waited for jobs to be processed and checkpoints to be written\n",
      "15/10/04 18:07:17 INFO JobGenerator: Stopped JobGenerator\n",
      "15/10/04 18:07:17 INFO JobScheduler: Stopped JobScheduler\n",
      "15/10/04 18:07:17 INFO StreamingContext: StreamingContext stopped successfully\n",
      "15/10/04 18:07:18 INFO SparkUI: Stopped Spark web UI at http://10.0.0.9:4043\n",
      "15/10/04 18:07:18 INFO DAGScheduler: Stopping DAGScheduler\n",
      "15/10/04 18:07:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "15/10/04 18:07:18 INFO Utils: path = /private/var/folders/vx/9464rz3s72b0l50g6wwpr2hm0000gn/T/spark-a8890cd7-845e-489d-8c60-b14b2e729b6c/blockmgr-059ebf67-f1a1-4abe-81cf-40176bcb0ccb, already present as root for deletion.\n",
      "15/10/04 18:07:18 INFO MemoryStore: MemoryStore cleared\n",
      "15/10/04 18:07:18 INFO BlockManager: BlockManager stopped\n",
      "15/10/04 18:07:18 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "15/10/04 18:07:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "15/10/04 18:07:18 INFO SparkContext: Successfully stopped SparkContext\n",
      "15/10/04 18:07:18 INFO SparkContext: SparkContext already stopped.\n",
      "15/10/04 18:07:18 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.\n",
      "15/10/04 18:07:18 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.\n",
      "15/10/04 18:07:18 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.\n",
      "15/10/04 18:07:18 INFO Utils: Shutdown hook called\n",
      "15/10/04 18:07:18 INFO Utils: Deleting directory /private/var/folders/vx/9464rz3s72b0l50g6wwpr2hm0000gn/T/spark-a8890cd7-845e-489d-8c60-b14b2e729b6c/pyspark-e0192817-f892-45ed-afd0-f8d6c559d0a2\n",
      "15/10/04 18:07:18 INFO Utils: Deleting directory /private/var/folders/vx/9464rz3s72b0l50g6wwpr2hm0000gn/T/spark-a8890cd7-845e-489d-8c60-b14b2e729b6c\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "$SPARK_HOME/bin/spark-submit test_window.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Windowing Operations With Inverse\n",
    "---------------------------------\n",
    "\n",
    "Q: How can I avoid the overhead of adding or averaging over the same\n",
    "values in a window?\n",
    "\n",
    "```python\n",
    "windows_word_counts = pair_ds.reduceByKeyAndWindow(\n",
    "    func=lambda x, y: x + y,\n",
    "    invFunc=lambda x, y: x - y, \n",
    "    windowDuration=30,\n",
    "    slideDuration=10)\n",
    "```\n",
    "\n",
    "- Creates window of length `windowDuration` (30 seconds)\n",
    "\n",
    "- Moves window every `slideDuration` (10 seconds)\n",
    "\n",
    "- Merges incoming values using `func`\n",
    "\n",
    "- Eliminates outgoing values using `invFunc`\n",
    "\n",
    "- `windowDuration` and `slideDuration` are in seconds\n",
    "\n",
    "- These must be multiples of the `batchDuration` of the DStream\n",
    "\n",
    "- This requires that *checkpointing* is enabled on the StreamingContext.\n",
    "\n",
    "<img src=\"images/streaming-windowed-stream.png\">\n",
    "\n",
    "<img src=\"images/streaming-windowed-stream-with-inv.png\">\n",
    "\n",
    "Streaming Durations\n",
    "-------------------\n",
    "\n",
    "Q: What are the different durations in a DStream and which one should\n",
    "I use?\n",
    "\n",
    "Type               |Meaning\n",
    "----               |-------\n",
    "Batch Duration     |How many seconds until next incoming RDD\n",
    "Slide Duration     |How many seconds until next window RDD\n",
    "Window Duration    |How many seconds to include in window RDD\n",
    "\n",
    "Duration Impact\n",
    "---------------\n",
    "\n",
    "Q: What is the impact of increasing these durations?\n",
    "\n",
    "Type                 |Increase                                   |Effect \n",
    "----                 |--------                                   |------ \n",
    "Batch Duration       |Larger but less frequent incoming RDDs     |Less Processing \n",
    "Slide Duration       |Less frequent window RDDs                  |Less Processing\n",
    "Window Duration      |Larger window RDDs                         |More Processing\n",
    "\n",
    "Duration Summary\n",
    "----------------\n",
    "\n",
    "- Batch and window duration control RDD size\n",
    "\n",
    "- Batch and slide duration control RDD frequency\n",
    "\n",
    "- Larger RDDs have more context and produce better insights.\n",
    "\n",
    "- Larger RDDs might require more processing.\n",
    "\n",
    "- Bundling frequent small RDDs into infrequent larger ones can reduce processing.\n",
    "\n",
    "State DStreams\n",
    "--------------\n",
    "\n",
    "Q: How can I aggregate a value over the lifetime of a streaming\n",
    "application?\n",
    "\n",
    "- You can do this with the `updateStateByKey` transform.\n",
    "\n",
    "```python\n",
    "# add new values with previous running count to get new count\n",
    "def updateFunction(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "       runningCount = 0\n",
    "    return sum(newValues, runningCount)  \n",
    "\n",
    "runningCounts = pairs.updateStateByKey(updateFunction)\n",
    "```\n",
    "\n",
    "- This takes a DStream made up of key-value RDDs\n",
    "\n",
    "- For each incoming RDD for each key it aggregates the values with the\n",
    "  previous values seen for that key.\n",
    "\n",
    "- Like the windowing transformations, this requires that checkpointing\n",
    "  be enabled on the StreamingContext.\n",
    "\n",
    "Testing Streaming Apps Using TextFileStream\n",
    "-------------------------------------------\n",
    "\n",
    "Q: The QueueStream does not work with windowing operations or any\n",
    "other operations that require checkpointing. How can code that uses\n",
    "`updateStateByKey` be tested? \n",
    "\n",
    "- We can use TextFileStream instead.\n",
    "- Lets define a function `xrange_write` which we will use for the following examples.\n",
    "- This will write numbers 0, 1, 2, ... to directory `input`.\n",
    "- It will write 5 numbers per second, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing text_file_util.py\n"
     ]
    }
   ],
   "source": [
    "%%file text_file_util.py\n",
    "import itertools\n",
    "import time\n",
    "import random\n",
    "import uuid\n",
    "\n",
    "from distutils import dir_util \n",
    "\n",
    "# Every batch_duration write a file with batch_size numbers, forever.\n",
    "# Start at 0 and keep incrementing. (For testing.)\n",
    "\n",
    "def xrange_write(\n",
    "        batch_size = 5,\n",
    "        batch_dir = 'input',\n",
    "        batch_duration = 1):\n",
    "    dir_util.mkpath('./input')\n",
    "    \n",
    "    # Repeat forever\n",
    "    for i in itertools.count():\n",
    "        # Generate data\n",
    "        min = batch_size * i \n",
    "        max = batch_size * (i + 1)\n",
    "        batch_data = xrange(min,max)\n",
    "      \n",
    "        # Write to the file\n",
    "        unique_file_name = str(uuid.uuid4())\n",
    "        file_path = batch_dir + '/' + unique_file_name\n",
    "        with open(file_path,'w') as batch_file: \n",
    "            for element in batch_data:\n",
    "                line = str(element) + \"\\n\"\n",
    "                batch_file.write(line)\n",
    "    \n",
    "        # Give streaming app time to catch up\n",
    "        time.sleep(batch_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting Events\n",
    "---------------\n",
    "\n",
    "Q: How can I count a certain type of event in incoming data?\n",
    "\n",
    "- You can use state DStreams.\n",
    "\n",
    "- This code takes a mod by 10 of the incoming numbers.\n",
    "\n",
    "- Then it counts how many times each number between 0 and 9 is seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_count.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_count.py\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from text_file_util import xrange_write\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# add new values with previous running count to get new count\n",
    "def updateFunction(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    return sum(newValues, runningCount)  \n",
    "\n",
    "print 'Initializing ssc'\n",
    "ssc = StreamingContext(SparkContext(), batchDuration=1)\n",
    "ssc.checkpoint('ckpt')\n",
    "\n",
    "ds = ssc.textFileStream('input') \\\n",
    "    .map(lambda x: int(x) % 10) \\\n",
    "    .map(lambda x: (x,1)) \\\n",
    "    .updateStateByKey(updateFunction)\n",
    "\n",
    "ds.pprint()\n",
    "ds.count().pprint()\n",
    "\n",
    "print 'Starting ssc'\n",
    "ssc.start()\n",
    "\n",
    "# Write data to textFileStream\n",
    "xrange_write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets run this and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is terminated.\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "$SPARK_HOME/bin/spark-submit test_count.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The program will run forever. To terminate hit `Ctrl-C`.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: How can you calculate a running average using a state DStream?\n",
    "</summary>\n",
    "1. In the above example, for the RDD key-value pair, replace `value`\n",
    "with `(sum,count)`. <br>\n",
    "2. In `updateStateByKey` add both to `sum` and `count`.<br>\n",
    "3. Use `map` to calculate `sum/count` which is the average.<br>\n",
    "</details>\n",
    "\n",
    "<!--\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: How can you calculate a running standard deviation using a state DStream?\n",
    "</summary>\n",
    "1. See https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm<br>  <-- Wrong!\n",
    "</details>\n",
    "-->\n",
    "\n",
    "\n",
    "\n",
    "Join \n",
    "----\n",
    "\n",
    "Q: How can I detect if an incoming credit card transaction is from a\n",
    "canceled card?\n",
    "\n",
    "- You can join DStreams against a batch RDD.\n",
    "- Store the historical data in the batch RDD.\n",
    "- Join it with the incoming DStream RDDs to determine next action.\n",
    "- Note: You must get the batch RDD using the `ssc.SparkContext`.\n",
    "\n",
    "```python\n",
    "dataset = ... # some RDD\n",
    "windowedStream = stream.window(20)\n",
    "joinedStream = windowedStream.transform(lambda rdd: rdd.join(dataset))\n",
    "```\n",
    "\n",
    "Detecting Bad Customers\n",
    "-----------------------\n",
    "Q: Create a streaming app that can join the incoming orders with our\n",
    "previous knowledge of whether this customer is good or bad.\n",
    "\n",
    "- Create the streaming app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_join.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_join.py\n",
    "# Import modules.\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import time\n",
    "\n",
    "# Create the StreamingContext.\n",
    "\n",
    "print 'Initializing ssc'\n",
    "ssc = StreamingContext(SparkContext(), batchDuration=1)\n",
    "\n",
    "\n",
    "# For testing create prepopulated QueueStream of streaming customer orders. \n",
    "\n",
    "print 'Initializing queue of customer transactions'\n",
    "transaction_rdd_queue = []\n",
    "for i in xrange(5): \n",
    "    transactions = [(customer_id, None) for customer_id in xrange(10)]\n",
    "    transaction_rdd = ssc.sparkContext.parallelize(transactions)\n",
    "    transaction_rdd_queue.append(transaction_rdd)\n",
    "pprint(transaction_rdd_queue)\n",
    "\n",
    "# Batch RDD of whether customers are good or bad. \n",
    "\n",
    "print 'Initializing bad customer rdd from batch sources'\n",
    "# (customer_id, is_good_customer)\n",
    "customers = [\n",
    "        (0,True),\n",
    "        (1,False),\n",
    "        (2,True),\n",
    "        (3,False),\n",
    "        (4,True),\n",
    "        (5,False),\n",
    "        (6,True),\n",
    "        (7,False),\n",
    "        (8,True),\n",
    "        (9,False) ]\n",
    "customer_rdd = ssc.sparkContext.parallelize(customers)\n",
    "\n",
    "# Join the streaming RDD and batch RDDs to filter out bad customers.\n",
    "print 'Creating queue stream'\n",
    "ds = ssc\\\n",
    "    .queueStream(transaction_rdd_queue)\\\n",
    "    .transform(lambda rdd: rdd.join(customer_rdd))\\\n",
    "    .filter(lambda (customer_id, (customer_data, is_good_customer)): is_good_customer)\n",
    "\n",
    "ds.pprint()\n",
    "\n",
    "ssc.start()\n",
    "time.sleep(6)\n",
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets run this and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ssc\n",
      "Initializing queue of customer transactions\n",
      "[ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:396,\n",
      " ParallelCollectionRDD[1] at parallelize at PythonRDD.scala:396,\n",
      " ParallelCollectionRDD[2] at parallelize at PythonRDD.scala:396,\n",
      " ParallelCollectionRDD[3] at parallelize at PythonRDD.scala:396,\n",
      " ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:396]\n",
      "Initializing bad customer rdd from batch sources\n",
      "Creating queue stream\n",
      "-------------------------------------------\n",
      "Time: 2015-10-03 19:02:41\n",
      "-------------------------------------------\n",
      "(0, (None, True))\n",
      "(8, (None, True))\n",
      "(2, (None, True))\n",
      "(4, (None, True))\n",
      "(6, (None, True))\n",
      "()\n",
      "-------------------------------------------\n",
      "Time: 2015-10-03 19:02:42\n",
      "-------------------------------------------\n",
      "(0, (None, True))\n",
      "(8, (None, True))\n",
      "(2, (None, True))\n",
      "(4, (None, True))\n",
      "(6, (None, True))\n",
      "()\n",
      "-------------------------------------------\n",
      "Time: 2015-10-03 19:02:43\n",
      "-------------------------------------------\n",
      "(0, (None, True))\n",
      "(8, (None, True))\n",
      "(2, (None, True))\n",
      "(4, (None, True))\n",
      "(6, (None, True))\n",
      "()\n",
      "-------------------------------------------\n",
      "Time: 2015-10-03 19:02:44\n",
      "-------------------------------------------\n",
      "(0, (None, True))\n",
      "(8, (None, True))\n",
      "(2, (None, True))\n",
      "(4, (None, True))\n",
      "(6, (None, True))\n",
      "()\n",
      "-------------------------------------------\n",
      "Time: 2015-10-03 19:02:45\n",
      "-------------------------------------------\n",
      "(0, (None, True))\n",
      "(8, (None, True))\n",
      "(2, (None, True))\n",
      "(4, (None, True))\n",
      "(6, (None, True))\n",
      "()\n",
      "-------------------------------------------\n",
      "Time: 2015-10-03 19:02:46\n",
      "-------------------------------------------\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "15/10/03 19:02:36 INFO SparkContext: Running Spark version 1.4.1\n",
      "15/10/03 19:02:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/10/03 19:02:37 INFO SecurityManager: Changing view acls to: Alexander\n",
      "15/10/03 19:02:37 INFO SecurityManager: Changing modify acls to: Alexander\n",
      "15/10/03 19:02:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(Alexander); users with modify permissions: Set(Alexander)\n",
      "15/10/03 19:02:38 INFO Slf4jLogger: Slf4jLogger started\n",
      "15/10/03 19:02:38 INFO Remoting: Starting remoting\n",
      "15/10/03 19:02:38 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.0.0.9:65029]\n",
      "15/10/03 19:02:38 INFO Utils: Successfully started service 'sparkDriver' on port 65029.\n",
      "15/10/03 19:02:38 INFO SparkEnv: Registering MapOutputTracker\n",
      "15/10/03 19:02:38 INFO SparkEnv: Registering BlockManagerMaster\n",
      "15/10/03 19:02:38 INFO DiskBlockManager: Created local directory at /private/var/folders/vx/9464rz3s72b0l50g6wwpr2hm0000gn/T/spark-9d30a6a7-1fba-4666-9b8e-83314e13730b/blockmgr-6f6dedbd-2726-49d7-b455-f0d08342e375\n",
      "15/10/03 19:02:38 INFO MemoryStore: MemoryStore started with capacity 265.4 MB\n",
      "15/10/03 19:02:38 INFO HttpFileServer: HTTP File server directory is /private/var/folders/vx/9464rz3s72b0l50g6wwpr2hm0000gn/T/spark-9d30a6a7-1fba-4666-9b8e-83314e13730b/httpd-3c50e172-6eba-4c39-8051-c0ed97ef72b5\n",
      "15/10/03 19:02:38 INFO HttpServer: Starting HTTP Server\n",
      "15/10/03 19:02:38 INFO Utils: Successfully started service 'HTTP file server' on port 65030.\n",
      "15/10/03 19:02:38 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "15/10/03 19:02:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "15/10/03 19:02:38 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "15/10/03 19:02:38 INFO SparkUI: Started SparkUI at http://10.0.0.9:4041\n",
      "15/10/03 19:02:39 INFO Utils: Copying /Users/Alexander/DSCI6007-student/week6/6.4/test_join.py to /private/var/folders/vx/9464rz3s72b0l50g6wwpr2hm0000gn/T/spark-9d30a6a7-1fba-4666-9b8e-83314e13730b/userFiles-03cdd851-c295-44c7-a772-722e527a0512/test_join.py\n",
      "15/10/03 19:02:39 INFO SparkContext: Added file file:/Users/Alexander/DSCI6007-student/week6/6.4/test_join.py at file:/Users/Alexander/DSCI6007-student/week6/6.4/test_join.py with timestamp 1443924159056\n",
      "15/10/03 19:02:39 INFO Executor: Starting executor ID driver on host localhost\n",
      "15/10/03 19:02:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65040.\n",
      "15/10/03 19:02:39 INFO NettyBlockTransferService: Server created on 65040\n",
      "15/10/03 19:02:39 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "15/10/03 19:02:39 INFO BlockManagerMasterEndpoint: Registering block manager localhost:65040 with 265.4 MB RAM, BlockManagerId(driver, localhost, 65040)\n",
      "15/10/03 19:02:39 INFO BlockManagerMaster: Registered BlockManager\n",
      "15/10/03 19:02:40 INFO ForEachDStream: metadataCleanupDelay = -1\n",
      "15/10/03 19:02:40 INFO PythonTransformedDStream: metadataCleanupDelay = -1\n",
      "15/10/03 19:02:40 INFO QueueInputDStream: metadataCleanupDelay = -1\n",
      "15/10/03 19:02:40 INFO QueueInputDStream: Slide time = 1000 ms\n",
      "15/10/03 19:02:40 INFO QueueInputDStream: Storage level = StorageLevel(false, false, false, false, 1)\n",
      "15/10/03 19:02:40 INFO QueueInputDStream: Checkpoint interval = null\n",
      "15/10/03 19:02:40 INFO QueueInputDStream: Remember duration = 1000 ms\n",
      "15/10/03 19:02:40 INFO QueueInputDStream: Initialized and validated org.apache.spark.streaming.dstream.QueueInputDStream@20195f05\n",
      "15/10/03 19:02:40 INFO PythonTransformedDStream: Slide time = 1000 ms\n",
      "15/10/03 19:02:40 INFO PythonTransformedDStream: Storage level = StorageLevel(false, false, false, false, 1)\n",
      "15/10/03 19:02:40 INFO PythonTransformedDStream: Checkpoint interval = null\n",
      "15/10/03 19:02:40 INFO PythonTransformedDStream: Remember duration = 1000 ms\n",
      "15/10/03 19:02:40 INFO PythonTransformedDStream: Initialized and validated org.apache.spark.streaming.api.python.PythonTransformedDStream@61ad51b9\n",
      "15/10/03 19:02:40 INFO ForEachDStream: Slide time = 1000 ms\n",
      "15/10/03 19:02:40 INFO ForEachDStream: Storage level = StorageLevel(false, false, false, false, 1)\n",
      "15/10/03 19:02:40 INFO ForEachDStream: Checkpoint interval = null\n",
      "15/10/03 19:02:40 INFO ForEachDStream: Remember duration = 1000 ms\n",
      "15/10/03 19:02:40 INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@39d07ac8\n",
      "15/10/03 19:02:40 INFO RecurringTimer: Started timer for JobGenerator at time 1443924161000\n",
      "15/10/03 19:02:40 INFO JobGenerator: Started JobGenerator at 1443924161000 ms\n",
      "15/10/03 19:02:40 INFO JobScheduler: Started JobScheduler\n",
      "15/10/03 19:02:40 INFO StreamingContext: StreamingContext started\n",
      "15/10/03 19:02:41 INFO JobScheduler: Added jobs for time 1443924161000 ms\n",
      "15/10/03 19:02:41 INFO JobScheduler: Starting job streaming job 1443924161000 ms.0 from job set of time 1443924161000 ms\n",
      "15/10/03 19:02:41 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/03 19:02:41 INFO DAGScheduler: Registering RDD 11 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/03 19:02:41 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:366) with 1 output partitions (allowLocal=true)\n",
      "15/10/03 19:02:41 INFO DAGScheduler: Final stage: ResultStage 1(runJob at PythonRDD.scala:366)\n",
      "15/10/03 19:02:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
      "15/10/03 19:02:41 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
      "15/10/03 19:02:41 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[11] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "15/10/03 19:02:41 INFO MemoryStore: ensureFreeSpace(9440) called with curMem=0, maxMem=278302556\n",
      "15/10/03 19:02:41 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.2 KB, free 265.4 MB)\n",
      "15/10/03 19:02:41 INFO MemoryStore: ensureFreeSpace(5001) called with curMem=9440, maxMem=278302556\n",
      "15/10/03 19:02:41 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.9 KB, free 265.4 MB)\n",
      "15/10/03 19:02:41 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:65040 (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:41 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:874\n",
      "15/10/03 19:02:41 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 0 (PairwiseRDD[11] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/03 19:02:41 INFO TaskSchedulerImpl: Adding task set 0.0 with 8 tasks\n",
      "15/10/03 19:02:41 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:41 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:41 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:41 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1584 bytes)\n",
      "15/10/03 19:02:41 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "15/10/03 19:02:41 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
      "15/10/03 19:02:41 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
      "15/10/03 19:02:41 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)\n",
      "15/10/03 19:02:41 INFO Executor: Fetching file:/Users/Alexander/DSCI6007-student/week6/6.4/test_join.py with timestamp 1443924159056\n",
      "15/10/03 19:02:41 INFO Utils: /Users/Alexander/DSCI6007-student/week6/6.4/test_join.py has been previously copied to /private/var/folders/vx/9464rz3s72b0l50g6wwpr2hm0000gn/T/spark-9d30a6a7-1fba-4666-9b8e-83314e13730b/userFiles-03cdd851-c295-44c7-a772-722e527a0512/test_join.py\n",
      "15/10/03 19:02:42 INFO JobScheduler: Added jobs for time 1443924162000 ms\n",
      "15/10/03 19:02:43 INFO JobScheduler: Added jobs for time 1443924163000 ms\n",
      "15/10/03 19:02:43 INFO PythonRDD: Times: total = 7, boot = 5, init = 2, finish = 0\n",
      "15/10/03 19:02:43 INFO PythonRDD: Times: total = 10, boot = 8, init = 2, finish = 0\n",
      "15/10/03 19:02:43 INFO PythonRDD: Times: total = 16, boot = 12, init = 3, finish = 1\n",
      "15/10/03 19:02:43 INFO PythonRDD: Times: total = 1868, boot = 1836, init = 31, finish = 1\n",
      "15/10/03 19:02:43 INFO PythonRDD: Times: total = 1855, boot = 1833, init = 20, finish = 2\n",
      "15/10/03 19:02:43 INFO PythonRDD: Times: total = 1855, boot = 1839, init = 14, finish = 2\n",
      "15/10/03 19:02:43 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 948 bytes result sent to driver\n",
      "15/10/03 19:02:43 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 948 bytes result sent to driver\n",
      "15/10/03 19:02:43 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 948 bytes result sent to driver\n",
      "15/10/03 19:02:43 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:43 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)\n",
      "15/10/03 19:02:43 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:43 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)\n",
      "15/10/03 19:02:43 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:43 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)\n",
      "15/10/03 19:02:43 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2096 ms on localhost (1/8)\n",
      "15/10/03 19:02:43 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 2086 ms on localhost (2/8)\n",
      "15/10/03 19:02:43 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2087 ms on localhost (3/8)\n",
      "15/10/03 19:02:43 INFO PythonRDD: Times: total = 53, boot = 52, init = 1, finish = 0\n",
      "15/10/03 19:02:43 INFO PythonRDD: Times: total = 236, boot = 235, init = 0, finish = 1\n",
      "15/10/03 19:02:43 INFO PythonRDD: Times: total = 65, boot = 62, init = 2, finish = 1\n",
      "15/10/03 19:02:43 INFO PythonRDD: Times: total = 2086, boot = 1841, init = 244, finish = 1\n",
      "15/10/03 19:02:43 INFO PythonRDD: Times: total = 74, boot = 63, init = 11, finish = 0\n",
      "15/10/03 19:02:43 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 948 bytes result sent to driver\n",
      "15/10/03 19:02:43 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, localhost, PROCESS_LOCAL, 1584 bytes)\n",
      "15/10/03 19:02:43 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)\n",
      "15/10/03 19:02:43 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 2196 ms on localhost (4/8)\n",
      "15/10/03 19:02:43 INFO PythonRDD: Times: total = 87, boot = 86, init = 1, finish = 0\n",
      "15/10/03 19:02:43 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 948 bytes result sent to driver\n",
      "15/10/03 19:02:43 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 180 ms on localhost (5/8)\n",
      "15/10/03 19:02:43 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 948 bytes result sent to driver\n",
      "15/10/03 19:02:43 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 188 ms on localhost (6/8)\n",
      "15/10/03 19:02:43 INFO PythonRDD: Times: total = 145, boot = 145, init = 0, finish = 0\n",
      "15/10/03 19:02:43 INFO PythonRDD: Times: total = 192, boot = 127, init = 64, finish = 1\n",
      "15/10/03 19:02:43 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 948 bytes result sent to driver\n",
      "15/10/03 19:02:43 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 227 ms on localhost (7/8)\n",
      "15/10/03 19:02:43 INFO PythonRDD: Times: total = 69, boot = 68, init = 1, finish = 0\n",
      "15/10/03 19:02:43 INFO PythonRDD: Times: total = 78, boot = 76, init = 1, finish = 1\n",
      "15/10/03 19:02:43 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 948 bytes result sent to driver\n",
      "15/10/03 19:02:43 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 109 ms on localhost (8/8)\n",
      "15/10/03 19:02:43 INFO DAGScheduler: ShuffleMapStage 0 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 2.326 s\n",
      "15/10/03 19:02:43 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "15/10/03 19:02:43 INFO DAGScheduler: looking for newly runnable stages\n",
      "15/10/03 19:02:43 INFO DAGScheduler: running: Set()\n",
      "15/10/03 19:02:43 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
      "15/10/03 19:02:43 INFO DAGScheduler: failed: Set()\n",
      "15/10/03 19:02:43 INFO DAGScheduler: Missing parents for ResultStage 1: List()\n",
      "15/10/03 19:02:43 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[15] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "15/10/03 19:02:43 INFO MemoryStore: ensureFreeSpace(9048) called with curMem=14441, maxMem=278302556\n",
      "15/10/03 19:02:43 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.8 KB, free 265.4 MB)\n",
      "15/10/03 19:02:43 INFO MemoryStore: ensureFreeSpace(5037) called with curMem=23489, maxMem=278302556\n",
      "15/10/03 19:02:43 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.9 KB, free 265.4 MB)\n",
      "15/10/03 19:02:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:65040 (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:43 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:874\n",
      "15/10/03 19:02:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[15] at RDD at PythonRDD.scala:43)\n",
      "15/10/03 19:02:43 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks\n",
      "15/10/03 19:02:43 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 8, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:43 INFO Executor: Running task 0.0 in stage 1.0 (TID 8)\n",
      "15/10/03 19:02:43 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
      "15/10/03 19:02:43 INFO PythonRDD: Times: total = 34, boot = -50, init = 84, finish = 0\n",
      "15/10/03 19:02:43 INFO PythonRDD: Times: total = 36, boot = -50, init = 86, finish = 0\n",
      "15/10/03 19:02:43 INFO Executor: Finished task 0.0 in stage 1.0 (TID 8). 995 bytes result sent to driver\n",
      "15/10/03 19:02:43 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 8) in 45 ms on localhost (1/1)\n",
      "15/10/03 19:02:43 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "15/10/03 19:02:43 INFO DAGScheduler: ResultStage 1 (runJob at PythonRDD.scala:366) finished in 0.045 s\n",
      "15/10/03 19:02:43 INFO DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:366, took 2.731624 s\n",
      "15/10/03 19:02:43 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/03 19:02:43 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 175 bytes\n",
      "15/10/03 19:02:43 INFO DAGScheduler: Got job 1 (runJob at PythonRDD.scala:366) with 4 output partitions (allowLocal=true)\n",
      "15/10/03 19:02:43 INFO DAGScheduler: Final stage: ResultStage 3(runJob at PythonRDD.scala:366)\n",
      "15/10/03 19:02:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
      "15/10/03 19:02:43 INFO DAGScheduler: Missing parents: List()\n",
      "15/10/03 19:02:43 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[32] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "15/10/03 19:02:43 INFO MemoryStore: ensureFreeSpace(9048) called with curMem=28526, maxMem=278302556\n",
      "15/10/03 19:02:43 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 8.8 KB, free 265.4 MB)\n",
      "15/10/03 19:02:43 INFO MemoryStore: ensureFreeSpace(5037) called with curMem=37574, maxMem=278302556\n",
      "15/10/03 19:02:43 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.9 KB, free 265.4 MB)\n",
      "15/10/03 19:02:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:65040 (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:43 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:874\n",
      "15/10/03 19:02:43 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 3 (PythonRDD[32] at RDD at PythonRDD.scala:43)\n",
      "15/10/03 19:02:43 INFO TaskSchedulerImpl: Adding task set 3.0 with 4 tasks\n",
      "15/10/03 19:02:43 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 9, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:43 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 10, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:43 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 11, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:43 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 12, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:43 INFO Executor: Running task 0.0 in stage 3.0 (TID 9)\n",
      "15/10/03 19:02:43 INFO Executor: Running task 2.0 in stage 3.0 (TID 11)\n",
      "15/10/03 19:02:43 INFO Executor: Running task 1.0 in stage 3.0 (TID 10)\n",
      "15/10/03 19:02:43 INFO Executor: Running task 3.0 in stage 3.0 (TID 12)\n",
      "15/10/03 19:02:43 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:43 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:43 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:43 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:43 INFO PythonRDD: Times: total = 9, boot = -141, init = 149, finish = 1\n",
      "15/10/03 19:02:43 INFO PythonRDD: Times: total = 11, boot = -151, init = 162, finish = 0\n",
      "15/10/03 19:02:43 INFO PythonRDD: Times: total = 7, boot = -135, init = 141, finish = 1\n",
      "15/10/03 19:02:43 INFO Executor: Finished task 0.0 in stage 3.0 (TID 9). 932 bytes result sent to driver\n",
      "15/10/03 19:02:43 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 9) in 18 ms on localhost (1/4)\n",
      "15/10/03 19:02:43 INFO PythonRDD: Times: total = 11, boot = -135, init = 146, finish = 0\n",
      "15/10/03 19:02:43 INFO Executor: Finished task 3.0 in stage 3.0 (TID 12). 970 bytes result sent to driver\n",
      "15/10/03 19:02:43 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 12) in 21 ms on localhost (2/4)\n",
      "15/10/03 19:02:43 INFO PythonRDD: Times: total = 17, boot = -38, init = 53, finish = 2\n",
      "15/10/03 19:02:43 INFO PythonRDD: Times: total = 27, boot = -41, init = 68, finish = 0\n",
      "15/10/03 19:02:43 INFO Executor: Finished task 2.0 in stage 3.0 (TID 11). 932 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 11) in 41 ms on localhost (3/4)\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 32, boot = -115, init = 147, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 35, boot = -122, init = 157, finish = 0\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 1.0 in stage 3.0 (TID 10). 970 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 10) in 46 ms on localhost (4/4)\n",
      "15/10/03 19:02:44 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "15/10/03 19:02:44 INFO DAGScheduler: ResultStage 3 (runJob at PythonRDD.scala:366) finished in 0.047 s\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Job 1 finished: runJob at PythonRDD.scala:366, took 0.064952 s\n",
      "15/10/03 19:02:44 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Got job 2 (runJob at PythonRDD.scala:366) with 3 output partitions (allowLocal=true)\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Final stage: ResultStage 5(runJob at PythonRDD.scala:366)\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Missing parents: List()\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Submitting ResultStage 5 (PythonRDD[35] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "15/10/03 19:02:44 INFO MemoryStore: ensureFreeSpace(9048) called with curMem=42611, maxMem=278302556\n",
      "15/10/03 19:02:44 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 8.8 KB, free 265.4 MB)\n",
      "15/10/03 19:02:44 INFO MemoryStore: ensureFreeSpace(5037) called with curMem=51659, maxMem=278302556\n",
      "15/10/03 19:02:44 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.9 KB, free 265.4 MB)\n",
      "15/10/03 19:02:44 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:65040 (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:44 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:874\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 5 (PythonRDD[35] at RDD at PythonRDD.scala:43)\n",
      "15/10/03 19:02:44 INFO TaskSchedulerImpl: Adding task set 5.0 with 3 tasks\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 13, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 14, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:44 INFO JobScheduler: Added jobs for time 1443924164000 ms\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 15, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:44 INFO Executor: Running task 1.0 in stage 5.0 (TID 14)\n",
      "15/10/03 19:02:44 INFO Executor: Running task 2.0 in stage 5.0 (TID 15)\n",
      "15/10/03 19:02:44 INFO Executor: Running task 0.0 in stage 5.0 (TID 13)\n",
      "15/10/03 19:02:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 12, boot = 10, init = 1, finish = 1\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 23, boot = 23, init = 0, finish = 0\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 0.0 in stage 5.0 (TID 13). 932 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 13) in 35 ms on localhost (1/3)\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 31, boot = 30, init = 0, finish = 1\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 41, boot = 40, init = 1, finish = 0\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 1.0 in stage 5.0 (TID 14). 970 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 14) in 51 ms on localhost (2/3)\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 52, boot = 52, init = 0, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 54, boot = 54, init = 0, finish = 0\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 2.0 in stage 5.0 (TID 15). 932 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 15) in 66 ms on localhost (3/3)\n",
      "15/10/03 19:02:44 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "15/10/03 19:02:44 INFO DAGScheduler: ResultStage 5 (runJob at PythonRDD.scala:366) finished in 0.070 s\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Job 2 finished: runJob at PythonRDD.scala:366, took 0.093719 s\n",
      "15/10/03 19:02:44 INFO JobScheduler: Finished job streaming job 1443924161000 ms.0 from job set of time 1443924161000 ms\n",
      "15/10/03 19:02:44 INFO JobScheduler: Total delay: 3.119 s for time 1443924161000 ms (execution: 2.989 s)\n",
      "15/10/03 19:02:44 INFO JobScheduler: Starting job streaming job 1443924162000 ms.0 from job set of time 1443924162000 ms\n",
      "15/10/03 19:02:44 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "15/10/03 19:02:44 INFO InputInfoTracker: remove old batch metadata: \n",
      "15/10/03 19:02:44 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Registering RDD 20 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Got job 3 (runJob at PythonRDD.scala:366) with 1 output partitions (allowLocal=true)\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Final stage: ResultStage 7(runJob at PythonRDD.scala:366)\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Submitting ShuffleMapStage 6 (PairwiseRDD[20] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "15/10/03 19:02:44 INFO MemoryStore: ensureFreeSpace(9440) called with curMem=56696, maxMem=278302556\n",
      "15/10/03 19:02:44 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 9.2 KB, free 265.3 MB)\n",
      "15/10/03 19:02:44 INFO MemoryStore: ensureFreeSpace(5003) called with curMem=66136, maxMem=278302556\n",
      "15/10/03 19:02:44 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.9 KB, free 265.3 MB)\n",
      "15/10/03 19:02:44 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:65040 (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:44 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:874\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 6 (PairwiseRDD[20] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/03 19:02:44 INFO TaskSchedulerImpl: Adding task set 6.0 with 8 tasks\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 16, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 17, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Starting task 2.0 in stage 6.0 (TID 18, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 19, localhost, PROCESS_LOCAL, 1584 bytes)\n",
      "15/10/03 19:02:44 INFO Executor: Running task 1.0 in stage 6.0 (TID 17)\n",
      "15/10/03 19:02:44 INFO Executor: Running task 0.0 in stage 6.0 (TID 16)\n",
      "15/10/03 19:02:44 INFO Executor: Running task 2.0 in stage 6.0 (TID 18)\n",
      "15/10/03 19:02:44 INFO Executor: Running task 3.0 in stage 6.0 (TID 19)\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 2, boot = -188, init = 189, finish = 1\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 14, boot = -95, init = 109, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 31, boot = 30, init = 1, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 34, boot = 25, init = 9, finish = 0\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 0.0 in stage 6.0 (TID 16). 948 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Starting task 4.0 in stage 6.0 (TID 20, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:44 INFO Executor: Running task 4.0 in stage 6.0 (TID 20)\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 16) in 62 ms on localhost (1/8)\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 2.0 in stage 6.0 (TID 18). 948 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Starting task 5.0 in stage 6.0 (TID 21, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:44 INFO Executor: Running task 5.0 in stage 6.0 (TID 21)\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 2.0 in stage 6.0 (TID 18) in 73 ms on localhost (2/8)\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 123, boot = 122, init = 1, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 126, boot = 126, init = 0, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 125, boot = 25, init = 100, finish = 0\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 1.0 in stage 6.0 (TID 17). 948 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 17) in 165 ms on localhost (3/8)\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Starting task 6.0 in stage 6.0 (TID 22, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:44 INFO Executor: Running task 6.0 in stage 6.0 (TID 22)\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 136, boot = 136, init = 0, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 138, boot = 75, init = 63, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 207, boot = 206, init = 1, finish = 0\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 4.0 in stage 6.0 (TID 20). 948 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Starting task 7.0 in stage 6.0 (TID 23, localhost, PROCESS_LOCAL, 1584 bytes)\n",
      "15/10/03 19:02:44 INFO Executor: Running task 7.0 in stage 6.0 (TID 23)\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 4.0 in stage 6.0 (TID 20) in 176 ms on localhost (4/8)\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 3.0 in stage 6.0 (TID 19). 948 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 3.0 in stage 6.0 (TID 19) in 244 ms on localhost (5/8)\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 177, boot = 177, init = 0, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 207, boot = 206, init = 1, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 127, boot = 127, init = 0, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 135, boot = 135, init = 0, finish = 0\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 5.0 in stage 6.0 (TID 21). 948 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 5.0 in stage 6.0 (TID 21) in 254 ms on localhost (6/8)\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 6.0 in stage 6.0 (TID 22). 948 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 6.0 in stage 6.0 (TID 22) in 164 ms on localhost (7/8)\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 113, boot = 112, init = 1, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 115, boot = 27, init = 88, finish = 0\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 7.0 in stage 6.0 (TID 23). 948 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 7.0 in stage 6.0 (TID 23) in 147 ms on localhost (8/8)\n",
      "15/10/03 19:02:44 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "15/10/03 19:02:44 INFO DAGScheduler: ShuffleMapStage 6 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.381 s\n",
      "15/10/03 19:02:44 INFO DAGScheduler: looking for newly runnable stages\n",
      "15/10/03 19:02:44 INFO DAGScheduler: running: Set()\n",
      "15/10/03 19:02:44 INFO DAGScheduler: waiting: Set(ResultStage 7)\n",
      "15/10/03 19:02:44 INFO DAGScheduler: failed: Set()\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Missing parents for ResultStage 7: List()\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Submitting ResultStage 7 (PythonRDD[42] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "15/10/03 19:02:44 INFO MemoryStore: ensureFreeSpace(9048) called with curMem=71139, maxMem=278302556\n",
      "15/10/03 19:02:44 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 8.8 KB, free 265.3 MB)\n",
      "15/10/03 19:02:44 INFO MemoryStore: ensureFreeSpace(5040) called with curMem=80187, maxMem=278302556\n",
      "15/10/03 19:02:44 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.9 KB, free 265.3 MB)\n",
      "15/10/03 19:02:44 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:65040 (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:44 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:874\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (PythonRDD[42] at RDD at PythonRDD.scala:43)\n",
      "15/10/03 19:02:44 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 24, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:44 INFO Executor: Running task 0.0 in stage 7.0 (TID 24)\n",
      "15/10/03 19:02:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 17, boot = -40, init = 57, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 19, boot = -86, init = 105, finish = 0\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 0.0 in stage 7.0 (TID 24). 995 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 24) in 24 ms on localhost (1/1)\n",
      "15/10/03 19:02:44 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "15/10/03 19:02:44 INFO DAGScheduler: ResultStage 7 (runJob at PythonRDD.scala:366) finished in 0.025 s\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Job 3 finished: runJob at PythonRDD.scala:366, took 0.503780 s\n",
      "15/10/03 19:02:44 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/03 19:02:44 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 175 bytes\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Got job 4 (runJob at PythonRDD.scala:366) with 4 output partitions (allowLocal=true)\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Final stage: ResultStage 9(runJob at PythonRDD.scala:366)\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Missing parents: List()\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Submitting ResultStage 9 (PythonRDD[43] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "15/10/03 19:02:44 INFO MemoryStore: ensureFreeSpace(9048) called with curMem=85227, maxMem=278302556\n",
      "15/10/03 19:02:44 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 8.8 KB, free 265.3 MB)\n",
      "15/10/03 19:02:44 INFO MemoryStore: ensureFreeSpace(5042) called with curMem=94275, maxMem=278302556\n",
      "15/10/03 19:02:44 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.9 KB, free 265.3 MB)\n",
      "15/10/03 19:02:44 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:65040 (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:44 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:874\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 9 (PythonRDD[43] at RDD at PythonRDD.scala:43)\n",
      "15/10/03 19:02:44 INFO TaskSchedulerImpl: Adding task set 9.0 with 4 tasks\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 25, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 26, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Starting task 2.0 in stage 9.0 (TID 27, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Starting task 3.0 in stage 9.0 (TID 28, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:44 INFO Executor: Running task 1.0 in stage 9.0 (TID 26)\n",
      "15/10/03 19:02:44 INFO Executor: Running task 2.0 in stage 9.0 (TID 27)\n",
      "15/10/03 19:02:44 INFO Executor: Running task 3.0 in stage 9.0 (TID 28)\n",
      "15/10/03 19:02:44 INFO Executor: Running task 0.0 in stage 9.0 (TID 25)\n",
      "15/10/03 19:02:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 8, boot = -78, init = 86, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 8, boot = -74, init = 82, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 10, boot = -103, init = 113, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 6, boot = -24, init = 30, finish = 0\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 1.0 in stage 9.0 (TID 26). 970 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 7, boot = -26, init = 33, finish = 0\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 3.0 in stage 9.0 (TID 28). 970 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 26) in 18 ms on localhost (1/4)\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 3.0 in stage 9.0 (TID 28) in 18 ms on localhost (2/4)\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 22, boot = -67, init = 88, finish = 1\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 23, boot = -103, init = 126, finish = 0\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 0.0 in stage 9.0 (TID 25). 932 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 25) in 31 ms on localhost (3/4)\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 23, boot = -68, init = 90, finish = 1\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 2.0 in stage 9.0 (TID 27). 932 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 2.0 in stage 9.0 (TID 27) in 33 ms on localhost (4/4)\n",
      "15/10/03 19:02:44 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "15/10/03 19:02:44 INFO DAGScheduler: ResultStage 9 (runJob at PythonRDD.scala:366) finished in 0.036 s\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Job 4 finished: runJob at PythonRDD.scala:366, took 0.047856 s\n",
      "15/10/03 19:02:44 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Got job 5 (runJob at PythonRDD.scala:366) with 3 output partitions (allowLocal=true)\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Final stage: ResultStage 11(runJob at PythonRDD.scala:366)\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Missing parents: List()\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Submitting ResultStage 11 (PythonRDD[44] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "15/10/03 19:02:44 INFO MemoryStore: ensureFreeSpace(9048) called with curMem=99317, maxMem=278302556\n",
      "15/10/03 19:02:44 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 8.8 KB, free 265.3 MB)\n",
      "15/10/03 19:02:44 INFO MemoryStore: ensureFreeSpace(5042) called with curMem=108365, maxMem=278302556\n",
      "15/10/03 19:02:44 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.9 KB, free 265.3 MB)\n",
      "15/10/03 19:02:44 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:65040 (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:44 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:874\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 11 (PythonRDD[44] at RDD at PythonRDD.scala:43)\n",
      "15/10/03 19:02:44 INFO TaskSchedulerImpl: Adding task set 11.0 with 3 tasks\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 29, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 30, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Starting task 2.0 in stage 11.0 (TID 31, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:44 INFO Executor: Running task 2.0 in stage 11.0 (TID 31)\n",
      "15/10/03 19:02:44 INFO Executor: Running task 0.0 in stage 11.0 (TID 29)\n",
      "15/10/03 19:02:44 INFO Executor: Running task 1.0 in stage 11.0 (TID 30)\n",
      "15/10/03 19:02:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 5, boot = -14, init = 19, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 15, boot = 14, init = 1, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 21, boot = 21, init = 0, finish = 0\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 2.0 in stage 11.0 (TID 31). 932 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 2.0 in stage 11.0 (TID 31) in 29 ms on localhost (1/3)\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 24, boot = 23, init = 0, finish = 1\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 25, boot = -7, init = 32, finish = 0\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 1.0 in stage 11.0 (TID 30). 970 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 30) in 34 ms on localhost (2/3)\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 30, boot = 29, init = 1, finish = 0\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 0.0 in stage 11.0 (TID 29). 932 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 29) in 43 ms on localhost (3/3)\n",
      "15/10/03 19:02:44 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "15/10/03 19:02:44 INFO DAGScheduler: ResultStage 11 (runJob at PythonRDD.scala:366) finished in 0.044 s\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Job 5 finished: runJob at PythonRDD.scala:366, took 0.062085 s\n",
      "15/10/03 19:02:44 INFO JobScheduler: Finished job streaming job 1443924162000 ms.0 from job set of time 1443924162000 ms\n",
      "15/10/03 19:02:44 INFO JobScheduler: Total delay: 2.789 s for time 1443924162000 ms (execution: 0.668 s)\n",
      "15/10/03 19:02:44 INFO JobScheduler: Starting job streaming job 1443924163000 ms.0 from job set of time 1443924163000 ms\n",
      "15/10/03 19:02:44 INFO PythonRDD: Removing RDD 14 from persistence list\n",
      "15/10/03 19:02:44 INFO ParallelCollectionRDD: Removing RDD 0 from persistence list\n",
      "15/10/03 19:02:44 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "15/10/03 19:02:44 INFO BlockManager: Removing RDD 14\n",
      "15/10/03 19:02:44 INFO InputInfoTracker: remove old batch metadata: \n",
      "15/10/03 19:02:44 INFO BlockManager: Removing RDD 0\n",
      "15/10/03 19:02:44 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Registering RDD 28 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Got job 6 (runJob at PythonRDD.scala:366) with 1 output partitions (allowLocal=true)\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Final stage: ResultStage 13(runJob at PythonRDD.scala:366)\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 12)\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Submitting ShuffleMapStage 12 (PairwiseRDD[28] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "15/10/03 19:02:44 INFO MemoryStore: ensureFreeSpace(9440) called with curMem=113407, maxMem=278302556\n",
      "15/10/03 19:02:44 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 9.2 KB, free 265.3 MB)\n",
      "15/10/03 19:02:44 INFO MemoryStore: ensureFreeSpace(5004) called with curMem=122847, maxMem=278302556\n",
      "15/10/03 19:02:44 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.9 KB, free 265.3 MB)\n",
      "15/10/03 19:02:44 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:65040 (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:44 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:874\n",
      "15/10/03 19:02:44 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 12 (PairwiseRDD[28] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/03 19:02:44 INFO TaskSchedulerImpl: Adding task set 12.0 with 8 tasks\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 32, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 33, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Starting task 2.0 in stage 12.0 (TID 34, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Starting task 3.0 in stage 12.0 (TID 35, localhost, PROCESS_LOCAL, 1584 bytes)\n",
      "15/10/03 19:02:44 INFO Executor: Running task 0.0 in stage 12.0 (TID 32)\n",
      "15/10/03 19:02:44 INFO Executor: Running task 2.0 in stage 12.0 (TID 34)\n",
      "15/10/03 19:02:44 INFO Executor: Running task 3.0 in stage 12.0 (TID 35)\n",
      "15/10/03 19:02:44 INFO Executor: Running task 1.0 in stage 12.0 (TID 33)\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 2, boot = -71, init = 73, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 1, boot = -13, init = 14, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 4, boot = -74, init = 78, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 11, boot = -6, init = 17, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 14, boot = 13, init = 1, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 16, boot = -36, init = 52, finish = 0\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 2.0 in stage 12.0 (TID 34). 948 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Starting task 4.0 in stage 12.0 (TID 36, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:44 INFO Executor: Running task 4.0 in stage 12.0 (TID 36)\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 2.0 in stage 12.0 (TID 34) in 34 ms on localhost (1/8)\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 3.0 in stage 12.0 (TID 35). 948 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Starting task 5.0 in stage 12.0 (TID 37, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:44 INFO Executor: Running task 5.0 in stage 12.0 (TID 37)\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 3.0 in stage 12.0 (TID 35) in 42 ms on localhost (2/8)\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 1.0 in stage 12.0 (TID 33). 948 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Starting task 6.0 in stage 12.0 (TID 38, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:44 INFO Executor: Running task 6.0 in stage 12.0 (TID 38)\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 33) in 52 ms on localhost (3/8)\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 47, boot = 47, init = 0, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 50, boot = -21, init = 70, finish = 1\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 0.0 in stage 12.0 (TID 32). 948 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Starting task 7.0 in stage 12.0 (TID 39, localhost, PROCESS_LOCAL, 1584 bytes)\n",
      "15/10/03 19:02:44 INFO Executor: Running task 7.0 in stage 12.0 (TID 39)\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 32) in 84 ms on localhost (4/8)\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 34, boot = 34, init = 0, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 56, boot = 56, init = 0, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 10, boot = 10, init = 0, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 51, boot = 51, init = 0, finish = 0\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 59, boot = 16, init = 42, finish = 1\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 54, boot = 4, init = 49, finish = 1\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 4.0 in stage 12.0 (TID 36). 948 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 4.0 in stage 12.0 (TID 36) in 86 ms on localhost (5/8)\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 5.0 in stage 12.0 (TID 37). 948 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 5.0 in stage 12.0 (TID 37) in 82 ms on localhost (6/8)\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 67, boot = 66, init = 0, finish = 1\n",
      "15/10/03 19:02:44 INFO PythonRDD: Times: total = 50, boot = 50, init = 0, finish = 0\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 6.0 in stage 12.0 (TID 38). 948 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 6.0 in stage 12.0 (TID 38) in 100 ms on localhost (7/8)\n",
      "15/10/03 19:02:44 INFO Executor: Finished task 7.0 in stage 12.0 (TID 39). 948 bytes result sent to driver\n",
      "15/10/03 19:02:44 INFO TaskSetManager: Finished task 7.0 in stage 12.0 (TID 39) in 81 ms on localhost (8/8)\n",
      "15/10/03 19:02:44 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "15/10/03 19:02:44 INFO DAGScheduler: ShuffleMapStage 12 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.166 s\n",
      "15/10/03 19:02:44 INFO DAGScheduler: looking for newly runnable stages\n",
      "15/10/03 19:02:44 INFO DAGScheduler: running: Set()\n",
      "15/10/03 19:02:44 INFO DAGScheduler: waiting: Set(ResultStage 13)\n",
      "15/10/03 19:02:44 INFO DAGScheduler: failed: Set()\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Missing parents for ResultStage 13: List()\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Submitting ResultStage 13 (PythonRDD[45] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "15/10/03 19:02:45 INFO MemoryStore: ensureFreeSpace(9048) called with curMem=127851, maxMem=278302556\n",
      "15/10/03 19:02:45 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 8.8 KB, free 265.3 MB)\n",
      "15/10/03 19:02:45 INFO MemoryStore: ensureFreeSpace(5042) called with curMem=136899, maxMem=278302556\n",
      "15/10/03 19:02:45 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.9 KB, free 265.3 MB)\n",
      "15/10/03 19:02:45 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:65040 (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:45 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:874\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (PythonRDD[45] at RDD at PythonRDD.scala:43)\n",
      "15/10/03 19:02:45 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 40, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:45 INFO Executor: Running task 0.0 in stage 13.0 (TID 40)\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "15/10/03 19:02:45 INFO JobScheduler: Added jobs for time 1443924165000 ms\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 36, boot = 13, init = 23, finish = 0\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 37, boot = 8, init = 29, finish = 0\n",
      "15/10/03 19:02:45 INFO Executor: Finished task 0.0 in stage 13.0 (TID 40). 995 bytes result sent to driver\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 40) in 45 ms on localhost (1/1)\n",
      "15/10/03 19:02:45 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "15/10/03 19:02:45 INFO DAGScheduler: ResultStage 13 (runJob at PythonRDD.scala:366) finished in 0.047 s\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Job 6 finished: runJob at PythonRDD.scala:366, took 0.252890 s\n",
      "15/10/03 19:02:45 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/03 19:02:45 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 175 bytes\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Got job 7 (runJob at PythonRDD.scala:366) with 4 output partitions (allowLocal=true)\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Final stage: ResultStage 15(runJob at PythonRDD.scala:366)\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Missing parents: List()\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Submitting ResultStage 15 (PythonRDD[54] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "15/10/03 19:02:45 INFO MemoryStore: ensureFreeSpace(9048) called with curMem=141941, maxMem=278302556\n",
      "15/10/03 19:02:45 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 8.8 KB, free 265.3 MB)\n",
      "15/10/03 19:02:45 INFO MemoryStore: ensureFreeSpace(5042) called with curMem=150989, maxMem=278302556\n",
      "15/10/03 19:02:45 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.9 KB, free 265.3 MB)\n",
      "15/10/03 19:02:45 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:65040 (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:45 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:874\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 15 (PythonRDD[54] at RDD at PythonRDD.scala:43)\n",
      "15/10/03 19:02:45 INFO TaskSchedulerImpl: Adding task set 15.0 with 4 tasks\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 41, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 1.0 in stage 15.0 (TID 42, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 2.0 in stage 15.0 (TID 43, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 3.0 in stage 15.0 (TID 44, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:45 INFO Executor: Running task 0.0 in stage 15.0 (TID 41)\n",
      "15/10/03 19:02:45 INFO Executor: Running task 2.0 in stage 15.0 (TID 43)\n",
      "15/10/03 19:02:45 INFO Executor: Running task 3.0 in stage 15.0 (TID 44)\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:45 INFO Executor: Running task 1.0 in stage 15.0 (TID 42)\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 10, boot = -135, init = 145, finish = 0\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 16, boot = -123, init = 138, finish = 1\n",
      "15/10/03 19:02:45 INFO Executor: Finished task 3.0 in stage 15.0 (TID 44). 970 bytes result sent to driver\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Finished task 3.0 in stage 15.0 (TID 44) in 24 ms on localhost (1/4)\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 26, boot = -93, init = 119, finish = 0\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 38, boot = -57, init = 95, finish = 0\n",
      "15/10/03 19:02:45 INFO Executor: Finished task 0.0 in stage 15.0 (TID 41). 932 bytes result sent to driver\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 41) in 47 ms on localhost (2/4)\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 53, boot = -54, init = 107, finish = 0\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 70, boot = -54, init = 124, finish = 0\n",
      "15/10/03 19:02:45 INFO Executor: Finished task 2.0 in stage 15.0 (TID 43). 932 bytes result sent to driver\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Finished task 2.0 in stage 15.0 (TID 43) in 83 ms on localhost (3/4)\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 126, boot = -21, init = 147, finish = 0\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 130, boot = -19, init = 149, finish = 0\n",
      "15/10/03 19:02:45 INFO Executor: Finished task 1.0 in stage 15.0 (TID 42). 970 bytes result sent to driver\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Finished task 1.0 in stage 15.0 (TID 42) in 147 ms on localhost (4/4)\n",
      "15/10/03 19:02:45 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "15/10/03 19:02:45 INFO DAGScheduler: ResultStage 15 (runJob at PythonRDD.scala:366) finished in 0.149 s\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Job 7 finished: runJob at PythonRDD.scala:366, took 0.168056 s\n",
      "15/10/03 19:02:45 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Got job 8 (runJob at PythonRDD.scala:366) with 3 output partitions (allowLocal=true)\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Final stage: ResultStage 17(runJob at PythonRDD.scala:366)\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Missing parents: List()\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Submitting ResultStage 17 (PythonRDD[55] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "15/10/03 19:02:45 INFO MemoryStore: ensureFreeSpace(9048) called with curMem=156031, maxMem=278302556\n",
      "15/10/03 19:02:45 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 8.8 KB, free 265.3 MB)\n",
      "15/10/03 19:02:45 INFO MemoryStore: ensureFreeSpace(5044) called with curMem=165079, maxMem=278302556\n",
      "15/10/03 19:02:45 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 4.9 KB, free 265.2 MB)\n",
      "15/10/03 19:02:45 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:65040 (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:45 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:874\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 17 (PythonRDD[55] at RDD at PythonRDD.scala:43)\n",
      "15/10/03 19:02:45 INFO TaskSchedulerImpl: Adding task set 17.0 with 3 tasks\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 45, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 1.0 in stage 17.0 (TID 46, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 2.0 in stage 17.0 (TID 47, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:45 INFO Executor: Running task 1.0 in stage 17.0 (TID 46)\n",
      "15/10/03 19:02:45 INFO Executor: Running task 0.0 in stage 17.0 (TID 45)\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "15/10/03 19:02:45 INFO Executor: Running task 2.0 in stage 17.0 (TID 47)\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 14, boot = -51, init = 65, finish = 0\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 42, boot = -132, init = 174, finish = 0\n",
      "15/10/03 19:02:45 INFO Executor: Finished task 1.0 in stage 17.0 (TID 46). 970 bytes result sent to driver\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Finished task 1.0 in stage 17.0 (TID 46) in 51 ms on localhost (1/3)\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 57, boot = 56, init = 1, finish = 0\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 114, boot = 114, init = 0, finish = 0\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 115, boot = 115, init = 0, finish = 0\n",
      "15/10/03 19:02:45 INFO Executor: Finished task 0.0 in stage 17.0 (TID 45). 932 bytes result sent to driver\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 45) in 131 ms on localhost (2/3)\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 126, boot = 35, init = 91, finish = 0\n",
      "15/10/03 19:02:45 INFO Executor: Finished task 2.0 in stage 17.0 (TID 47). 932 bytes result sent to driver\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Finished task 2.0 in stage 17.0 (TID 47) in 138 ms on localhost (3/3)\n",
      "15/10/03 19:02:45 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "15/10/03 19:02:45 INFO DAGScheduler: ResultStage 17 (runJob at PythonRDD.scala:366) finished in 0.142 s\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Job 8 finished: runJob at PythonRDD.scala:366, took 0.161974 s\n",
      "15/10/03 19:02:45 INFO JobScheduler: Finished job streaming job 1443924163000 ms.0 from job set of time 1443924163000 ms\n",
      "15/10/03 19:02:45 INFO JobScheduler: Total delay: 2.423 s for time 1443924163000 ms (execution: 0.633 s)\n",
      "15/10/03 19:02:45 INFO PythonRDD: Removing RDD 23 from persistence list\n",
      "15/10/03 19:02:45 INFO JobScheduler: Starting job streaming job 1443924164000 ms.0 from job set of time 1443924164000 ms\n",
      "15/10/03 19:02:45 INFO BlockManager: Removing RDD 23\n",
      "15/10/03 19:02:45 INFO ParallelCollectionRDD: Removing RDD 1 from persistence list\n",
      "15/10/03 19:02:45 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "15/10/03 19:02:45 INFO BlockManager: Removing RDD 1\n",
      "15/10/03 19:02:45 INFO InputInfoTracker: remove old batch metadata: \n",
      "15/10/03 19:02:45 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Registering RDD 38 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Got job 9 (runJob at PythonRDD.scala:366) with 1 output partitions (allowLocal=true)\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Final stage: ResultStage 19(runJob at PythonRDD.scala:366)\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 18)\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Submitting ShuffleMapStage 18 (PairwiseRDD[38] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "15/10/03 19:02:45 INFO MemoryStore: ensureFreeSpace(9440) called with curMem=170123, maxMem=278302556\n",
      "15/10/03 19:02:45 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 9.2 KB, free 265.2 MB)\n",
      "15/10/03 19:02:45 INFO MemoryStore: ensureFreeSpace(5005) called with curMem=179563, maxMem=278302556\n",
      "15/10/03 19:02:45 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 4.9 KB, free 265.2 MB)\n",
      "15/10/03 19:02:45 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:65040 (size: 4.9 KB, free: 265.3 MB)\n",
      "15/10/03 19:02:45 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:874\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 18 (PairwiseRDD[38] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/03 19:02:45 INFO TaskSchedulerImpl: Adding task set 18.0 with 8 tasks\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 48, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 1.0 in stage 18.0 (TID 49, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 2.0 in stage 18.0 (TID 50, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 3.0 in stage 18.0 (TID 51, localhost, PROCESS_LOCAL, 1584 bytes)\n",
      "15/10/03 19:02:45 INFO Executor: Running task 3.0 in stage 18.0 (TID 51)\n",
      "15/10/03 19:02:45 INFO Executor: Running task 1.0 in stage 18.0 (TID 49)\n",
      "15/10/03 19:02:45 INFO Executor: Running task 2.0 in stage 18.0 (TID 50)\n",
      "15/10/03 19:02:45 INFO Executor: Running task 0.0 in stage 18.0 (TID 48)\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 1, boot = -28, init = 29, finish = 0\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 4, boot = -8, init = 11, finish = 1\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 7, boot = -7, init = 13, finish = 1\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 15, boot = -1, init = 16, finish = 0\n",
      "15/10/03 19:02:45 INFO Executor: Finished task 3.0 in stage 18.0 (TID 51). 948 bytes result sent to driver\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 4.0 in stage 18.0 (TID 52, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:45 INFO Executor: Running task 4.0 in stage 18.0 (TID 52)\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Finished task 3.0 in stage 18.0 (TID 51) in 37 ms on localhost (1/8)\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 29, boot = 29, init = 0, finish = 0\n",
      "15/10/03 19:02:45 INFO Executor: Finished task 1.0 in stage 18.0 (TID 49). 948 bytes result sent to driver\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 5.0 in stage 18.0 (TID 53, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:45 INFO Executor: Running task 5.0 in stage 18.0 (TID 53)\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Finished task 1.0 in stage 18.0 (TID 49) in 49 ms on localhost (2/8)\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 63, boot = 62, init = 1, finish = 0\n",
      "15/10/03 19:02:45 INFO Executor: Finished task 0.0 in stage 18.0 (TID 48). 948 bytes result sent to driver\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 6.0 in stage 18.0 (TID 54, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:45 INFO Executor: Running task 6.0 in stage 18.0 (TID 54)\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 48) in 101 ms on localhost (3/8)\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 103, boot = 102, init = 1, finish = 0\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 108, boot = 63, init = 45, finish = 0\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 90, boot = 90, init = 0, finish = 0\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 91, boot = 68, init = 23, finish = 0\n",
      "15/10/03 19:02:45 INFO Executor: Finished task 2.0 in stage 18.0 (TID 50). 948 bytes result sent to driver\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 7.0 in stage 18.0 (TID 55, localhost, PROCESS_LOCAL, 1584 bytes)\n",
      "15/10/03 19:02:45 INFO Executor: Running task 7.0 in stage 18.0 (TID 55)\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Finished task 2.0 in stage 18.0 (TID 50) in 151 ms on localhost (4/8)\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 101, boot = 100, init = 1, finish = 0\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 6, boot = 6, init = 0, finish = 0\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 112, boot = 85, init = 27, finish = 0\n",
      "15/10/03 19:02:45 INFO Executor: Finished task 5.0 in stage 18.0 (TID 53). 948 bytes result sent to driver\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Finished task 5.0 in stage 18.0 (TID 53) in 142 ms on localhost (5/8)\n",
      "15/10/03 19:02:45 INFO Executor: Finished task 4.0 in stage 18.0 (TID 52). 948 bytes result sent to driver\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Finished task 4.0 in stage 18.0 (TID 52) in 153 ms on localhost (6/8)\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 90, boot = 90, init = 0, finish = 0\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 92, boot = 78, init = 13, finish = 1\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 47, boot = 46, init = 1, finish = 0\n",
      "15/10/03 19:02:45 INFO Executor: Finished task 6.0 in stage 18.0 (TID 54). 948 bytes result sent to driver\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Finished task 6.0 in stage 18.0 (TID 54) in 122 ms on localhost (7/8)\n",
      "15/10/03 19:02:45 INFO Executor: Finished task 7.0 in stage 18.0 (TID 55). 948 bytes result sent to driver\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Finished task 7.0 in stage 18.0 (TID 55) in 75 ms on localhost (8/8)\n",
      "15/10/03 19:02:45 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
      "15/10/03 19:02:45 INFO DAGScheduler: ShuffleMapStage 18 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.229 s\n",
      "15/10/03 19:02:45 INFO DAGScheduler: looking for newly runnable stages\n",
      "15/10/03 19:02:45 INFO DAGScheduler: running: Set()\n",
      "15/10/03 19:02:45 INFO DAGScheduler: waiting: Set(ResultStage 19)\n",
      "15/10/03 19:02:45 INFO DAGScheduler: failed: Set()\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Missing parents for ResultStage 19: List()\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Submitting ResultStage 19 (PythonRDD[56] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "15/10/03 19:02:45 INFO MemoryStore: ensureFreeSpace(9048) called with curMem=184568, maxMem=278302556\n",
      "15/10/03 19:02:45 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 8.8 KB, free 265.2 MB)\n",
      "15/10/03 19:02:45 INFO MemoryStore: ensureFreeSpace(5046) called with curMem=193616, maxMem=278302556\n",
      "15/10/03 19:02:45 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 4.9 KB, free 265.2 MB)\n",
      "15/10/03 19:02:45 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:65040 (size: 4.9 KB, free: 265.3 MB)\n",
      "15/10/03 19:02:45 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:874\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (PythonRDD[56] at RDD at PythonRDD.scala:43)\n",
      "15/10/03 19:02:45 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 56, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:45 INFO Executor: Running task 0.0 in stage 19.0 (TID 56)\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 17, boot = -50, init = 67, finish = 0\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 19, boot = -24, init = 43, finish = 0\n",
      "15/10/03 19:02:45 INFO Executor: Finished task 0.0 in stage 19.0 (TID 56). 995 bytes result sent to driver\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 56) in 25 ms on localhost (1/1)\n",
      "15/10/03 19:02:45 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "15/10/03 19:02:45 INFO DAGScheduler: ResultStage 19 (runJob at PythonRDD.scala:366) finished in 0.027 s\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Job 9 finished: runJob at PythonRDD.scala:366, took 0.296360 s\n",
      "15/10/03 19:02:45 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/03 19:02:45 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 3 is 175 bytes\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Got job 10 (runJob at PythonRDD.scala:366) with 4 output partitions (allowLocal=true)\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Final stage: ResultStage 21(runJob at PythonRDD.scala:366)\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Missing parents: List()\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Submitting ResultStage 21 (PythonRDD[57] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "15/10/03 19:02:45 INFO MemoryStore: ensureFreeSpace(9048) called with curMem=198662, maxMem=278302556\n",
      "15/10/03 19:02:45 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 8.8 KB, free 265.2 MB)\n",
      "15/10/03 19:02:45 INFO MemoryStore: ensureFreeSpace(5046) called with curMem=207710, maxMem=278302556\n",
      "15/10/03 19:02:45 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.9 KB, free 265.2 MB)\n",
      "15/10/03 19:02:45 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:65040 (size: 4.9 KB, free: 265.3 MB)\n",
      "15/10/03 19:02:45 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:874\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 21 (PythonRDD[57] at RDD at PythonRDD.scala:43)\n",
      "15/10/03 19:02:45 INFO TaskSchedulerImpl: Adding task set 21.0 with 4 tasks\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 57, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 1.0 in stage 21.0 (TID 58, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 2.0 in stage 21.0 (TID 59, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 3.0 in stage 21.0 (TID 60, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:45 INFO Executor: Running task 0.0 in stage 21.0 (TID 57)\n",
      "15/10/03 19:02:45 INFO Executor: Running task 1.0 in stage 21.0 (TID 58)\n",
      "15/10/03 19:02:45 INFO Executor: Running task 2.0 in stage 21.0 (TID 59)\n",
      "15/10/03 19:02:45 INFO Executor: Running task 3.0 in stage 21.0 (TID 60)\n",
      "15/10/03 19:02:45 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:65040 in memory (size: 4.9 KB, free: 265.3 MB)\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:45 INFO BlockManagerInfo: Removed broadcast_13_piece0 on localhost:65040 in memory (size: 4.9 KB, free: 265.3 MB)\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "15/10/03 19:02:45 INFO BlockManagerInfo: Removed broadcast_12_piece0 on localhost:65040 in memory (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:45 INFO BlockManagerInfo: Removed broadcast_11_piece0 on localhost:65040 in memory (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 15, boot = -104, init = 119, finish = 0\n",
      "15/10/03 19:02:45 INFO BlockManagerInfo: Removed broadcast_10_piece0 on localhost:65040 in memory (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:45 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:65040 in memory (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:45 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:65040 in memory (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:45 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:65040 in memory (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 24, boot = -83, init = 106, finish = 1\n",
      "15/10/03 19:02:45 INFO BlockManagerInfo: Removed broadcast_5_piece0 on localhost:65040 in memory (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:45 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:65040 in memory (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:45 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:65040 in memory (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:45 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:65040 in memory (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:45 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:65040 in memory (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:45 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:65040 in memory (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 36, boot = -41, init = 77, finish = 0\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 38, boot = -44, init = 81, finish = 1\n",
      "15/10/03 19:02:45 INFO Executor: Finished task 3.0 in stage 21.0 (TID 60). 970 bytes result sent to driver\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Finished task 3.0 in stage 21.0 (TID 60) in 50 ms on localhost (1/4)\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 51, boot = -88, init = 139, finish = 0\n",
      "15/10/03 19:02:45 INFO Executor: Finished task 0.0 in stage 21.0 (TID 57). 932 bytes result sent to driver\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 57) in 59 ms on localhost (2/4)\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 74, boot = -78, init = 152, finish = 0\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 77, boot = -79, init = 156, finish = 0\n",
      "15/10/03 19:02:45 INFO Executor: Finished task 1.0 in stage 21.0 (TID 58). 970 bytes result sent to driver\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Finished task 1.0 in stage 21.0 (TID 58) in 84 ms on localhost (3/4)\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 80, boot = -84, init = 164, finish = 0\n",
      "15/10/03 19:02:45 INFO Executor: Finished task 2.0 in stage 21.0 (TID 59). 932 bytes result sent to driver\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Finished task 2.0 in stage 21.0 (TID 59) in 90 ms on localhost (4/4)\n",
      "15/10/03 19:02:45 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
      "15/10/03 19:02:45 INFO DAGScheduler: ResultStage 21 (runJob at PythonRDD.scala:366) finished in 0.091 s\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Job 10 finished: runJob at PythonRDD.scala:366, took 0.119240 s\n",
      "15/10/03 19:02:45 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Got job 11 (runJob at PythonRDD.scala:366) with 3 output partitions (allowLocal=true)\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Final stage: ResultStage 23(runJob at PythonRDD.scala:366)\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Missing parents: List()\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Submitting ResultStage 23 (PythonRDD[58] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "15/10/03 19:02:45 INFO MemoryStore: ensureFreeSpace(9048) called with curMem=14094, maxMem=278302556\n",
      "15/10/03 19:02:45 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 8.8 KB, free 265.4 MB)\n",
      "15/10/03 19:02:45 INFO MemoryStore: ensureFreeSpace(5046) called with curMem=23142, maxMem=278302556\n",
      "15/10/03 19:02:45 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 4.9 KB, free 265.4 MB)\n",
      "15/10/03 19:02:45 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:65040 (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:45 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:874\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 23 (PythonRDD[58] at RDD at PythonRDD.scala:43)\n",
      "15/10/03 19:02:45 INFO TaskSchedulerImpl: Adding task set 23.0 with 3 tasks\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 61, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 1.0 in stage 23.0 (TID 62, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 2.0 in stage 23.0 (TID 63, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:45 INFO Executor: Running task 0.0 in stage 23.0 (TID 61)\n",
      "15/10/03 19:02:45 INFO Executor: Running task 1.0 in stage 23.0 (TID 62)\n",
      "15/10/03 19:02:45 INFO Executor: Running task 2.0 in stage 23.0 (TID 63)\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 4, boot = -27, init = 31, finish = 0\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 4, boot = -42, init = 46, finish = 0\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 6, boot = -21, init = 27, finish = 0\n",
      "15/10/03 19:02:45 INFO Executor: Finished task 0.0 in stage 23.0 (TID 61). 932 bytes result sent to driver\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 61) in 12 ms on localhost (1/3)\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 8, boot = -20, init = 28, finish = 0\n",
      "15/10/03 19:02:45 INFO Executor: Finished task 1.0 in stage 23.0 (TID 62). 970 bytes result sent to driver\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Finished task 1.0 in stage 23.0 (TID 62) in 14 ms on localhost (2/3)\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 15, boot = -4, init = 19, finish = 0\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 16, boot = -25, init = 41, finish = 0\n",
      "15/10/03 19:02:45 INFO Executor: Finished task 2.0 in stage 23.0 (TID 63). 932 bytes result sent to driver\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Finished task 2.0 in stage 23.0 (TID 63) in 22 ms on localhost (3/3)\n",
      "15/10/03 19:02:45 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "15/10/03 19:02:45 INFO DAGScheduler: ResultStage 23 (runJob at PythonRDD.scala:366) finished in 0.025 s\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Job 11 finished: runJob at PythonRDD.scala:366, took 0.042137 s\n",
      "15/10/03 19:02:45 INFO JobScheduler: Finished job streaming job 1443924164000 ms.0 from job set of time 1443924164000 ms\n",
      "15/10/03 19:02:45 INFO JobScheduler: Total delay: 1.934 s for time 1443924164000 ms (execution: 0.511 s)\n",
      "15/10/03 19:02:45 INFO PythonRDD: Removing RDD 31 from persistence list\n",
      "15/10/03 19:02:45 INFO JobScheduler: Starting job streaming job 1443924165000 ms.0 from job set of time 1443924165000 ms\n",
      "15/10/03 19:02:45 INFO ParallelCollectionRDD: Removing RDD 2 from persistence list\n",
      "15/10/03 19:02:45 INFO BlockManager: Removing RDD 31\n",
      "15/10/03 19:02:45 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()\n",
      "15/10/03 19:02:45 INFO InputInfoTracker: remove old batch metadata: \n",
      "15/10/03 19:02:45 INFO BlockManager: Removing RDD 2\n",
      "15/10/03 19:02:45 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Registering RDD 50 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Got job 12 (runJob at PythonRDD.scala:366) with 1 output partitions (allowLocal=true)\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Final stage: ResultStage 25(runJob at PythonRDD.scala:366)\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 24)\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 24)\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Submitting ShuffleMapStage 24 (PairwiseRDD[50] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "15/10/03 19:02:45 INFO MemoryStore: ensureFreeSpace(9440) called with curMem=28188, maxMem=278302556\n",
      "15/10/03 19:02:45 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 9.2 KB, free 265.4 MB)\n",
      "15/10/03 19:02:45 INFO MemoryStore: ensureFreeSpace(5002) called with curMem=37628, maxMem=278302556\n",
      "15/10/03 19:02:45 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 4.9 KB, free 265.4 MB)\n",
      "15/10/03 19:02:45 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:65040 (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:45 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:874\n",
      "15/10/03 19:02:45 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 24 (PairwiseRDD[50] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/03 19:02:45 INFO TaskSchedulerImpl: Adding task set 24.0 with 8 tasks\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 64, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 1.0 in stage 24.0 (TID 65, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 2.0 in stage 24.0 (TID 66, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:45 INFO TaskSetManager: Starting task 3.0 in stage 24.0 (TID 67, localhost, PROCESS_LOCAL, 1584 bytes)\n",
      "15/10/03 19:02:45 INFO Executor: Running task 1.0 in stage 24.0 (TID 65)\n",
      "15/10/03 19:02:45 INFO Executor: Running task 0.0 in stage 24.0 (TID 64)\n",
      "15/10/03 19:02:45 INFO Executor: Running task 2.0 in stage 24.0 (TID 66)\n",
      "15/10/03 19:02:45 INFO Executor: Running task 3.0 in stage 24.0 (TID 67)\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 4, boot = -75, init = 79, finish = 0\n",
      "15/10/03 19:02:45 INFO PythonRDD: Times: total = 6, boot = -72, init = 77, finish = 1\n",
      "15/10/03 19:02:46 INFO Executor: Finished task 0.0 in stage 24.0 (TID 64). 948 bytes result sent to driver\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Starting task 4.0 in stage 24.0 (TID 68, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:46 INFO Executor: Running task 4.0 in stage 24.0 (TID 68)\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 64) in 35 ms on localhost (1/8)\n",
      "15/10/03 19:02:46 INFO JobScheduler: Added jobs for time 1443924166000 ms\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 60, boot = -13, init = 30, finish = 43\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 70, boot = -11, init = 80, finish = 1\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 72, boot = -19, init = 91, finish = 0\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 79, boot = -20, init = 99, finish = 0\n",
      "15/10/03 19:02:46 INFO Executor: Finished task 2.0 in stage 24.0 (TID 66). 948 bytes result sent to driver\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Starting task 5.0 in stage 24.0 (TID 69, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:46 INFO Executor: Running task 5.0 in stage 24.0 (TID 69)\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Finished task 2.0 in stage 24.0 (TID 66) in 104 ms on localhost (2/8)\n",
      "15/10/03 19:02:46 INFO Executor: Finished task 3.0 in stage 24.0 (TID 67). 948 bytes result sent to driver\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Starting task 6.0 in stage 24.0 (TID 70, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:46 INFO Executor: Running task 6.0 in stage 24.0 (TID 70)\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Finished task 3.0 in stage 24.0 (TID 67) in 110 ms on localhost (3/8)\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 24, boot = 23, init = 0, finish = 1\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 131, boot = 131, init = 0, finish = 0\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 134, boot = -25, init = 158, finish = 1\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 120, boot = 120, init = 0, finish = 0\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 123, boot = 116, init = 7, finish = 0\n",
      "15/10/03 19:02:46 INFO Executor: Finished task 1.0 in stage 24.0 (TID 65). 948 bytes result sent to driver\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Starting task 7.0 in stage 24.0 (TID 71, localhost, PROCESS_LOCAL, 1584 bytes)\n",
      "15/10/03 19:02:46 INFO Executor: Running task 7.0 in stage 24.0 (TID 71)\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Finished task 1.0 in stage 24.0 (TID 65) in 170 ms on localhost (4/8)\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 64, boot = 64, init = 0, finish = 0\n",
      "15/10/03 19:02:46 INFO Executor: Finished task 4.0 in stage 24.0 (TID 68). 948 bytes result sent to driver\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Finished task 4.0 in stage 24.0 (TID 68) in 156 ms on localhost (5/8)\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 77, boot = 15, init = 62, finish = 0\n",
      "15/10/03 19:02:46 INFO Executor: Finished task 6.0 in stage 24.0 (TID 70). 948 bytes result sent to driver\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Finished task 6.0 in stage 24.0 (TID 70) in 115 ms on localhost (6/8)\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 163, boot = 162, init = 1, finish = 0\n",
      "15/10/03 19:02:46 INFO Executor: Finished task 5.0 in stage 24.0 (TID 69). 948 bytes result sent to driver\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Finished task 5.0 in stage 24.0 (TID 69) in 224 ms on localhost (7/8)\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 234, boot = 232, init = 2, finish = 0\n",
      "15/10/03 19:02:46 INFO JobGenerator: Stopping JobGenerator immediately\n",
      "15/10/03 19:02:46 INFO RecurringTimer: Stopped timer for JobGenerator after time 1443924166000\n",
      "15/10/03 19:02:46 INFO JobGenerator: Stopped JobGenerator\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 286, boot = 285, init = 1, finish = 0\n",
      "15/10/03 19:02:46 INFO Executor: Finished task 7.0 in stage 24.0 (TID 71). 948 bytes result sent to driver\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Finished task 7.0 in stage 24.0 (TID 71) in 333 ms on localhost (8/8)\n",
      "15/10/03 19:02:46 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
      "15/10/03 19:02:46 INFO DAGScheduler: ShuffleMapStage 24 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.504 s\n",
      "15/10/03 19:02:46 INFO DAGScheduler: looking for newly runnable stages\n",
      "15/10/03 19:02:46 INFO DAGScheduler: running: Set()\n",
      "15/10/03 19:02:46 INFO DAGScheduler: waiting: Set(ResultStage 25)\n",
      "15/10/03 19:02:46 INFO DAGScheduler: failed: Set()\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Missing parents for ResultStage 25: List()\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Submitting ResultStage 25 (PythonRDD[59] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "15/10/03 19:02:46 INFO MemoryStore: ensureFreeSpace(9048) called with curMem=42630, maxMem=278302556\n",
      "15/10/03 19:02:46 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 8.8 KB, free 265.4 MB)\n",
      "15/10/03 19:02:46 INFO MemoryStore: ensureFreeSpace(5045) called with curMem=51678, maxMem=278302556\n",
      "15/10/03 19:02:46 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 4.9 KB, free 265.4 MB)\n",
      "15/10/03 19:02:46 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on localhost:65040 (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:46 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:874\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (PythonRDD[59] at RDD at PythonRDD.scala:43)\n",
      "15/10/03 19:02:46 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 72, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:46 INFO Executor: Running task 0.0 in stage 25.0 (TID 72)\n",
      "15/10/03 19:02:46 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 50, boot = -82, init = 131, finish = 1\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 52, boot = -328, init = 379, finish = 1\n",
      "15/10/03 19:02:46 INFO Executor: Finished task 0.0 in stage 25.0 (TID 72). 995 bytes result sent to driver\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 72) in 57 ms on localhost (1/1)\n",
      "15/10/03 19:02:46 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "15/10/03 19:02:46 INFO DAGScheduler: ResultStage 25 (runJob at PythonRDD.scala:366) finished in 0.059 s\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Job 12 finished: runJob at PythonRDD.scala:366, took 0.600898 s\n",
      "15/10/03 19:02:46 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/03 19:02:46 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 4 is 175 bytes\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Got job 13 (runJob at PythonRDD.scala:366) with 4 output partitions (allowLocal=true)\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Final stage: ResultStage 27(runJob at PythonRDD.scala:366)\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Missing parents: List()\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Submitting ResultStage 27 (PythonRDD[68] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "15/10/03 19:02:46 INFO MemoryStore: ensureFreeSpace(9048) called with curMem=56723, maxMem=278302556\n",
      "15/10/03 19:02:46 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 8.8 KB, free 265.3 MB)\n",
      "15/10/03 19:02:46 INFO MemoryStore: ensureFreeSpace(5045) called with curMem=65771, maxMem=278302556\n",
      "15/10/03 19:02:46 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 4.9 KB, free 265.3 MB)\n",
      "15/10/03 19:02:46 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on localhost:65040 (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:46 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:874\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 27 (PythonRDD[68] at RDD at PythonRDD.scala:43)\n",
      "15/10/03 19:02:46 INFO TaskSchedulerImpl: Adding task set 27.0 with 4 tasks\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 73, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Starting task 1.0 in stage 27.0 (TID 74, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Starting task 2.0 in stage 27.0 (TID 75, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Starting task 3.0 in stage 27.0 (TID 76, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:46 INFO Executor: Running task 0.0 in stage 27.0 (TID 73)\n",
      "15/10/03 19:02:46 INFO Executor: Running task 1.0 in stage 27.0 (TID 74)\n",
      "15/10/03 19:02:46 INFO Executor: Running task 3.0 in stage 27.0 (TID 76)\n",
      "15/10/03 19:02:46 INFO Executor: Running task 2.0 in stage 27.0 (TID 75)\n",
      "15/10/03 19:02:46 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:46 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:46 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:46 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 5, boot = -99, init = 104, finish = 0\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 4, boot = -17, init = 21, finish = 0\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 5, boot = -270, init = 275, finish = 0\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 7, boot = -201, init = 208, finish = 0\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 7, boot = -161, init = 168, finish = 0\n",
      "15/10/03 19:02:46 INFO Executor: Finished task 0.0 in stage 27.0 (TID 73). 932 bytes result sent to driver\n",
      "15/10/03 19:02:46 INFO Executor: Finished task 1.0 in stage 27.0 (TID 74). 970 bytes result sent to driver\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 73) in 13 ms on localhost (1/4)\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Finished task 1.0 in stage 27.0 (TID 74) in 14 ms on localhost (2/4)\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 12, boot = -139, init = 145, finish = 6\n",
      "15/10/03 19:02:46 INFO Executor: Finished task 2.0 in stage 27.0 (TID 75). 932 bytes result sent to driver\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Finished task 2.0 in stage 27.0 (TID 75) in 18 ms on localhost (3/4)\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 40, boot = -16, init = 20, finish = 36\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 42, boot = -95, init = 136, finish = 1\n",
      "15/10/03 19:02:46 INFO Executor: Finished task 3.0 in stage 27.0 (TID 76). 970 bytes result sent to driver\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Finished task 3.0 in stage 27.0 (TID 76) in 48 ms on localhost (4/4)\n",
      "15/10/03 19:02:46 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "15/10/03 19:02:46 INFO DAGScheduler: ResultStage 27 (runJob at PythonRDD.scala:366) finished in 0.050 s\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Job 13 finished: runJob at PythonRDD.scala:366, took 0.066631 s\n",
      "15/10/03 19:02:46 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Got job 14 (runJob at PythonRDD.scala:366) with 3 output partitions (allowLocal=true)\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Final stage: ResultStage 29(runJob at PythonRDD.scala:366)\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 28)\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Missing parents: List()\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Submitting ResultStage 29 (PythonRDD[69] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "15/10/03 19:02:46 INFO MemoryStore: ensureFreeSpace(9048) called with curMem=70816, maxMem=278302556\n",
      "15/10/03 19:02:46 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 8.8 KB, free 265.3 MB)\n",
      "15/10/03 19:02:46 INFO MemoryStore: ensureFreeSpace(5045) called with curMem=79864, maxMem=278302556\n",
      "15/10/03 19:02:46 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 4.9 KB, free 265.3 MB)\n",
      "15/10/03 19:02:46 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on localhost:65040 (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:46 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:874\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 29 (PythonRDD[69] at RDD at PythonRDD.scala:43)\n",
      "15/10/03 19:02:46 INFO TaskSchedulerImpl: Adding task set 29.0 with 3 tasks\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 77, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Starting task 1.0 in stage 29.0 (TID 78, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Starting task 2.0 in stage 29.0 (TID 79, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:46 INFO Executor: Running task 0.0 in stage 29.0 (TID 77)\n",
      "15/10/03 19:02:46 INFO Executor: Running task 1.0 in stage 29.0 (TID 78)\n",
      "15/10/03 19:02:46 INFO Executor: Running task 2.0 in stage 29.0 (TID 79)\n",
      "15/10/03 19:02:46 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:46 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:46 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks\n",
      "15/10/03 19:02:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 5, boot = -40, init = 45, finish = 0\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 6, boot = -24, init = 30, finish = 0\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 6, boot = -6, init = 12, finish = 0\n",
      "15/10/03 19:02:46 INFO Executor: Finished task 2.0 in stage 29.0 (TID 79). 932 bytes result sent to driver\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Finished task 2.0 in stage 29.0 (TID 79) in 13 ms on localhost (1/3)\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 20, boot = 4, init = 16, finish = 0\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 17, boot = -14, init = 31, finish = 0\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 19, boot = -30, init = 49, finish = 0\n",
      "15/10/03 19:02:46 INFO Executor: Finished task 1.0 in stage 29.0 (TID 78). 970 bytes result sent to driver\n",
      "15/10/03 19:02:46 INFO Executor: Finished task 0.0 in stage 29.0 (TID 77). 932 bytes result sent to driver\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Finished task 1.0 in stage 29.0 (TID 78) in 27 ms on localhost (2/3)\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 77) in 27 ms on localhost (3/3)\n",
      "15/10/03 19:02:46 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool \n",
      "15/10/03 19:02:46 INFO DAGScheduler: ResultStage 29 (runJob at PythonRDD.scala:366) finished in 0.029 s\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Job 14 finished: runJob at PythonRDD.scala:366, took 0.044476 s\n",
      "15/10/03 19:02:46 INFO JobScheduler: Finished job streaming job 1443924165000 ms.0 from job set of time 1443924165000 ms\n",
      "15/10/03 19:02:46 INFO JobScheduler: Total delay: 1.690 s for time 1443924165000 ms (execution: 0.756 s)\n",
      "15/10/03 19:02:46 INFO JobScheduler: Starting job streaming job 1443924166000 ms.0 from job set of time 1443924166000 ms\n",
      "15/10/03 19:02:46 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Registering RDD 64 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Got job 15 (runJob at PythonRDD.scala:366) with 1 output partitions (allowLocal=true)\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Final stage: ResultStage 31(runJob at PythonRDD.scala:366)\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 30)\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Submitting ShuffleMapStage 30 (PairwiseRDD[64] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents\n",
      "15/10/03 19:02:46 INFO MemoryStore: ensureFreeSpace(9736) called with curMem=84909, maxMem=278302556\n",
      "15/10/03 19:02:46 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 9.5 KB, free 265.3 MB)\n",
      "15/10/03 19:02:46 INFO MemoryStore: ensureFreeSpace(5145) called with curMem=94645, maxMem=278302556\n",
      "15/10/03 19:02:46 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 5.0 KB, free 265.3 MB)\n",
      "15/10/03 19:02:46 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on localhost:65040 (size: 5.0 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:46 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:874\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 30 (PairwiseRDD[64] at call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)\n",
      "15/10/03 19:02:46 INFO TaskSchedulerImpl: Adding task set 30.0 with 5 tasks\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 80, localhost, PROCESS_LOCAL, 1540 bytes)\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Starting task 1.0 in stage 30.0 (TID 81, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Starting task 2.0 in stage 30.0 (TID 82, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Starting task 3.0 in stage 30.0 (TID 83, localhost, PROCESS_LOCAL, 1554 bytes)\n",
      "15/10/03 19:02:46 INFO Executor: Running task 0.0 in stage 30.0 (TID 80)\n",
      "15/10/03 19:02:46 INFO Executor: Running task 1.0 in stage 30.0 (TID 81)\n",
      "15/10/03 19:02:46 INFO Executor: Running task 2.0 in stage 30.0 (TID 82)\n",
      "15/10/03 19:02:46 INFO Executor: Running task 3.0 in stage 30.0 (TID 83)\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 2, boot = -7, init = 9, finish = 0\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 1, boot = -16, init = 17, finish = 0\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 7, boot = 7, init = 0, finish = 0\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 7, boot = -12, init = 19, finish = 0\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 9, boot = -54, init = 63, finish = 0\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 22, boot = -20, init = 42, finish = 0\n",
      "15/10/03 19:02:46 INFO Executor: Finished task 3.0 in stage 30.0 (TID 83). 945 bytes result sent to driver\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Starting task 4.0 in stage 30.0 (TID 84, localhost, PROCESS_LOCAL, 1584 bytes)\n",
      "15/10/03 19:02:46 INFO Executor: Finished task 2.0 in stage 30.0 (TID 82). 945 bytes result sent to driver\n",
      "15/10/03 19:02:46 INFO Executor: Running task 4.0 in stage 30.0 (TID 84)\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Finished task 3.0 in stage 30.0 (TID 83) in 32 ms on localhost (1/5)\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Finished task 2.0 in stage 30.0 (TID 82) in 34 ms on localhost (2/5)\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 38, boot = 38, init = 0, finish = 0\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 40, boot = -55, init = 95, finish = 0\n",
      "15/10/03 19:02:46 INFO Executor: Finished task 1.0 in stage 30.0 (TID 81). 945 bytes result sent to driver\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Finished task 1.0 in stage 30.0 (TID 81) in 50 ms on localhost (3/5)\n",
      "15/10/03 19:02:46 INFO Executor: Finished task 0.0 in stage 30.0 (TID 80). 945 bytes result sent to driver\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 80) in 52 ms on localhost (4/5)\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 45, boot = 45, init = 0, finish = 0\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 52, boot = 51, init = 1, finish = 0\n",
      "15/10/03 19:02:46 INFO Executor: Finished task 4.0 in stage 30.0 (TID 84). 945 bytes result sent to driver\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Finished task 4.0 in stage 30.0 (TID 84) in 80 ms on localhost (5/5)\n",
      "15/10/03 19:02:46 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool \n",
      "15/10/03 19:02:46 INFO DAGScheduler: ShuffleMapStage 30 (call at /Users/Alexander/spark-1.4.1-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.113 s\n",
      "15/10/03 19:02:46 INFO DAGScheduler: looking for newly runnable stages\n",
      "15/10/03 19:02:46 INFO DAGScheduler: running: Set()\n",
      "15/10/03 19:02:46 INFO DAGScheduler: waiting: Set(ResultStage 31)\n",
      "15/10/03 19:02:46 INFO DAGScheduler: failed: Set()\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Missing parents for ResultStage 31: List()\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Submitting ResultStage 31 (PythonRDD[70] at RDD at PythonRDD.scala:43), which is now runnable\n",
      "15/10/03 19:02:46 INFO MemoryStore: ensureFreeSpace(9048) called with curMem=99790, maxMem=278302556\n",
      "15/10/03 19:02:46 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 8.8 KB, free 265.3 MB)\n",
      "15/10/03 19:02:46 INFO MemoryStore: ensureFreeSpace(5040) called with curMem=108838, maxMem=278302556\n",
      "15/10/03 19:02:46 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 4.9 KB, free 265.3 MB)\n",
      "15/10/03 19:02:46 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on localhost:65040 (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:46 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:874\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (PythonRDD[70] at RDD at PythonRDD.scala:43)\n",
      "15/10/03 19:02:46 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 85, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:46 INFO Executor: Running task 0.0 in stage 31.0 (TID 85)\n",
      "15/10/03 19:02:46 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 5 blocks\n",
      "15/10/03 19:02:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 11, boot = -46, init = 57, finish = 0\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 12, boot = -22, init = 34, finish = 0\n",
      "15/10/03 19:02:46 INFO Executor: Finished task 0.0 in stage 31.0 (TID 85). 932 bytes result sent to driver\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 85) in 18 ms on localhost (1/1)\n",
      "15/10/03 19:02:46 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool \n",
      "15/10/03 19:02:46 INFO DAGScheduler: ResultStage 31 (runJob at PythonRDD.scala:366) finished in 0.019 s\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Job 15 finished: runJob at PythonRDD.scala:366, took 0.171194 s\n",
      "15/10/03 19:02:46 INFO SparkContext: Starting job: runJob at PythonRDD.scala:366\n",
      "15/10/03 19:02:46 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 5 is 175 bytes\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Got job 16 (runJob at PythonRDD.scala:366) with 4 output partitions (allowLocal=true)\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Final stage: ResultStage 33(runJob at PythonRDD.scala:366)\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Missing parents: List()\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Submitting ResultStage 33 (PythonRDD[71] at RDD at PythonRDD.scala:43), which has no missing parents\n",
      "15/10/03 19:02:46 INFO MemoryStore: ensureFreeSpace(9048) called with curMem=113878, maxMem=278302556\n",
      "15/10/03 19:02:46 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 8.8 KB, free 265.3 MB)\n",
      "15/10/03 19:02:46 INFO MemoryStore: ensureFreeSpace(5040) called with curMem=122926, maxMem=278302556\n",
      "15/10/03 19:02:46 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 4.9 KB, free 265.3 MB)\n",
      "15/10/03 19:02:46 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on localhost:65040 (size: 4.9 KB, free: 265.4 MB)\n",
      "15/10/03 19:02:46 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:874\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 33 (PythonRDD[71] at RDD at PythonRDD.scala:43)\n",
      "15/10/03 19:02:46 INFO TaskSchedulerImpl: Adding task set 33.0 with 4 tasks\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 86, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Starting task 1.0 in stage 33.0 (TID 87, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Starting task 2.0 in stage 33.0 (TID 88, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Starting task 3.0 in stage 33.0 (TID 89, localhost, PROCESS_LOCAL, 1236 bytes)\n",
      "15/10/03 19:02:46 INFO Executor: Running task 0.0 in stage 33.0 (TID 86)\n",
      "15/10/03 19:02:46 INFO Executor: Running task 1.0 in stage 33.0 (TID 87)\n",
      "15/10/03 19:02:46 INFO Executor: Running task 2.0 in stage 33.0 (TID 88)\n",
      "15/10/03 19:02:46 INFO Executor: Running task 3.0 in stage 33.0 (TID 89)\n",
      "15/10/03 19:02:46 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 5 blocks\n",
      "15/10/03 19:02:46 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 5 blocks\n",
      "15/10/03 19:02:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:46 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 5 blocks\n",
      "15/10/03 19:02:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:46 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 5 blocks\n",
      "15/10/03 19:02:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 4, boot = -14, init = 18, finish = 0\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 4, boot = -66, init = 70, finish = 0\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 6, boot = -96, init = 102, finish = 0\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 5, boot = -22, init = 27, finish = 0\n",
      "15/10/03 19:02:46 INFO Executor: Finished task 3.0 in stage 33.0 (TID 89). 932 bytes result sent to driver\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 6, boot = -112, init = 118, finish = 0\n",
      "15/10/03 19:02:46 INFO Executor: Finished task 2.0 in stage 33.0 (TID 88). 932 bytes result sent to driver\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Finished task 3.0 in stage 33.0 (TID 89) in 9 ms on localhost (1/4)\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Finished task 2.0 in stage 33.0 (TID 88) in 11 ms on localhost (2/4)\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 32, boot = -84, init = 116, finish = 0\n",
      "15/10/03 19:02:46 INFO Executor: Finished task 1.0 in stage 33.0 (TID 87). 932 bytes result sent to driver\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Finished task 1.0 in stage 33.0 (TID 87) in 37 ms on localhost (3/4)\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 55, boot = -64, init = 119, finish = 0\n",
      "15/10/03 19:02:46 INFO PythonRDD: Times: total = 57, boot = -120, init = 177, finish = 0\n",
      "15/10/03 19:02:46 INFO Executor: Finished task 0.0 in stage 33.0 (TID 86). 932 bytes result sent to driver\n",
      "15/10/03 19:02:46 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 86) in 63 ms on localhost (4/4)\n",
      "15/10/03 19:02:46 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool \n",
      "15/10/03 19:02:46 INFO DAGScheduler: ResultStage 33 (runJob at PythonRDD.scala:366) finished in 0.064 s\n",
      "15/10/03 19:02:46 INFO DAGScheduler: Job 16 finished: runJob at PythonRDD.scala:366, took 0.076119 s\n",
      "15/10/03 19:02:46 INFO JobScheduler: Finished job streaming job 1443924166000 ms.0 from job set of time 1443924166000 ms\n",
      "15/10/03 19:02:46 INFO JobScheduler: Total delay: 0.977 s for time 1443924166000 ms (execution: 0.287 s)\n",
      "15/10/03 19:02:46 INFO JobScheduler: Stopped JobScheduler\n",
      "15/10/03 19:02:46 INFO StreamingContext: StreamingContext stopped successfully\n",
      "15/10/03 19:02:47 INFO SparkUI: Stopped Spark web UI at http://10.0.0.9:4041\n",
      "15/10/03 19:02:47 INFO DAGScheduler: Stopping DAGScheduler\n",
      "15/10/03 19:02:47 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "15/10/03 19:02:47 INFO Utils: path = /private/var/folders/vx/9464rz3s72b0l50g6wwpr2hm0000gn/T/spark-9d30a6a7-1fba-4666-9b8e-83314e13730b/blockmgr-6f6dedbd-2726-49d7-b455-f0d08342e375, already present as root for deletion.\n",
      "15/10/03 19:02:47 INFO MemoryStore: MemoryStore cleared\n",
      "15/10/03 19:02:47 INFO BlockManager: BlockManager stopped\n",
      "15/10/03 19:02:47 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "15/10/03 19:02:47 INFO SparkContext: Successfully stopped SparkContext\n",
      "15/10/03 19:02:47 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "15/10/03 19:02:47 INFO SparkContext: SparkContext already stopped.\n",
      "15/10/03 19:02:47 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.\n",
      "15/10/03 19:02:47 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.\n",
      "15/10/03 19:02:47 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.\n",
      "15/10/03 19:02:48 INFO Utils: Shutdown hook called\n",
      "15/10/03 19:02:48 INFO Utils: Deleting directory /private/var/folders/vx/9464rz3s72b0l50g6wwpr2hm0000gn/T/spark-9d30a6a7-1fba-4666-9b8e-83314e13730b/pyspark-2ddc307c-a3e7-4c2f-9b6c-d15e492b8503\n",
      "15/10/03 19:02:48 INFO Utils: Deleting directory /private/var/folders/vx/9464rz3s72b0l50g6wwpr2hm0000gn/T/spark-9d30a6a7-1fba-4666-9b8e-83314e13730b\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "$SPARK_HOME/bin/spark-submit test_join.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: If you are joining with a large batch RDD how can you minimize the\n",
    "shuffling of the records?\n",
    "</summary>\n",
    "1. Use `partitionBy` on the incoming RDDs as well as on the batch\n",
    "RDD.<br>\n",
    "2. This will ensure that records are partitioned by their keys.<br>\n",
    "3. This can make a real difference in the performance of your Big Data\n",
    "streaming app.<br>\n",
    "</details>\n",
    "\n",
    "Cluster View\n",
    "------------\n",
    "\n",
    "<img src=\"images/streaming-daemons.png\">\n",
    "\n",
    "Checkpointing\n",
    "-------------\n",
    "\n",
    "Q: How can I protect my streaming app against failure?\n",
    "\n",
    "- Streaming apps run for much longer than batch apps.\n",
    "\n",
    "- They can run for days and weeks.\n",
    "\n",
    "- So fault-tolerance is important for them.\n",
    "\n",
    "- To enable recovery from failure you must enable checkpointing.\n",
    "\n",
    "- If a checkpointed application crashes, you restart it and it\n",
    "  recovers the state of the RDDs when it crashed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_checkpointing.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_checkpointing.py\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from text_file_util import xrange_write\n",
    "    \n",
    "from pprint import pprint\n",
    "    \n",
    "def updateFunction(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    return sum(newValues, runningCount)  \n",
    "    \n",
    "checkpointDir = 'ckpt'\n",
    "    \n",
    "def functionToCreateContext():\n",
    "    ssc = StreamingContext(SparkContext(), batchDuration=2)\n",
    "    \n",
    "    # Add new values with previous running count to get new count\n",
    "    ds = ssc.textFileStream('input') \\\n",
    "        .map(lambda x: int(x) % 10) \\\n",
    "        .map(lambda x: (x,1)) \\\n",
    "        .updateStateByKey(updateFunction)\n",
    "    ds.pprint()\n",
    "    ds.count().pprint()\n",
    "    \n",
    "    # Set up checkpoint\n",
    "    ssc.checkpoint(checkpointDir)\n",
    "    return ssc\n",
    "    \n",
    "print 'Initializing ssc'\n",
    "ssc = StreamingContext.getOrCreate(\n",
    "    checkpointDir, functionToCreateContext)\n",
    "    \n",
    "print 'Starting ssc'\n",
    "ssc.start()\n",
    "    \n",
    "# Write data to textFileStream\n",
    "xrange_write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets run this and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "$SPARK_HOME/bin/spark-submit test_checkpointing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The program will run forever. To terminate hit `Ctrl-C`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
