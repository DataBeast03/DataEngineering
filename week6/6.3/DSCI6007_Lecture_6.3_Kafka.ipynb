{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Kafka\n",
    "============\n",
    "\n",
    "Kafka\n",
    "-----\n",
    "\n",
    "![](images/kafka-logo.png)\n",
    "\n",
    "What is Kafka?\n",
    "\n",
    "- Kafka is like TiVo for real time messages.\n",
    "\n",
    "- Producers write events.\n",
    "\n",
    "- Consumers process events when they can.\n",
    "\n",
    "What are events?\n",
    "\n",
    "- Events are messages or objects.\n",
    "\n",
    "- Think data structures containing fields.\n",
    "\n",
    "- Like SQL records or CSV records.\n",
    "\n",
    "Objectives\n",
    "----------\n",
    "\n",
    "By the end of today's lesson, you will be able to:\n",
    "\n",
    "- Discuss what problem Kafka solves and why it exists\n",
    "\n",
    "- Discuss what problems Kafka does not solve, and for which you need\n",
    "  to integrate it with HBase and Spark Streaming.\n",
    "\n",
    "- Install and launch Kafka. \n",
    "\n",
    "- Implement Kafka to handle heavy message volumes using *partitions*,\n",
    "  *consumer groups*, and *replication*.\n",
    "\n",
    "- Implement Kafka *producers* and *consumers* that can send and\n",
    "  receive messages through Kafka. \n",
    "\n",
    "Etymology\n",
    "---------\n",
    "\n",
    "Why is it called Kafka?\n",
    "\n",
    "According to [Jay Kreps](https://news.ycombinator.com/item?id=6878186)\n",
    "\n",
    "- The rationale was that it should be a writer because we were\n",
    "  building a distributed log or journal (a service dedicated to\n",
    "  writing). \n",
    "\n",
    "- The writer needed to be someone that (1) I liked, (2) sounded cool\n",
    "  as a name, (3) was dead (because it would be creepy to use the name\n",
    "  of someone who was alive).\n",
    "\n",
    "Jay Kreps\n",
    "---------\n",
    "\n",
    "![](images/kafka-jay-kreps.jpg)\n",
    "\n",
    "Who is Jay Kreps?\n",
    "\n",
    "- Co-founder of <http://confluent.io>. \n",
    "\n",
    "- Architect and co-creator of Apache Kafka.\n",
    "\n",
    "- Author of [*I Heart Logs*](http://shop.oreilly.com/product/0636920034339.do)\n",
    "\n",
    "Franz Kafka\n",
    "-----------\n",
    "\n",
    "![](images/kafka-franz.jpg)\n",
    "\n",
    "History of Kafka\n",
    "----------------\n",
    "\n",
    "Year |Event\n",
    "---- |-----\n",
    "2011 |Open sourced at LinkedIn\n",
    "2012 |Version 0.7.1 released\n",
    "2012 |Graduated from Apache incubator status\n",
    "2013 |Version 0.8.0 released\n",
    "2014 |Confluent spins off from LinkedIn with goal to productize Kafka\n",
    "2014 |Version 0.8.1 released\n",
    "2015 |Version 0.8.2.0 released\n",
    "\n",
    "Programming Languages\n",
    "---------------------\n",
    "\n",
    "What language is Kafka written in?\n",
    "\n",
    "- Kafka is mostly written in Scala.\n",
    "\n",
    "- Its API supports Scala, Java, and Python.\n",
    "\n",
    "Why Does Kafka Exist\n",
    "--------------------\n",
    "\n",
    "LinkedInâ€™s motivation for Kafka was *A unified platform for handling all the\n",
    " data feeds a large company might have.* Some of the 'must haves' kept\n",
    "in mind by LinkedIn engineers were:\n",
    "\n",
    "- High throughput to support high volume event feeds.\n",
    "- Support  processing of these feeds to create new, derived feeds.\n",
    "- Support large data backlogs to handle periodic ingestion from offline systems.\n",
    "- Support low-latency delivery to handle more traditional messaging use cases.\n",
    "- Guarantee fault-tolerance in the presence of machine failures.\n",
    "\n",
    "Kafka Architecture\n",
    "------------------\n",
    "\n",
    "Image of a 2013 discussion of Kafka's Data Architecture at LinkedIn:\n",
    "\n",
    "![](images/kafka-linkedIn2013.png)\n",
    "\n",
    "Use Cases\n",
    "---------\n",
    "\n",
    "- **LinkedIn** \n",
    "    - Processes more than 200 billion events/day via Kafka. \n",
    "    - Primary types of data being processed through Kafka are:\n",
    "        - Metrics: operational telemetry data\n",
    "        - Tracking: everything a LinkedIn.com user does\n",
    "        - Queuing: between LinkedIn apps, e.g. for sending emails\n",
    "- **Netflix** \n",
    "    - Real time monitoring and event processing.\n",
    "- **Twitter** \n",
    "    - As part of their Storm and Heron  data pipelines.\n",
    "- **Spotify**\n",
    "    - Log delivery (from 4h down to 10s), Hadoop.\n",
    "- **Loggly** \n",
    "    - Log collection and processing.\n",
    "- **Mozilla** \n",
    "    - Telemetry data.\n",
    "    - Data that Firefox receives from browsers in the wild.\n",
    "\n",
    "Why Is Kafka Fast\n",
    "-----------------\n",
    "\n",
    "- Fast writes:\n",
    "    - While Kafka persists all data to disk, essentially all writes go\n",
    "      to the page cache of OS, i.e. RAM.\n",
    "    - Configuration hardware specs and OS tuning.\n",
    "- Fast reads:\n",
    "    - Very efficient to transfer data from page cache to a network\n",
    "      socket\n",
    "    - Kafka uses the `sendfile()` system call to transfer data\n",
    "      directly from disk to network, without copying it into multiple\n",
    "      buffers.\n",
    "- Combination of the two = fast Kafka!\n",
    "    - For example, on a Kafka cluster where the consumers are mostly\n",
    "      caught up you will see no read activity on the disks as they\n",
    "      will be serving data entirely from cache.\n",
    "\n",
    "Kafka and Big Data Systems\n",
    "--------------------------\n",
    "\n",
    "Why is Kafka used in Big Data systems?\n",
    "\n",
    "- Traditionally Big Data systems were batch-oriented.\n",
    "\n",
    "- There are many benefits to doing analytics in real time. \n",
    "\n",
    "- You can detect fraud, network intrusion, etc. and respond to events immediately.\n",
    "\n",
    "- Storm and Spark Streaming let you process data in real time.\n",
    "\n",
    "Why do we need Kafka with Storm and Spark Streaming?\n",
    "\n",
    "- What happens when your data comes in faster than you can process it?\n",
    "\n",
    "- Kafka buffers data so your processes don't crash.\n",
    "\n",
    "- If systems crash or go offline for maintenance Kafka will let them replay the data.\n",
    "\n",
    "- Kafka stores data for a week by default.\n",
    "\n",
    "Kafka Analogies\n",
    "---------------\n",
    "\n",
    "- Kafka is like a *Universal Power Supply* for data.\n",
    "\n",
    "- Kafka is like TiVo.\n",
    "\n",
    "- Kafka is like Twitter.\n",
    "\n",
    "Kafka, Spark, HBase\n",
    "-------------------\n",
    "\n",
    "<img src=\"images/kafka-spark-hbase.png\">\n",
    "\n",
    "Kafka, Spark, HBase\n",
    "-------------------\n",
    "\n",
    "How does Kafka relate to HBase and Spark?\n",
    "\n",
    "- Kafka, Spark Streaming, and HBase are frequently used together.\n",
    "\n",
    "- Kafka is the real time data pipeline.\n",
    "\n",
    "- Spark Streaming is the real time data processing engine.\n",
    "\n",
    "- HBase is the real time data storage.\n",
    "\n",
    "- Data comes in through Kafka, is processed by Spark Streaming, and is\n",
    "  then saved to HBase.\n",
    "\n",
    "\n",
    "Kafka Architecture\n",
    "==================\n",
    "\n",
    "Producers, Consumers, Brokers\n",
    "-----------------------------\n",
    "\n",
    "<img src=\"images/kafka-producer-broker-consumer.png\">\n",
    "\n",
    "Producers, Consumers, Brokers\n",
    "-----------------------------\n",
    "\n",
    "What are *producers*, *consumer*, *brokers*?\n",
    "\n",
    "- Kafka producers write messages to brokers.\n",
    "\n",
    "- Kafka brokers buffer and retain messages. By default they retain\n",
    "  messages for a week.\n",
    "\n",
    "- Kafka consumers read messages.\n",
    "\n",
    "- Producers and consumers are external to Kafka.\n",
    "\n",
    "- Brokers are what constitute Kafka.\n",
    "\n",
    "Producers, Consumers, Brokers\n",
    "-----------------------------\n",
    "\n",
    "Intuitively, what are *producers*, *consumer*, *brokers*?\n",
    "\n",
    "- Think of producers as people posting tweets.\n",
    "\n",
    "- Think of consumers as people reading tweets.\n",
    "\n",
    "- Think of brokers as Twitter itself.\n",
    "\n",
    "Topics\n",
    "------\n",
    "\n",
    "<img src=\"images/kafka-producer-topic-consumer.png\">\n",
    "\n",
    "Topics\n",
    "------\n",
    "\n",
    "What are *topics*?\n",
    "\n",
    "- Topics define message streams.\n",
    "\n",
    "- Producers write to specific topics.\n",
    "\n",
    "- Consumers read from specific topics.\n",
    "\n",
    "- Think of these as your favorite show on TV or as Twitter hashtags.\n",
    "\n",
    "Partitions\n",
    "----------\n",
    "\n",
    "<img src=\"images/kafka-topic-partitions.png\">\n",
    "\n",
    "\n",
    "Partitions\n",
    "----------\n",
    "\n",
    "What are *partitions*?\n",
    "\n",
    "- A topic is made up of partitions.\n",
    "\n",
    "- Producer write messages into partitions.\n",
    "\n",
    "- Each message goes into a particular partition.\n",
    "\n",
    "- Each partition is on a specific broker.\n",
    "\n",
    "Pop Quiz: Partitions\n",
    "--------------------\n",
    "\n",
    "<details><summary>\n",
    "Can a partition have more data than the disk space on a single broker\n",
    "machine?\n",
    "</summary>\n",
    "1. No. <br>\n",
    "2. A partition must fit on a single machine.<br>\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Can a topic have more data than the disk space on a single broker\n",
    "machine?\n",
    "</summary>\n",
    "1. Yes. <br>\n",
    "2. A topic can have multiple partitions.<br>\n",
    "3. With partitions Kafka a topic hold more messages than can fit on a machine.<br>\n",
    "</details>\n",
    "\n",
    "Partitions and Messages\n",
    "-----------------------\n",
    "\n",
    "![](images/kafka-anatomy-of-topic.png)\n",
    "\n",
    "Partitions and Messages\n",
    "-----------------------\n",
    "\n",
    "How does the producer determine which partition a message belongs to?\n",
    "\n",
    "- Default partitioning: round-robin.\n",
    "\n",
    "- Key partitioning: each message has a key and is partitioned by its\n",
    "  key.\n",
    "\n",
    "- Custom partitioner: a custom partitioner class has an opportunity to\n",
    "  look at the key and message and determine which partition to send\n",
    "  the message to.\n",
    "\n",
    "Pop Quiz: Partitioners\n",
    "----------------------\n",
    "\n",
    "<details><summary>\n",
    "If my topic has 5 partitions, will I have 5 copies of every message?\n",
    "</summary>\n",
    "1. No.<br>\n",
    "2. Each message is written in a particular partition.<br>\n",
    "</details>\n",
    "\n",
    "\n",
    "Partitions\n",
    "----------\n",
    "\n",
    "Why do partitions exist?\n",
    "\n",
    "- Partitions make the data more parallel.\n",
    "\n",
    "- Different consumers can consume partitions in parallel.\n",
    "\n",
    "Partitions, Consumers, Consumer Groups\n",
    "--------------------------------------\n",
    "\n",
    "![](images/kafka-consumergroup.png)\n",
    "\n",
    "Partitions, Consumers, Consumer Groups\n",
    "--------------------------------------\n",
    "\n",
    "How do collections of consumers coordinate their consuming? And what\n",
    "are *consumer groups*?\n",
    "\n",
    "- Consumer groups are collections of consumers that consume a topic\n",
    "  together.\n",
    "\n",
    "- The partitions are divided between the consumers in a *consumer\n",
    "  group*.\n",
    "\n",
    "- Each consumer group will see all the messages in the topic.\n",
    "\n",
    "- Each consumer in a consumer group will see all the messages in the\n",
    "  partitions assigned to it.\n",
    "\n",
    "- Each partition has exactly one consumer in each consumer group.\n",
    "\n",
    "\n",
    "Pop Quiz: Consumer Groups\n",
    "-------------------------\n",
    "\n",
    "<details><summary>\n",
    "Suppose you order two pizza for 4 people. One is meat the other is\n",
    "vegetarian. What Kafka concepts do slices, pizzas, and people\n",
    "correspond to?\n",
    "</summary>\n",
    "1. The slices are messages.<br>\n",
    "2. The pizzas are partitions.<br>\n",
    "3. The two vegetarians are consumers who are assigned the vegetarian\n",
    "partition.<br>\n",
    "4. The two non-vegetarians are consumers who are assigned the meat\n",
    "partition.<br>\n",
    "</details>\n",
    "\n",
    "Pop Quiz: Consumer Groups\n",
    "-------------------------\n",
    "\n",
    "<details><summary>\n",
    "If two consumer groups are consuming from the same topic, will each\n",
    "one of them get all the messages in the topic?\n",
    "</summary>\n",
    "1. Yes.<br>\n",
    "2. Every consumer group sees all the messages.<br>\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "If a consumer group has 2 consumers and the topic has 8 partitions,\n",
    "how many partitions will each consumer get?\n",
    "</summary>\n",
    "Each consumer will get 4 of the partitions.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "If a consumer group has 8 consumers and the topic has 2 partitions,\n",
    "how many partitions will each consumer get?\n",
    "</summary>\n",
    "1. Two of the 8 consumers will get a partition each.<br>\n",
    "2. The other 6 consumers will not get any partitions.<br>\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "If a consumer group has 2 consumers and the topic has 2 partitions,\n",
    "will each consumer see all the messages? \n",
    "</summary>\n",
    "1. No.<br>\n",
    "2. Each consumer will only see the topics in its partition.<br>\n",
    "</details>\n",
    "\n",
    "Partition Offset\n",
    "----------------\n",
    "\n",
    "How does a consumer keep track of where it is in the partition?\n",
    "\n",
    "- Messages in partitions are assigned a sequential id number called\n",
    "  the _offset_ that uniquely identifies each message within the\n",
    "  partition.\n",
    "\n",
    "- Consumers track their position in partitions through tuples made up\n",
    "  of `(topic, partition, offset)`.\n",
    "\n",
    "\n",
    "Replicas\n",
    "--------\n",
    "\n",
    "<img src=\"images/kafka-partition-replicas.png\">\n",
    "\n",
    "Replicas\n",
    "--------\n",
    "\n",
    "What happens if a broker fails? Do we lose all the messages on the\n",
    "partitions on that broker? \n",
    "\n",
    "- To prevent losing the partitions, partitions are replicated across\n",
    "  brokers.\n",
    "\n",
    "- The identical copies of a partition are called *replicas*.\n",
    "\n",
    "Leaders and Followers\n",
    "---------------------\n",
    "\n",
    "How does Kafka determine which replica to read from and write to?\n",
    "\n",
    "- The replicas are pure backup.\n",
    "\n",
    "- They are never read from or written to.\n",
    "\n",
    "- The brokers holding the partitions for a topic elect a leader.\n",
    "\n",
    "- The leader receives all the read and write requests.\n",
    "\n",
    "- The followers are plain consumers and try to keep up.\n",
    "\n",
    "Elections\n",
    "---------\n",
    "\n",
    "What happens if a leader dies?\n",
    "\n",
    "- The followers who are in sync with the leader nominate themselves\n",
    "  for the office of the leader.\n",
    "\n",
    "- If no one is in sync then the follower with the most messages\n",
    "  becomes the leader.\n",
    "\n",
    "Pop Quiz: Replication\n",
    "---------------------\n",
    "\n",
    "<details><summary>\n",
    "If a topic has 8 partitions, and each partition is replicated 3 times,\n",
    "how many copies of each message in this topic exist in Kafka?\n",
    "</summary>\n",
    "3 copies.\n",
    "</details>\n",
    "\n",
    "\n",
    "Installing Kafka\n",
    "================\n",
    "\n",
    "Downloading\n",
    "-----------\n",
    "\n",
    "How can I download Kafka?\n",
    "\n",
    "- Go to <http://kafka.apache.org/downloads.html>\n",
    "\n",
    "- Download the version labeled `Scala 2.10 - kafka_2.10-0.8.2.0.tgz (asc, md5)\n",
    "\n",
    "- This is the version Kafka's\n",
    "  [quickstart](http://kafka.apache.org/documentation.html#quickstart)\n",
    "  recommends.\n",
    "\n",
    "- Uncompress and untar the binary. Then switch to expanded directory.\n",
    "\n",
    "```bash\n",
    "tar xvzf kafka_2.10-0.8.2.0.tgz \n",
    "cd kafka_2.10-0.8.2.0\n",
    "```\n",
    "\n",
    "[kafka-quickstart]: \n",
    "\n",
    "Launching\n",
    "---------\n",
    "\n",
    "What are the steps for launching Kafka?\n",
    "\n",
    "- Start Zookeeper unless you already have it running. \n",
    "\n",
    "```bash\n",
    "./bin/zookeeper-server-start.sh config/zookeeper.properties\n",
    "```\n",
    "\n",
    "- Start the Kafka server which will launch the brokers.\n",
    "\n",
    "```bash\n",
    "./bin/kafka-server-start.sh config/server.properties\n",
    "```\n",
    "\n",
    "Kick Tires\n",
    "----------\n",
    "\n",
    "Let's take Kafka out for a spin. Kafka has command line producers and\n",
    "consumers that you can use to test it.\n",
    "\n",
    "- Create a topic.\n",
    "\n",
    "```bash\n",
    "./bin/kafka-topics.sh \\\n",
    "  --zookeeper localhost:2181 \\\n",
    "  --topic test \\\n",
    "  --create \\\n",
    "  --replication-factor 1 \\\n",
    "  --partitions 1\n",
    "```\n",
    "\n",
    "- See that the topic was created.\n",
    "\n",
    "```bash\n",
    "./bin/kafka-topics.sh  \\\n",
    "  --zookeeper localhost:2181 \\\n",
    "  --list\n",
    "```\n",
    "\n",
    "- Publish hello world message through command line producer.\n",
    "\n",
    "```bash\n",
    "echo 'hello world' | \\\n",
    "  ./bin/kafka-console-producer.sh \\\n",
    "    --broker-list localhost:9092 \\\n",
    "    --topic test \n",
    "```\n",
    "\n",
    "- Ignore warning `WARN Property topic is not valid (kafka.utils.VerifiableProperties)`. \n",
    "\n",
    "- This is a [known bug](https://issues.apache.org/jira/browse/KAFKA-1711).\n",
    "\n",
    "- Consume message through command line consumer.\n",
    "\n",
    "```bash\n",
    "./bin/kafka-console-consumer.sh \\\n",
    "  --zookeeper localhost:2181 \\\n",
    "  --topic test \\\n",
    "  --from-beginning\n",
    "```\n",
    "\n",
    "Help\n",
    "----\n",
    "\n",
    "What other options do these command line tools have?\n",
    "\n",
    "- For help on these command line tools run them without any arguments.\n",
    "\n",
    "- They will list out all their options and what effect they have.\n",
    "\n",
    "\n",
    "Kafka Commands\n",
    "==============\n",
    "\n",
    "Creating Topics\n",
    "---------------\n",
    "\n",
    "How can I create a topic?\n",
    "\n",
    "You can create topics using the command line `kafka-topics.sh` tool.\n",
    "\n",
    "```bash\n",
    "./bin/kafka-topics.sh \\\n",
    "  --zookeeper localhost:2181 \\\n",
    "  --topic my-topic \\\n",
    "  --create \\\n",
    "  --replication-factor 1 \\\n",
    "  --partitions 1\n",
    "```\n",
    "\n",
    "Changing Topic Configuration\n",
    "----------------------------\n",
    "\n",
    "How can I change the configuration value per topic?\n",
    "\n",
    "You can change the maximum message size from `1000*1000` to `64000`.\n",
    "\n",
    "```bash\n",
    "./bin/kafka-topics.sh \\\n",
    "  --zookeeper localhost:2181 \\\n",
    "  --topic my-topic \\\n",
    "  --alter \\\n",
    "  --config max.message.bytes=64000\n",
    "```\n",
    "\n",
    "Describing Topic\n",
    "----------------\n",
    "\n",
    "How can I find the configuration settings of a topic?\n",
    "\n",
    "```bash\n",
    "./bin/kafka-topics.sh \\\n",
    "  --zookeeper localhost:2181 \\\n",
    "  --topic my-topic \\\n",
    "  --describe\n",
    "```\n",
    "\n",
    "Reverting Configuration Overrides\n",
    "---------------------------------\n",
    "\n",
    "How can I revert a configuration override?\n",
    "\n",
    "```bash\n",
    "./bin/kafka-topics.sh \\\n",
    "  --zookeeper localhost:2181 \\\n",
    "  --topic my-topic \\\n",
    "  --alter \\\n",
    "  --deleteConfig max.message.bytes\n",
    "```\n",
    "\n",
    "Deleting Topics\n",
    "---------------\n",
    "\n",
    "How can I delete a topic?\n",
    "\n",
    "```bash\n",
    "./bin/kafka-topics.sh \\\n",
    "  --zookeeper localhost:2181 \\\n",
    "  --topic my-topic \\\n",
    "  --delete \n",
    "```\n",
    "\n",
    "Note: `--delete` was\n",
    "[broken](https://issues.apache.org/jira/browse/KAFKA-1397) until\n",
    "version `0.8.2.0`.\n",
    "\n",
    "Kafka API\n",
    "=========\n",
    "\n",
    "Sending and Receiving Strings\n",
    "-----------------------------\n",
    "\n",
    "How can I use Kafka to send and receive strings?\n",
    "\n",
    "```python\n",
    "import threading, logging, time\n",
    "\n",
    "from kafka.client   import KafkaClient\n",
    "from kafka.consumer import SimpleConsumer\n",
    "from kafka.producer import SimpleProducer\n",
    "\n",
    "class Producer(threading.Thread):\n",
    "    '''Sends messages to Kafka topic.'''\n",
    "    daemon = True\n",
    "    def run(self):\n",
    "        client = KafkaClient(\"localhost:9092\")\n",
    "        producer = SimpleProducer(client)\n",
    "        while True:\n",
    "            producer.send_messages('my-topic', \"test\")\n",
    "            producer.send_messages('my-topic', \"Hello, world!\")\n",
    "            time.sleep(1)\n",
    "\n",
    "\n",
    "class Consumer(threading.Thread):\n",
    "    '''Consumes messages from Kafka topic.'''\n",
    "    daemon = True\n",
    "    def run(self):\n",
    "        client = KafkaClient(\"localhost:9092\")\n",
    "        consumer = SimpleConsumer(client, \"test-group\", \"my-topic\")\n",
    "        for message in consumer:\n",
    "            print(message)\n",
    "\n",
    "def main():\n",
    "    '''Starts producer and consumer threads.'''\n",
    "    threads = [ Producer(), Consumer() ]\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "    time.sleep(5)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s.%(name)s:%(message)s',\n",
    "        level=logging.DEBUG)\n",
    "    main()\n",
    "```\n",
    "\n",
    "Kafka and Avro\n",
    "--------------\n",
    "\n",
    "<img src=\"images/kafka-avro.png\">\n",
    "\n",
    "Kafka and Avro\n",
    "--------------\n",
    "\n",
    "Why is Avro relevant to Kafka?\n",
    "\n",
    "- Kafka messages are byte sequences.\n",
    "\n",
    "- Kafka only knows how to handle byte sequences.\n",
    "\n",
    "- To send rich data structures through Kafka you have to serialize\n",
    "  them.\n",
    "\n",
    "- Avro gives you efficient and portable serialization.\n",
    "\n",
    "Sending and Receiving Avro\n",
    "--------------------------\n",
    "\n",
    "How can I use Kafka to send and receive arbitrary data structures?\n",
    "\n",
    "```python\n",
    "import io, random, threading, logging, time\n",
    "\n",
    "import avro.io\n",
    "import avro.schema\n",
    "\n",
    "from kafka.client   import KafkaClient\n",
    "from kafka.consumer import KafkaConsumer\n",
    "from kafka.producer import SimpleProducer\n",
    "\n",
    "KAFKA_TOPIC = 'my-topic'\n",
    "\n",
    "AVRO_SCHEMA_STRING = '''{\n",
    "\t\"namespace\": \"example.avro\",\n",
    "\t\"type\": \"record\",\n",
    "\t\"name\": \"User\",\n",
    "\t\"fields\": [\n",
    "\t\t{\"name\": \"name\", \"type\": \"string\"},\n",
    "\t\t{\"name\": \"favorite_number\",  \"type\": [\"int\", \"null\"]},\n",
    "\t\t{\"name\": \"favorite_color\", \"type\": [\"string\", \"null\"]}\n",
    "\t]\n",
    "}\n",
    "'''\n",
    "\n",
    "class AvroSerDe:\n",
    "    '''Serializes and deserializes data structures using Avro.'''\n",
    "    def __init__(self, avro_schema_string):\n",
    "        self.schema = avro.schema.parse(avro_schema_string)\n",
    "        self.datum_writer = avro.io.DatumWriter(self.schema)\n",
    "        self.datum_reader = avro.io.DatumReader(self.schema)\n",
    "\n",
    "    def obj_to_bytes(self, obj):\n",
    "        bytes_writer = io.BytesIO()\n",
    "        encoder = avro.io.BinaryEncoder(bytes_writer)\n",
    "        self.datum_writer.write(obj, encoder)\n",
    "        raw_bytes = bytes_writer.getvalue()\n",
    "        return raw_bytes\n",
    "\n",
    "    def bytes_to_obj(self, raw_bytes):\n",
    "        bytes_reader = io.BytesIO(raw_bytes)\n",
    "        decoder = avro.io.BinaryDecoder(bytes_reader)\n",
    "        obj = self.datum_reader.read(decoder)\n",
    "        return obj\n",
    "\n",
    "def random_user():\n",
    "    '''Generate random user.'''\n",
    "    user = {'name': random.choice(['Alice','Bob','Cher','Diane','Elvis']),\n",
    "            'favorite_color': random.choice(['red','green','blue']),\n",
    "            'favorite_number': random.randint(0,10)}\n",
    "    return user\n",
    "\n",
    "class Producer(threading.Thread):\n",
    "    '''Produces users and publishes them to Kafka topic.'''\n",
    "    daemon = True\n",
    "    def run(self):\n",
    "        avro_serde = AvroSerDe(AVRO_SCHEMA_STRING)\n",
    "        client = KafkaClient('localhost:9092')\n",
    "        producer = SimpleProducer(client)\n",
    "        while True:\n",
    "            raw_bytes = avro_serde.obj_to_bytes(random_user())\n",
    "            producer.send_messages(KAFKA_TOPIC, raw_bytes)\n",
    "            time.sleep(1)\n",
    "\n",
    "class Consumer(threading.Thread):\n",
    "    '''Consumes users from Kafka topic.'''\n",
    "    daemon = True\n",
    "    def run(self):\n",
    "        avro_serde = AvroSerDe(AVRO_SCHEMA_STRING)\n",
    "        client = KafkaClient('localhost:9092')\n",
    "        consumer = KafkaConsumer(KAFKA_TOPIC,\n",
    "                                 group_id='my_group',\n",
    "                                 bootstrap_servers=['localhost:9092'])\n",
    "        for message in consumer:\n",
    "            user = avro_serde.bytes_to_obj(message.value)\n",
    "            print '--> ' + str(user)\n",
    "\n",
    "def main():\n",
    "    '''Starts producer and consumer threads.'''\n",
    "    threads = [ Producer(), Consumer() ]\n",
    "    for t in threads: t.start()\n",
    "    time.sleep(5)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s.%(name)s:%(message)s',\n",
    "        level=logging.DEBUG)\n",
    "    main()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
