{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab\n",
    "===\n",
    "\n",
    "**Repeat the exercise from class, but this time use StarCluster.**\n",
    "\n",
    "Add the `ipcluster` plugin if you haven't already.\n",
    "\n",
    "Near the bottom of `.starcluster/config`:\n",
    "```bash\n",
    "######################\n",
    "## Built-in Plugins ##\n",
    "######################\n",
    "# The following plugins ship with StarCluster and should work out-of-the-box.\n",
    "# Uncomment as needed. Don't forget to update your PLUGINS list!\n",
    "# See http://star.mit.edu/cluster/docs/latest/plugins for plugin details.\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "[plugin ipcluster]\n",
    "SETUP_CLASS = starcluster.plugins.ipcluster.IPCluster\n",
    "# Enable the IPython notebook server (optional)\n",
    "ENABLE_NOTEBOOK = True\n",
    "# Set a password for the notebook for increased security\n",
    "# This is optional but *highly* recommended\n",
    "NOTEBOOK_PASSWD = a-secret-password\n",
    "```\n",
    "\n",
    "Set `CLUSTER_SIZE` to `3` for more memory *(see [aws.amazon.com/ec2/instance-types](http://aws.amazon.com/ec2/instance-types/) for details)*:\n",
    "```bash\n",
    "[cluster smallcluster]\n",
    "# number of ec2 instances to launch\n",
    "CLUSTER_SIZE = 3\n",
    "NODE_IMAGE_ID = ami-6b211202\n",
    "PLUGINS = ipcluster\n",
    "SPOT_BID = 0.10\n",
    "```\n",
    "Also set `SPOT_BID` to `0.10` (or less?) to save \\$\\$\\$ *(see [aws.amazon.com/ec2/purchasing-options/spot-instances](http://aws.amazon.com/ec2/purchasing-options/spot-instances/) for details)*\n",
    "\n",
    "Start your new cluster:\n",
    "\n",
    "`$ starcluster start my_cluster`\n",
    "\n",
    "Copy your credentials to your cluster:\n",
    "\n",
    "`$ starcluster put my_cluster --user sgeadmin ~/Downloads/credentials.csv /home/sgeadmin/`\n",
    "\n",
    "This should, as a side effect, add your cluster to the list of known hosts on your machine. \n",
    "In my experience, it often doesn't, however. \n",
    "Therefore, **you will want to:**\n",
    "\n",
    "```bash\n",
    "starcluster sshmaster my_cluster\n",
    "```\n",
    "NOTE: Logs me into my master/leader machine\n",
    "\n",
    "**before you do the following (or `Client` will hang forever):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.parallel import Client\n",
    "from os.path import expanduser\n",
    "\n",
    "url_file = expanduser('~/.starcluster/ipcluster/SecurityGroup:@sc-my_cluster-us-east-1.json')\n",
    "sshkey = expanduser('~/.ssh/Amazon_AWS_DataGuy.pem') \n",
    "client = Client(url_file, \n",
    "                sshkey = sshkey)\n",
    "\n",
    "# the 'client' object can be used to reference the leader & worker instances \n",
    "# that are working on the cloud cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see how many engines you have running:\n",
    "# One engine is the Leader\n",
    "# The other two engines are the followers\n",
    "\n",
    "dview = client.direct_view()\n",
    "len(client.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# enables us to better functionally use each individual engine\n",
    "all_engines = client[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hostname():\n",
    "    \"\"\"Return the name of the host where the function is being called\"\"\"\n",
    "    import socket\n",
    "    return socket.gethostname()\n",
    "\n",
    "hostname_apply_result = all_engines.apply(hostname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['master', 'node002', 'node001']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get engine names\n",
    "hostname_apply_result.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'master', 1: 'node002', 2: 'node001'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# organize engine names with key values paris \n",
    "hostnames = hostname_apply_result.get_dict()\n",
    "hostname_apply_result.get_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# By using the engine name as the key, \n",
    "# we can refer to unique engines \n",
    "# and assign tasks to a specific engine \n",
    "one_engine_by_host = dict((hostname, engine_id) for engine_id, hostname\n",
    "                      in hostnames.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'master': 0, 'node001': 2, 'node002': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_engine_by_host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing numpy on engine(s)\n"
     ]
    }
   ],
   "source": [
    "# import needed libraries to all engines being used \n",
    "with all_engines.sync_imports():\n",
    "    import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading/unpacking scikit-learn\r\n",
      "  Running setup.py egg_info for package scikit-learn\r\n",
      "    Partial import of sklearn during the build process.\r\n",
      "    \r\n",
      "Installing collected packages: scikit-learn\r\n",
      "  Running setup.py install for scikit-learn\r\n",
      "    Partial import of sklearn during the build process.\r\n",
      "    blas_opt_info:\r\n",
      "    blas_mkl_info:\r\n",
      "      libraries mkl,vml,guide not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/x86_64-linux-gnu']\r\n",
      "      NOT AVAILABLE\r\n",
      "    \r\n",
      "    openblas_info:\r\n",
      "      libraries openblas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/x86_64-linux-gnu']\r\n",
      "      NOT AVAILABLE\r\n",
      "    \r\n",
      "    atlas_blas_threads_info:\r\n",
      "    Setting PTATLAS=ATLAS\r\n",
      "      libraries ptf77blas,ptcblas,atlas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/x86_64-linux-gnu']\r\n",
      "      NOT AVAILABLE\r\n",
      "    \r\n",
      "    atlas_blas_info:\r\n",
      "      libraries f77blas,cblas,atlas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/x86_64-linux-gnu']\r\n",
      "      NOT AVAILABLE\r\n",
      "    \r\n",
      "    /usr/local/lib/python2.7/dist-packages/numpy/distutils/system_info.py:1522: UserWarning:\r\n",
      "        Atlas (http://math-atlas.sourceforge.net/) libraries not found.\r\n",
      "        Directories to search for the libraries can be specified in the\r\n",
      "        numpy/distutils/site.cfg file (section [atlas]) or by setting\r\n",
      "        the ATLAS environment variable.\r\n",
      "      warnings.warn(AtlasNotFoundError.__doc__)\r\n",
      "    blas_info:\r\n",
      "      FOUND:\r\n",
      "        libraries = ['blas']\r\n",
      "        library_dirs = ['/usr/lib']\r\n",
      "        language = f77\r\n",
      "    \r\n",
      "      FOUND:\r\n",
      "        libraries = ['blas']\r\n",
      "        library_dirs = ['/usr/lib']\r\n",
      "        define_macros = [('NO_ATLAS_INFO', 1)]\r\n",
      "        language = f77\r\n",
      "    \r\n",
      "    sklearn/setup.py:73: UserWarning:\r\n",
      "        Blas (http://www.netlib.org/blas/) libraries not found.\r\n",
      "        Directories to search for the libraries can be specified in the\r\n",
      "        numpy/distutils/site.cfg file (section [blas]) or by setting\r\n",
      "        the BLAS environment variable.\r\n",
      "      warnings.warn(BlasNotFoundError.__doc__)\r\n",
      "    unifing config_cc, config, build_clib, build_ext, build commands --compiler options\r\n",
      "    unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options\r\n",
      "    build_src\r\n",
      "    building library \"libsvm-skl\" sources\r\n",
      "    building library \"cblas\" sources\r\n",
      "    building extension \"sklearn.__check_build._check_build\" sources\r\n",
      "    building extension \"sklearn.svm.libsvm\" sources\r\n",
      "    building extension \"sklearn.svm.liblinear\" sources\r\n",
      "    building extension \"sklearn.svm.libsvm_sparse\" sources\r\n",
      "    building extension \"sklearn.datasets._svmlight_format\" sources\r\n",
      "    building extension \"sklearn.feature_extraction._hashing\" sources\r\n",
      "    building extension \"sklearn.cluster._dbscan_inner\" sources\r\n",
      "    building extension \"sklearn.cluster._hierarchical\" sources\r\n",
      "    building extension \"sklearn.cluster._k_means\" sources\r\n",
      "    building extension \"sklearn.ensemble._gradient_boosting\" sources\r\n",
      "    building extension \"sklearn.utils.sparsetools._traversal\" sources\r\n",
      "    building extension \"sklearn.utils.sparsetools._graph_tools\" sources\r\n",
      "    building extension \"sklearn.utils.sparsefuncs_fast\" sources\r\n",
      "    building extension \"sklearn.utils.arrayfuncs\" sources\r\n",
      "    building extension \"sklearn.utils.murmurhash\" sources\r\n",
      "    building extension \"sklearn.utils.lgamma\" sources\r\n",
      "    building extension \"sklearn.utils.graph_shortest_path\" sources\r\n",
      "    building extension \"sklearn.utils.fast_dict\" sources\r\n",
      "    building extension \"sklearn.utils.seq_dataset\" sources\r\n",
      "    building extension \"sklearn.utils.weight_vector\" sources\r\n",
      "    building extension \"sklearn.utils._random\" sources\r\n",
      "    building extension \"sklearn.utils._logistic_sigmoid\" sources\r\n",
      "    building extension \"sklearn.neighbors.ball_tree\" sources\r\n",
      "    building extension \"sklearn.neighbors.kd_tree\" sources\r\n",
      "    building extension \"sklearn.neighbors.dist_metrics\" sources\r\n",
      "    building extension \"sklearn.neighbors.typedefs\" sources\r\n",
      "    building extension \"sklearn.manifold._utils\" sources\r\n",
      "    building extension \"sklearn.metrics.pairwise_fast\" sources\r\n",
      "    building extension \"sklearn.tree._tree\" sources\r\n",
      "    building extension \"sklearn.tree._utils\" sources\r\n",
      "    building extension \"sklearn.metrics/cluster.expected_mutual_info_fast\" sources\r\n",
      "    building extension \"sklearn._hmmc\" sources\r\n",
      "    building extension \"sklearn._isotonic\" sources\r\n",
      "    building extension \"sklearn.linear_model.cd_fast\" sources\r\n",
      "    building extension \"sklearn.linear_model.sgd_fast\" sources\r\n",
      "    building extension \"sklearn.utils.sparsetools._traversal\" sources\r\n",
      "    building extension \"sklearn.utils.sparsetools._graph_tools\" sources\r\n",
      "    building extension \"sklearn.utils.sparsefuncs_fast\" sources\r\n",
      "    building extension \"sklearn.utils.arrayfuncs\" sources\r\n",
      "    building extension \"sklearn.utils.murmurhash\" sources\r\n",
      "    building extension \"sklearn.utils.lgamma\" sources\r\n",
      "    building extension \"sklearn.utils.graph_shortest_path\" sources\r\n",
      "    building extension \"sklearn.utils.fast_dict\" sources\r\n",
      "    building extension \"sklearn.utils.seq_dataset\" sources\r\n",
      "    building extension \"sklearn.utils.weight_vector\" sources\r\n",
      "    building extension \"sklearn.utils._random\" sources\r\n",
      "    building extension \"sklearn.utils._logistic_sigmoid\" sources\r\n",
      "    building data_files sources\r\n",
      "    build_src: building npy-pkg config files\r\n",
      "    customize UnixCCompiler\r\n",
      "    customize UnixCCompiler using build_clib\r\n",
      "    customize UnixCCompiler\r\n",
      "    customize UnixCCompiler using build_ext\r\n",
      "    resetting extension 'sklearn.svm.liblinear' language from 'f77' to 'c++'.\r\n",
      "    customize UnixCCompiler\r\n",
      "    customize UnixCCompiler using build_ext\r\n",
      "    customize Gnu95FCompiler\r\n",
      "    Found executable /usr/bin/gfortran\r\n",
      "    customize Gnu95FCompiler\r\n",
      "    customize Gnu95FCompiler using build_ext\r\n",
      "    error: could not create '/usr/local/lib/python2.7/dist-packages/sklearn': Permission denied\r\n",
      "    Complete output from command /usr/bin/python -c \"import setuptools;__file__='/tmp/pip-build-sgeadmin/scikit-learn/setup.py';exec(compile(open(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /tmp/pip-zOc0JL-record/install-record.txt --single-version-externally-managed:\r\n",
      "    Partial import of sklearn during the build process.\r\n",
      "\r\n",
      "blas_opt_info:\r\n",
      "\r\n",
      "blas_mkl_info:\r\n",
      "\r\n",
      "  libraries mkl,vml,guide not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/x86_64-linux-gnu']\r\n",
      "\r\n",
      "  NOT AVAILABLE\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "openblas_info:\r\n",
      "\r\n",
      "  libraries openblas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/x86_64-linux-gnu']\r\n",
      "\r\n",
      "  NOT AVAILABLE\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "atlas_blas_threads_info:\r\n",
      "\r\n",
      "Setting PTATLAS=ATLAS\r\n",
      "\r\n",
      "  libraries ptf77blas,ptcblas,atlas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/x86_64-linux-gnu']\r\n",
      "\r\n",
      "  NOT AVAILABLE\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "atlas_blas_info:\r\n",
      "\r\n",
      "  libraries f77blas,cblas,atlas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/x86_64-linux-gnu']\r\n",
      "\r\n",
      "  NOT AVAILABLE\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "/usr/local/lib/python2.7/dist-packages/numpy/distutils/system_info.py:1522: UserWarning:\r\n",
      "\r\n",
      "    Atlas (http://math-atlas.sourceforge.net/) libraries not found.\r\n",
      "\r\n",
      "    Directories to search for the libraries can be specified in the\r\n",
      "\r\n",
      "    numpy/distutils/site.cfg file (section [atlas]) or by setting\r\n",
      "\r\n",
      "    the ATLAS environment variable.\r\n",
      "\r\n",
      "  warnings.warn(AtlasNotFoundError.__doc__)\r\n",
      "\r\n",
      "blas_info:\r\n",
      "\r\n",
      "  FOUND:\r\n",
      "\r\n",
      "    libraries = ['blas']\r\n",
      "\r\n",
      "    library_dirs = ['/usr/lib']\r\n",
      "\r\n",
      "    language = f77\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "  FOUND:\r\n",
      "\r\n",
      "    libraries = ['blas']\r\n",
      "\r\n",
      "    library_dirs = ['/usr/lib']\r\n",
      "\r\n",
      "    define_macros = [('NO_ATLAS_INFO', 1)]\r\n",
      "\r\n",
      "    language = f77\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "sklearn/setup.py:73: UserWarning:\r\n",
      "\r\n",
      "    Blas (http://www.netlib.org/blas/) libraries not found.\r\n",
      "\r\n",
      "    Directories to search for the libraries can be specified in the\r\n",
      "\r\n",
      "    numpy/distutils/site.cfg file (section [blas]) or by setting\r\n",
      "\r\n",
      "    the BLAS environment variable.\r\n",
      "\r\n",
      "  warnings.warn(BlasNotFoundError.__doc__)\r\n",
      "\r\n",
      "running install\r\n",
      "\r\n",
      "running build\r\n",
      "\r\n",
      "running config_cc\r\n",
      "\r\n",
      "unifing config_cc, config, build_clib, build_ext, build commands --compiler options\r\n",
      "\r\n",
      "running config_fc\r\n",
      "\r\n",
      "unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options\r\n",
      "\r\n",
      "running build_src\r\n",
      "\r\n",
      "build_src\r\n",
      "\r\n",
      "building library \"libsvm-skl\" sources\r\n",
      "\r\n",
      "building library \"cblas\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.__check_build._check_build\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.svm.libsvm\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.svm.liblinear\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.svm.libsvm_sparse\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.datasets._svmlight_format\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.feature_extraction._hashing\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.cluster._dbscan_inner\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.cluster._hierarchical\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.cluster._k_means\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.ensemble._gradient_boosting\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.utils.sparsetools._traversal\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.utils.sparsetools._graph_tools\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.utils.sparsefuncs_fast\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.utils.arrayfuncs\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.utils.murmurhash\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.utils.lgamma\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.utils.graph_shortest_path\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.utils.fast_dict\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.utils.seq_dataset\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.utils.weight_vector\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.utils._random\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.utils._logistic_sigmoid\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.neighbors.ball_tree\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.neighbors.kd_tree\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.neighbors.dist_metrics\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.neighbors.typedefs\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.manifold._utils\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.metrics.pairwise_fast\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.tree._tree\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.tree._utils\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.metrics/cluster.expected_mutual_info_fast\" sources\r\n",
      "\r\n",
      "building extension \"sklearn._hmmc\" sources\r\n",
      "\r\n",
      "building extension \"sklearn._isotonic\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.linear_model.cd_fast\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.linear_model.sgd_fast\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.utils.sparsetools._traversal\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.utils.sparsetools._graph_tools\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.utils.sparsefuncs_fast\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.utils.arrayfuncs\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.utils.murmurhash\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.utils.lgamma\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.utils.graph_shortest_path\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.utils.fast_dict\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.utils.seq_dataset\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.utils.weight_vector\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.utils._random\" sources\r\n",
      "\r\n",
      "building extension \"sklearn.utils._logistic_sigmoid\" sources\r\n",
      "\r\n",
      "building data_files sources\r\n",
      "\r\n",
      "build_src: building npy-pkg config files\r\n",
      "\r\n",
      "running build_py\r\n",
      "\r\n",
      "running build_clib\r\n",
      "\r\n",
      "customize UnixCCompiler\r\n",
      "\r\n",
      "customize UnixCCompiler using build_clib\r\n",
      "\r\n",
      "running build_ext\r\n",
      "\r\n",
      "customize UnixCCompiler\r\n",
      "\r\n",
      "customize UnixCCompiler using build_ext\r\n",
      "\r\n",
      "resetting extension 'sklearn.svm.liblinear' language from 'f77' to 'c++'.\r\n",
      "\r\n",
      "customize UnixCCompiler\r\n",
      "\r\n",
      "customize UnixCCompiler using build_ext\r\n",
      "\r\n",
      "customize Gnu95FCompiler\r\n",
      "\r\n",
      "Found executable /usr/bin/gfortran\r\n",
      "\r\n",
      "customize Gnu95FCompiler\r\n",
      "\r\n",
      "customize Gnu95FCompiler using build_ext\r\n",
      "\r\n",
      "running install_lib\r\n",
      "\r\n",
      "creating /usr/local/lib/python2.7/dist-packages/sklearn\r\n",
      "\r\n",
      "error: could not create '/usr/local/lib/python2.7/dist-packages/sklearn': Permission denied\r\n",
      "\r\n",
      "----------------------------------------\r\n",
      "Command /usr/bin/python -c \"import setuptools;__file__='/tmp/pip-build-sgeadmin/scikit-learn/setup.py';exec(compile(open(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /tmp/pip-zOc0JL-record/install-record.txt --single-version-externally-managed failed with error code 1 in /tmp/pip-build-sgeadmin/scikit-learn\r\n",
      "Storing complete log in /home/sgeadmin/.pip/pip.log\r\n"
     ]
    }
   ],
   "source": [
    "%%px  --targets=1 \n",
    "\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StarCluster - (http://star.mit.edu/cluster) (v. 0.95.6)\n",
      "Software Tools for Academics and Researchers (STAR)\n",
      "Please submit bug reports to starcluster@mit.edu\n",
      "\n",
      "digits_cv_000.pkl 100% |||||||||||||||||||||||||||||| Time: 00:00:00 577.39 K/s\n",
      "digits_cv_000.pkl_01.npy 100% ||||||||||||||||||||||| Time: 00:00:01 639.64 K/s\n",
      "digits_cv_000.pkl_02.npy 100% ||||||||||||||||||||||| Time: 00:00:00  25.84 M/s\n",
      "digits_cv_000.pkl_03.npy 100% ||||||||||||||||||||||| Time: 00:00:00  28.14 M/s\n",
      "digits_cv_000.pkl_04.npy 100% ||||||||||||||||||||||| Time: 00:00:00   4.09 M/s\n",
      "digits_cv_001.pkl 100% |||||||||||||||||||||||||||||| Time: 00:00:00 572.60 K/s\n",
      "digits_cv_001.pkl_01.npy 100% ||||||||||||||||||||||| Time: 00:00:00 875.33 K/s\n",
      "digits_cv_001.pkl_02.npy 100% ||||||||||||||||||||||| Time: 00:00:00  14.99 M/s\n",
      "digits_cv_001.pkl_03.npy 100% ||||||||||||||||||||||| Time: 00:00:00  27.39 M/s\n",
      "digits_cv_001.pkl_04.npy 100% ||||||||||||||||||||||| Time: 00:00:00   7.16 M/s\n",
      "digits_cv_002.pkl 100% |||||||||||||||||||||||||||||| Time: 00:00:00 530.21 K/s\n",
      "digits_cv_002.pkl_01.npy 100% ||||||||||||||||||||||| Time: 00:00:00 759.49 K/s\n",
      "digits_cv_002.pkl_02.npy 100% ||||||||||||||||||||||| Time: 00:00:00  25.49 M/s\n",
      "digits_cv_002.pkl_03.npy 100% ||||||||||||||||||||||| Time: 00:00:00  28.18 M/s\n",
      "digits_cv_002.pkl_04.npy 100% ||||||||||||||||||||||| Time: 00:00:00   4.99 M/s\n",
      "digits_cv_003.pkl 100% |||||||||||||||||||||||||||||| Time: 00:00:00 442.34 K/s\n",
      "digits_cv_003.pkl_01.npy 100% ||||||||||||||||||||||| Time: 00:00:00  29.30 M/s\n",
      "digits_cv_003.pkl_02.npy 100% ||||||||||||||||||||||| Time: 00:00:00  24.03 M/s\n",
      "digits_cv_003.pkl_03.npy 100% ||||||||||||||||||||||| Time: 00:00:00  30.54 M/s\n",
      "digits_cv_003.pkl_04.npy 100% ||||||||||||||||||||||| Time: 00:00:00   5.95 M/s\n",
      "digits_cv_004.pkl 100% |||||||||||||||||||||||||||||| Time: 00:00:00 643.51 K/s\n",
      "digits_cv_004.pkl_01.npy 100% ||||||||||||||||||||||| Time: 00:00:00 849.75 K/s\n",
      "digits_cv_004.pkl_02.npy 100% ||||||||||||||||||||||| Time: 00:00:00  15.83 M/s\n",
      "digits_cv_004.pkl_03.npy 100% ||||||||||||||||||||||| Time: 00:00:00   2.48 M/s\n",
      "digits_cv_004.pkl_04.npy 100% ||||||||||||||||||||||| Time: 00:00:00   7.30 M/s\n"
     ]
    }
   ],
   "source": [
    "# insert cross validation data files into starcluster instance \n",
    "\n",
    "! starcluster put my_cluster --user sgeadmin digits_cv_00* /mnt/sgeadmin/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to copy the files from the ephemeral drive (*i.e* `/mnt/`) on the host node to each of the other nodes. \n",
    "*e.g.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px -t0\n",
    "%%bash\n",
    "scp /mnt/sgeadmin/digits_cv_00* node001:/mnt/sgeadmin/\n",
    "scp /mnt/sgeadmin/digits_cv_00* node002:/mnt/sgeadmin/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "You will also want to create a new list of filenames:\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memmaping CV Splits for Multiprocess Dataset Sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can leverage the previous tools to build a utility function that $\\textbf{extracts Cross Validation splits ahead of time}$ to persist them on the hard drive in a format suitable for memmaping by IPython engine processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "import os\n",
    "\n",
    "def persist_cv_splits(X, y, n_cv_iter=5, name='data',\n",
    "    suffix=\"_cv_%03d.pkl\", test_size=0.25, random_state=None):\n",
    "    \"\"\"Materialize randomized train test splits of a dataset.\"\"\"\n",
    "\n",
    "    cv = ShuffleSplit(X.shape[0], n_iter=n_cv_iter,\n",
    "        test_size=test_size, random_state=random_state)\n",
    "    cv_split_filenames = []\n",
    "    \n",
    "    for i, (train, test) in enumerate(cv):\n",
    "        cv_fold = (X[train], y[train], X[test], y[test])\n",
    "        cv_split_filename = name + suffix % i\n",
    "        cv_split_filename = os.path.abspath(cv_split_filename)\n",
    "        joblib.dump(cv_fold, cv_split_filename)\n",
    "        cv_split_filenames.append(cv_split_filename)\n",
    "    \n",
    "    return cv_split_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "digits_split_filenames = persist_cv_splits(digits.data, digits.target,\n",
    "    name='digits', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/Alexander/DSCI6007-student/week1/1.4/digits_cv_000.pkl',\n",
       " '/Users/Alexander/DSCI6007-student/week1/1.4/digits_cv_001.pkl',\n",
       " '/Users/Alexander/DSCI6007-student/week1/1.4/digits_cv_002.pkl',\n",
       " '/Users/Alexander/DSCI6007-student/week1/1.4/digits_cv_003.pkl',\n",
       " '/Users/Alexander/DSCI6007-student/week1/1.4/digits_cv_004.pkl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits_split_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remote_filenames = ['/mnt/sgeadmin/' + filename.split('/')[-1] for filename in digits_split_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/mnt/sgeadmin/digits_cv_000.pkl',\n",
       " '/mnt/sgeadmin/digits_cv_001.pkl',\n",
       " '/mnt/sgeadmin/digits_cv_002.pkl',\n",
       " '/mnt/sgeadmin/digits_cv_003.pkl',\n",
       " '/mnt/sgeadmin/digits_cv_004.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 Alexander  staff   280B Aug 31 19:40 digits_cv_000.pkl\r\n",
      "-rw-r--r--  1 Alexander  staff   674K Aug 31 19:40 digits_cv_000.pkl_01.npy\r\n",
      "-rw-r--r--  1 Alexander  staff    11K Aug 31 19:40 digits_cv_000.pkl_02.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   225K Aug 31 19:40 digits_cv_000.pkl_03.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   3.6K Aug 31 19:40 digits_cv_000.pkl_04.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   280B Aug 31 19:40 digits_cv_001.pkl\r\n",
      "-rw-r--r--  1 Alexander  staff   674K Aug 31 19:40 digits_cv_001.pkl_01.npy\r\n",
      "-rw-r--r--  1 Alexander  staff    11K Aug 31 19:40 digits_cv_001.pkl_02.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   225K Aug 31 19:40 digits_cv_001.pkl_03.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   3.6K Aug 31 19:40 digits_cv_001.pkl_04.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   280B Aug 31 19:40 digits_cv_002.pkl\r\n",
      "-rw-r--r--  1 Alexander  staff   674K Aug 31 19:40 digits_cv_002.pkl_01.npy\r\n",
      "-rw-r--r--  1 Alexander  staff    11K Aug 31 19:40 digits_cv_002.pkl_02.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   225K Aug 31 19:40 digits_cv_002.pkl_03.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   3.6K Aug 31 19:40 digits_cv_002.pkl_04.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   280B Aug 31 19:40 digits_cv_003.pkl\r\n",
      "-rw-r--r--  1 Alexander  staff   674K Aug 31 19:40 digits_cv_003.pkl_01.npy\r\n",
      "-rw-r--r--  1 Alexander  staff    11K Aug 31 19:40 digits_cv_003.pkl_02.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   225K Aug 31 19:40 digits_cv_003.pkl_03.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   3.6K Aug 31 19:40 digits_cv_003.pkl_04.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   280B Aug 31 19:40 digits_cv_004.pkl\r\n",
      "-rw-r--r--  1 Alexander  staff   674K Aug 31 19:40 digits_cv_004.pkl_01.npy\r\n",
      "-rw-r--r--  1 Alexander  staff    11K Aug 31 19:40 digits_cv_004.pkl_02.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   225K Aug 31 19:40 digits_cv_004.pkl_03.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   3.6K Aug 31 19:40 digits_cv_004.pkl_04.npy\r\n"
     ]
    }
   ],
   "source": [
    "ls -lh digits*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Model Selection and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': array([   0.1,    1. ,   10. ,  100. ]),\n",
      " 'gamma': array([  1.00000000e-04,   1.00000000e-03,   1.00000000e-02,\n",
      "         1.00000000e-01,   1.00000000e+00])}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "svc_params = {\n",
    "    'C': np.logspace(-1, 2, 4),\n",
    "    'gamma': np.logspace(-4, 0, 5),\n",
    "}\n",
    "pprint (svc_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GridSearchCV` internally uses the following `ParameterGrid` utility iterator class to build the possible combinations of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'C': 0.10000000000000001, 'gamma': 0.0001},\n",
       " {'C': 0.10000000000000001, 'gamma': 0.001},\n",
       " {'C': 0.10000000000000001, 'gamma': 0.01},\n",
       " {'C': 0.10000000000000001, 'gamma': 0.10000000000000001},\n",
       " {'C': 0.10000000000000001, 'gamma': 1.0},\n",
       " {'C': 1.0, 'gamma': 0.0001},\n",
       " {'C': 1.0, 'gamma': 0.001},\n",
       " {'C': 1.0, 'gamma': 0.01},\n",
       " {'C': 1.0, 'gamma': 0.10000000000000001},\n",
       " {'C': 1.0, 'gamma': 1.0},\n",
       " {'C': 10.0, 'gamma': 0.0001},\n",
       " {'C': 10.0, 'gamma': 0.001},\n",
       " {'C': 10.0, 'gamma': 0.01},\n",
       " {'C': 10.0, 'gamma': 0.10000000000000001},\n",
       " {'C': 10.0, 'gamma': 1.0},\n",
       " {'C': 100.0, 'gamma': 0.0001},\n",
       " {'C': 100.0, 'gamma': 0.001},\n",
       " {'C': 100.0, 'gamma': 0.01},\n",
       " {'C': 100.0, 'gamma': 0.10000000000000001},\n",
       " {'C': 100.0, 'gamma': 1.0}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.grid_search import ParameterGrid\n",
    "\n",
    "list(ParameterGrid(svc_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function to load the data from a CV split file and compute the validation score for a given parameter set and model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_evaluation(cv_split_filename, model, params):\n",
    "    \"\"\"Function executed by a worker to evaluate a model on a CV split\"\"\"\n",
    "    # All module imports should be executed in the worker namespace\n",
    "    from sklearn.externals import joblib\n",
    "\n",
    "    X_train, y_train, X_validation, y_validation = joblib.load(\n",
    "        cv_split_filename, mmap_mode='c')\n",
    "    \n",
    "    model.set_params(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    validation_score = model.score(X_validation, y_validation)\n",
    "    return validation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search(lb_view, model, cv_split_filenames, param_grid):\n",
    "    \"\"\"Launch all grid search evaluation tasks.\"\"\"\n",
    "    all_tasks = []\n",
    "    all_parameters = list(ParameterGrid(param_grid))\n",
    "    \n",
    "    for i, params in enumerate(all_parameters):\n",
    "        task_for_params = []\n",
    "        \n",
    "        for j, cv_split_filename in enumerate(cv_split_filenames):    \n",
    "            t = lb_view.apply(\n",
    "                compute_evaluation, cv_split_filename, model, params)\n",
    "            task_for_params.append(t) \n",
    "        \n",
    "        all_tasks.append(task_for_params)\n",
    "        \n",
    "    return all_parameters, all_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "lb_view = client.load_balanced_view()\n",
    "model = SVC()\n",
    "svc_params = {\n",
    "    'C': np.logspace(-1, 2, 4),\n",
    "    'gamma': np.logspace(-4, 0, 5),\n",
    "}\n",
    "\n",
    "all_parameters, all_tasks = grid_search(\n",
    "   lb_view, model, digits_split_filenames, svc_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `grid_search` function is using the asynchronous API of the `LoadBalancedView`, we can hence monitor the progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def progress(tasks):\n",
    "    return np.mean([task.ready() for task_group in tasks\n",
    "                                 for task in task_group])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks completed: 100.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"Tasks completed: {0}%\".format(100 * progress(all_tasks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better, we can introspect the completed task to find the best parameters set so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_bests(all_parameters, all_tasks, n_top=5):\n",
    "    \"\"\"Compute the mean score of the completed tasks\"\"\"\n",
    "    mean_scores = []\n",
    "    \n",
    "    for param, task_group in zip(all_parameters, all_tasks):\n",
    "        scores = [t.get() for t in task_group if t.ready()]\n",
    "        if len(scores) == 0:\n",
    "            continue\n",
    "        mean_scores.append((np.mean(scores), param))\n",
    "                   \n",
    "    return sorted(mean_scores, reverse=True)[:n_top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks completed: 100.0%\n"
     ]
    },
    {
     "ename": "RemoteError",
     "evalue": "ImportError(No module named sklearn.svm.classes)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/kernel/zmq/serialize.pyc\u001b[0m in \u001b[0;36munpack_apply_message\u001b[1;34m(bufs, g, copy)\u001b[0m",
      "\u001b[0;32m    184\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m",
      "\u001b[0;32m    185\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nargs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m",
      "\u001b[1;32m--> 186\u001b[1;33m         \u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg_bufs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munserialize_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg_bufs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m",
      "\u001b[0m\u001b[0;32m    187\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m",
      "\u001b[0;32m    188\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/kernel/zmq/serialize.pyc\u001b[0m in \u001b[0;36munserialize_object\u001b[1;34m(buffers, g)\u001b[0m",
      "\u001b[0;32m    123\u001b[0m         \u001b[1;31m# a zmq message\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m",
      "\u001b[0;32m    124\u001b[0m         \u001b[0mpobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m",
      "\u001b[1;32m--> 125\u001b[1;33m     \u001b[0mcanned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m",
      "\u001b[0m\u001b[0;32m    126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mistype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence_types\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcanned\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mMAX_ITEMS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m",
      "\u001b[0;32m    127\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcanned\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named sklearn.svm.classes"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "print(\"Tasks completed: {0}%\".format(100 * progress(all_tasks)))\n",
    "pprint(find_bests(all_parameters, all_tasks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A More Complete Parallel Model Selection and Assessment Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often wasteful to search all the possible combinations of parameters as done previously, especially if the number of parameters is large (e.g. more than 3).\n",
    "\n",
    "To speed up the discovery of good parameters combinations, it is often faster to randomized the search order and allocate a budget of evaluations, e.g. 10 or 100 combinations.\n",
    "\n",
    "See [this JMLR paper by James Bergstra](http://jmlr.csail.mit.edu/papers/v13/bergstra12a.html) for an empirical analysis of the problem. The interested reader should also have a look at [hyperopt](https://github.com/jaberg/hyperopt) that further refines this parameter search method using meta-optimizers.\n",
    "\n",
    "Randomized Parameter Search has just been implemented in the master branch of scikit-learn be part of the 0.14 release."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A More Complete Parallel Model Selection and Assessment Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107a81790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Some nice default configuration for plots\n",
    "plt.rcParams['figure.figsize'] = 10, 7.5\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.gray();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lb_view = client.load_balanced_view()\n",
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "CompositeError",
     "evalue": "one or more exceptions from call to method: load_in_memory\n[1:apply]: ImportError: No module named sklearn.externals\n[2:apply]: ImportError: No module named sklearn.externals\n[0:apply]: ImportError: No module named sklearn.externals",
     "output_type": "error",
     "traceback": [
      "[1:apply]: ",
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)\u001b[1;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m",
      "\u001b[1;32m/home/sgeadmin/mmap_utils.py\u001b[0m in \u001b[0;36mload_in_memory\u001b[1;34m(filenames)\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named sklearn.externals",
      "",
      "[2:apply]: ",
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)\u001b[1;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m",
      "\u001b[1;32m/home/sgeadmin/mmap_utils.py\u001b[0m in \u001b[0;36mload_in_memory\u001b[1;34m(filenames)\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named sklearn.externals",
      "",
      "[0:apply]: ",
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)\u001b[1;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m",
      "\u001b[1;32m/home/sgeadmin/mmap_utils.py\u001b[0m in \u001b[0;36mload_in_memory\u001b[1;34m(filenames)\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named sklearn.externals",
      ""
     ]
    }
   ],
   "source": [
    "import sys, imp\n",
    "from collections import OrderedDict\n",
    "sys.path.append('..')\n",
    "import model_selection, mmap_utils\n",
    "imp.reload(model_selection), imp.reload(mmap_utils)\n",
    "\n",
    "lb_view.abort()\n",
    "\n",
    "svc_params = OrderedDict([\n",
    "    ('gamma', np.logspace(-4, 0, 5)),\n",
    "    ('C', np.logspace(-1, 2, 4)),\n",
    "])\n",
    "\n",
    "search = model_selection.RandomizedGridSeach(lb_view)\n",
    "search.launch_for_splits(model, svc_params, digits_split_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#I cant go any further without first import (or installing) sklearn!!!\n",
    "# but starcluster doesn't allow me to do either!?!?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
