{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab\n",
    "===\n",
    "\n",
    "**Repeat the exercise from class, but this time use StarCluster.**\n",
    "\n",
    "Add the `ipcluster` plugin if you haven't already.\n",
    "\n",
    "Near the bottom of `.starcluster/config`:\n",
    "```bash\n",
    "######################\n",
    "## Built-in Plugins ##\n",
    "######################\n",
    "# The following plugins ship with StarCluster and should work out-of-the-box.\n",
    "# Uncomment as needed. Don't forget to update your PLUGINS list!\n",
    "# See http://star.mit.edu/cluster/docs/latest/plugins for plugin details.\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "[plugin ipcluster]\n",
    "SETUP_CLASS = starcluster.plugins.ipcluster.IPCluster\n",
    "# Enable the IPython notebook server (optional)\n",
    "ENABLE_NOTEBOOK = True\n",
    "# Set a password for the notebook for increased security\n",
    "# This is optional but *highly* recommended\n",
    "NOTEBOOK_PASSWD = a-secret-password\n",
    "```\n",
    "\n",
    "Set `CLUSTER_SIZE` to `3` for more memory *(see [aws.amazon.com/ec2/instance-types](http://aws.amazon.com/ec2/instance-types/) for details)*:\n",
    "```bash\n",
    "[cluster smallcluster]\n",
    "# number of ec2 instances to launch\n",
    "CLUSTER_SIZE = 3\n",
    "NODE_IMAGE_ID = ami-6b211202\n",
    "PLUGINS = ipcluster\n",
    "SPOT_BID = 0.10\n",
    "```\n",
    "Also set `SPOT_BID` to `0.10` (or less?) to save \\$\\$\\$ *(see [aws.amazon.com/ec2/purchasing-options/spot-instances](http://aws.amazon.com/ec2/purchasing-options/spot-instances/) for details)*\n",
    "\n",
    "Start your new cluster:\n",
    "\n",
    "`$ starcluster start my_cluster`\n",
    "\n",
    "Copy your credentials to your cluster:\n",
    "\n",
    "`$ starcluster put my_cluster --user sgeadmin ~/Downloads/credentials.csv /home/sgeadmin/`\n",
    "\n",
    "This should, as a side effect, add your cluster to the list of known hosts on your machine. \n",
    "In my experience, it often doesn't, however. \n",
    "Therefore, **you will want to:**\n",
    "\n",
    "```bash\n",
    "starcluster sshmaster my_cluster\n",
    "```\n",
    "NOTE: Logs me into my master/leader machine\n",
    "\n",
    "**before you do the following (or `Client` will hang forever):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.parallel import Client\n",
    "from os.path import expanduser\n",
    "\n",
    "url_file = expanduser('~/.starcluster/ipcluster/SecurityGroup:@sc-my_cluster-us-east-1.json')\n",
    "sshkey = expanduser('~/.ssh/Amazon_AWS_DataGuy.pem') \n",
    "client = Client(url_file, \n",
    "                sshkey = sshkey)\n",
    "\n",
    "# the 'client' object can be used to reference the leader & worker instances \n",
    "# that are working on the cloud cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see how many engines you have running:\n",
    "# One engine is the Leader\n",
    "# The other two engines are the followers\n",
    "\n",
    "dview = client.direct_view()\n",
    "len(client.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# enables us to better functionally use each individual engine\n",
    "all_engines = client[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hostname():\n",
    "    \"\"\"Return the name of the host where the function is being called\"\"\"\n",
    "    import socket\n",
    "    return socket.gethostname()\n",
    "\n",
    "hostname_apply_result = all_engines.apply(hostname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['master', 'node001', 'node002']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get engine names\n",
    "hostname_apply_result.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'master', 1: 'node001', 2: 'node002'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# organize engine names with key values paris \n",
    "hostnames = hostname_apply_result.get_dict()\n",
    "hostname_apply_result.get_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# By using the engine name as the key, \n",
    "# we can refer to unique engines \n",
    "# and assign tasks to a specific engine \n",
    "one_engine_by_host = dict((hostname, engine_id) for engine_id, hostname\n",
    "                      in hostnames.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'master': 0, 'node001': 1, 'node002': 2}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_engine_by_host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing numpy on engine(s)\n"
     ]
    }
   ],
   "source": [
    "# import needed libraries to all engines being used \n",
    "with all_engines.sync_imports():\n",
    "    import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): scikit-learn in /usr/local/lib/python2.7/dist-packages\r\n",
      "Cleaning up...\r\n"
     ]
    }
   ],
   "source": [
    "%%px  --targets=1 \n",
    "\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StarCluster - (http://star.mit.edu/cluster) (v. 0.95.6)\n",
      "Software Tools for Academics and Researchers (STAR)\n",
      "Please submit bug reports to starcluster@mit.edu\n",
      "\n",
      "digits_cv_000.pkl 100% |||||||||||||||||||||||||||||| Time: 00:00:00 595.84 K/s\n",
      "digits_cv_000.pkl_01.npy 100% ||||||||||||||||||||||| Time: 00:00:01 606.39 K/s\n",
      "digits_cv_000.pkl_02.npy 100% ||||||||||||||||||||||| Time: 00:00:00   5.89 M/s\n",
      "digits_cv_000.pkl_03.npy 100% ||||||||||||||||||||||| Time: 00:00:00  21.25 M/s\n",
      "digits_cv_000.pkl_04.npy 100% ||||||||||||||||||||||| Time: 00:00:00   7.36 M/s\n",
      "digits_cv_001.pkl 100% |||||||||||||||||||||||||||||| Time: 00:00:00 567.89 K/s\n",
      "digits_cv_001.pkl_01.npy 100% ||||||||||||||||||||||| Time: 00:00:00   1.34 M/s\n",
      "digits_cv_001.pkl_02.npy 100% ||||||||||||||||||||||| Time: 00:00:00   5.69 M/s\n",
      "digits_cv_001.pkl_03.npy 100% ||||||||||||||||||||||| Time: 00:00:00  21.42 M/s\n",
      "digits_cv_001.pkl_04.npy 100% ||||||||||||||||||||||| Time: 00:00:00   8.09 M/s\n",
      "digits_cv_002.pkl 100% |||||||||||||||||||||||||||||| Time: 00:00:00 648.13 K/s\n",
      "digits_cv_002.pkl_01.npy 100% ||||||||||||||||||||||| Time: 00:00:00 871.84 K/s\n",
      "digits_cv_002.pkl_02.npy 100% ||||||||||||||||||||||| Time: 00:00:00   5.96 M/s\n",
      "digits_cv_002.pkl_03.npy 100% ||||||||||||||||||||||| Time: 00:00:00 365.93 K/s\n",
      "digits_cv_002.pkl_04.npy 100% ||||||||||||||||||||||| Time: 00:00:00   6.75 M/s\n",
      "digits_cv_003.pkl 100% |||||||||||||||||||||||||||||| Time: 00:00:00 615.52 K/s\n",
      "digits_cv_003.pkl_01.npy 100% ||||||||||||||||||||||| Time: 00:00:01 488.68 K/s\n",
      "digits_cv_003.pkl_02.npy 100% ||||||||||||||||||||||| Time: 00:00:00   5.94 M/s\n",
      "digits_cv_003.pkl_03.npy 100% ||||||||||||||||||||||| Time: 00:00:00   1.02 M/s\n",
      "digits_cv_003.pkl_04.npy 100% ||||||||||||||||||||||| Time: 00:00:00   5.79 M/s\n",
      "digits_cv_004.pkl 100% |||||||||||||||||||||||||||||| Time: 00:00:00 588.38 K/s\n",
      "digits_cv_004.pkl_01.npy 100% ||||||||||||||||||||||| Time: 00:00:00 825.59 K/s\n",
      "digits_cv_004.pkl_02.npy 100% ||||||||||||||||||||||| Time: 00:00:00   5.67 M/s\n",
      "digits_cv_004.pkl_03.npy 100% ||||||||||||||||||||||| Time: 00:00:00   2.13 M/s\n",
      "digits_cv_004.pkl_04.npy 100% ||||||||||||||||||||||| Time: 00:00:00   7.28 M/s\n"
     ]
    }
   ],
   "source": [
    "# insert cross validation data files into starcluster instance \n",
    "\n",
    "! starcluster put my_cluster --user sgeadmin digits_cv_00* /mnt/sgeadmin/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to copy the files from the ephemeral drive (*i.e* `/mnt/`) on the host node to each of the other nodes. \n",
    "*e.g.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px -t0\n",
    "%%bash\n",
    "scp /mnt/sgeadmin/digits_cv_00* node001:/mnt/sgeadmin/\n",
    "scp /mnt/sgeadmin/digits_cv_00* node002:/mnt/sgeadmin/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "You will also want to create a new list of filenames:\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memmaping CV Splits for Multiprocess Dataset Sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can leverage the previous tools to build a utility function that $\\textbf{extracts Cross Validation splits ahead of time}$ to persist them on the hard drive in a format suitable for memmaping by IPython engine processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "import os\n",
    "\n",
    "def persist_cv_splits(X, y, n_cv_iter=5, name='data',\n",
    "    suffix=\"_cv_%03d.pkl\", test_size=0.25, random_state=None):\n",
    "    \"\"\"Materialize randomized train test splits of a dataset.\"\"\"\n",
    "\n",
    "    cv = ShuffleSplit(X.shape[0], n_iter=n_cv_iter,\n",
    "        test_size=test_size, random_state=random_state)\n",
    "    cv_split_filenames = []\n",
    "    \n",
    "    for i, (train, test) in enumerate(cv):\n",
    "        cv_fold = (X[train], y[train], X[test], y[test])\n",
    "        cv_split_filename = name + suffix % i\n",
    "        cv_split_filename = os.path.abspath(cv_split_filename)\n",
    "        joblib.dump(cv_fold, cv_split_filename)\n",
    "        cv_split_filenames.append(cv_split_filename)\n",
    "    \n",
    "    return cv_split_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "digits_split_filenames = persist_cv_splits(digits.data, digits.target,\n",
    "    name='digits', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remote_filenames = ['/mnt/sgeadmin/' + filename.split('/')[-1] for filename in digits_split_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/mnt/sgeadmin/digits_cv_000.pkl',\n",
       " '/mnt/sgeadmin/digits_cv_001.pkl',\n",
       " '/mnt/sgeadmin/digits_cv_002.pkl',\n",
       " '/mnt/sgeadmin/digits_cv_003.pkl',\n",
       " '/mnt/sgeadmin/digits_cv_004.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 Alexander  staff   280B Sep  6 16:40 digits_cv_000.pkl\r\n",
      "-rw-r--r--  1 Alexander  staff   674K Sep  6 16:40 digits_cv_000.pkl_01.npy\r\n",
      "-rw-r--r--  1 Alexander  staff    11K Sep  6 16:40 digits_cv_000.pkl_02.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   225K Sep  6 16:40 digits_cv_000.pkl_03.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   3.6K Sep  6 16:40 digits_cv_000.pkl_04.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   280B Sep  6 16:40 digits_cv_001.pkl\r\n",
      "-rw-r--r--  1 Alexander  staff   674K Sep  6 16:40 digits_cv_001.pkl_01.npy\r\n",
      "-rw-r--r--  1 Alexander  staff    11K Sep  6 16:40 digits_cv_001.pkl_02.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   225K Sep  6 16:40 digits_cv_001.pkl_03.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   3.6K Sep  6 16:40 digits_cv_001.pkl_04.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   280B Sep  6 16:40 digits_cv_002.pkl\r\n",
      "-rw-r--r--  1 Alexander  staff   674K Sep  6 16:40 digits_cv_002.pkl_01.npy\r\n",
      "-rw-r--r--  1 Alexander  staff    11K Sep  6 16:40 digits_cv_002.pkl_02.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   225K Sep  6 16:40 digits_cv_002.pkl_03.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   3.6K Sep  6 16:40 digits_cv_002.pkl_04.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   280B Sep  6 16:40 digits_cv_003.pkl\r\n",
      "-rw-r--r--  1 Alexander  staff   674K Sep  6 16:40 digits_cv_003.pkl_01.npy\r\n",
      "-rw-r--r--  1 Alexander  staff    11K Sep  6 16:40 digits_cv_003.pkl_02.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   225K Sep  6 16:40 digits_cv_003.pkl_03.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   3.6K Sep  6 16:40 digits_cv_003.pkl_04.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   280B Sep  6 16:40 digits_cv_004.pkl\r\n",
      "-rw-r--r--  1 Alexander  staff   674K Sep  6 16:40 digits_cv_004.pkl_01.npy\r\n",
      "-rw-r--r--  1 Alexander  staff    11K Sep  6 16:40 digits_cv_004.pkl_02.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   225K Sep  6 16:40 digits_cv_004.pkl_03.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   3.6K Sep  6 16:40 digits_cv_004.pkl_04.npy\r\n"
     ]
    }
   ],
   "source": [
    "ls -lh digits*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Model Selection and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': array([   0.1,    1. ,   10. ,  100. ]),\n",
      " 'gamma': array([  1.00000000e-04,   1.00000000e-03,   1.00000000e-02,\n",
      "         1.00000000e-01,   1.00000000e+00])}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "svc_params = {\n",
    "    'C': np.logspace(-1, 2, 4),\n",
    "    'gamma': np.logspace(-4, 0, 5),\n",
    "}\n",
    "pprint (svc_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GridSearchCV` internally uses the following `ParameterGrid` utility iterator class to build the possible combinations of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'C': 0.10000000000000001, 'gamma': 0.0001},\n",
       " {'C': 0.10000000000000001, 'gamma': 0.001},\n",
       " {'C': 0.10000000000000001, 'gamma': 0.01},\n",
       " {'C': 0.10000000000000001, 'gamma': 0.10000000000000001},\n",
       " {'C': 0.10000000000000001, 'gamma': 1.0},\n",
       " {'C': 1.0, 'gamma': 0.0001},\n",
       " {'C': 1.0, 'gamma': 0.001},\n",
       " {'C': 1.0, 'gamma': 0.01},\n",
       " {'C': 1.0, 'gamma': 0.10000000000000001},\n",
       " {'C': 1.0, 'gamma': 1.0},\n",
       " {'C': 10.0, 'gamma': 0.0001},\n",
       " {'C': 10.0, 'gamma': 0.001},\n",
       " {'C': 10.0, 'gamma': 0.01},\n",
       " {'C': 10.0, 'gamma': 0.10000000000000001},\n",
       " {'C': 10.0, 'gamma': 1.0},\n",
       " {'C': 100.0, 'gamma': 0.0001},\n",
       " {'C': 100.0, 'gamma': 0.001},\n",
       " {'C': 100.0, 'gamma': 0.01},\n",
       " {'C': 100.0, 'gamma': 0.10000000000000001},\n",
       " {'C': 100.0, 'gamma': 1.0}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.grid_search import ParameterGrid\n",
    "\n",
    "list(ParameterGrid(svc_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function to load the data from a CV split file and compute the validation score for a given parameter set and model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_evaluation(cv_split_filename, model, params):\n",
    "    \"\"\"Function executed by a worker to evaluate a model on a CV split\"\"\"\n",
    "    # All module imports should be executed in the worker namespace\n",
    "    from sklearn.externals import joblib\n",
    "\n",
    "    X_train, y_train, X_validation, y_validation = joblib.load(\n",
    "        cv_split_filename, mmap_mode='c')\n",
    "    \n",
    "    model.set_params(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    validation_score = model.score(X_validation, y_validation)\n",
    "    return validation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search(lb_view, model, cv_split_filenames, param_grid):\n",
    "    \"\"\"Launch all grid search evaluation tasks.\"\"\"\n",
    "    all_tasks = []\n",
    "    all_parameters = list(ParameterGrid(param_grid))\n",
    "    \n",
    "    for i, params in enumerate(all_parameters):\n",
    "        task_for_params = []\n",
    "        \n",
    "        for j, cv_split_filename in enumerate(cv_split_filenames):    \n",
    "            t = lb_view.apply(\n",
    "                compute_evaluation, cv_split_filename, model, params)\n",
    "            task_for_params.append(t) \n",
    "        \n",
    "        all_tasks.append(task_for_params)\n",
    "        \n",
    "    return all_parameters, all_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import ParameterGrid\n",
    "lb_view = client.load_balanced_view()\n",
    "model = SVC()\n",
    "svc_params = {\n",
    "    'C': np.logspace(-1, 2, 4),\n",
    "    'gamma': np.logspace(-4, 0, 5),\n",
    "}\n",
    "\n",
    "all_parameters, all_tasks = grid_search(\n",
    "   lb_view, model, digits_split_filenames, svc_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `grid_search` function is using the asynchronous API of the `LoadBalancedView`, we can hence monitor the progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def progress(tasks):\n",
    "    return np.mean([task.ready() for task_group in tasks\n",
    "                                 for task in task_group])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks completed: 0.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"Tasks completed: {0}%\".format(100 * progress(all_tasks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better, we can introspect the completed task to find the best parameters set so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_bests(all_parameters, all_tasks, n_top=5):\n",
    "    \"\"\"Compute the mean score of the completed tasks\"\"\"\n",
    "    mean_scores = []\n",
    "    \n",
    "    for param, task_group in zip(all_parameters, all_tasks):\n",
    "        scores = [t.get() for t in task_group if t.ready()]\n",
    "        if len(scores) == 0:\n",
    "            continue\n",
    "        mean_scores.append((np.mean(scores), param))\n",
    "                   \n",
    "    return sorted(mean_scores, reverse=True)[:n_top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks completed: 100.0%\n"
     ]
    },
    {
     "ename": "RemoteError",
     "evalue": "IOError([Errno 2] No such file or directory: '/Users/Alexander/DSCI6007-student/week1/1.4/digits_cv_000.pkl')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)\u001b[1;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m",
      "\u001b[1;32m<ipython-input-27-0c0098e5e308>\u001b[0m in \u001b[0;36mcompute_evaluation\u001b[1;34m(cv_split_filename, model, params)\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/numpy_pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m",
      "\u001b[0;32m    407\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0mwas\u001b[0m \u001b[0msaved\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0marrays\u001b[0m \u001b[0mcannot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mmemmaped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m",
      "\u001b[0;32m    408\u001b[0m     \"\"\"",
      "\u001b[1;32m--> 409\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile_handle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m",
      "\u001b[0m\u001b[0;32m    410\u001b[0m         \u001b[1;31m# We are careful to open the file handle early and keep it open to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m",
      "\u001b[0;32m    411\u001b[0m         \u001b[1;31m# avoid race-conditions on renames. That said, if data are stored in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/Users/Alexander/DSCI6007-student/week1/1.4/digits_cv_000.pkl'"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "print(\"Tasks completed: {0}%\".format(100 * progress(all_tasks)))\n",
    "pprint(find_bests(all_parameters, all_tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A More Complete Parallel Model Selection and Assessment Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often wasteful to search all the possible combinations of parameters as done previously, especially if the number of parameters is large (e.g. more than 3).\n",
    "\n",
    "To speed up the discovery of good parameters combinations, it is often faster to randomized the search order and allocate a budget of evaluations, e.g. 10 or 100 combinations.\n",
    "\n",
    "See [this JMLR paper by James Bergstra](http://jmlr.csail.mit.edu/papers/v13/bergstra12a.html) for an empirical analysis of the problem. The interested reader should also have a look at [hyperopt](https://github.com/jaberg/hyperopt) that further refines this parameter search method using meta-optimizers.\n",
    "\n",
    "Randomized Parameter Search has just been implemented in the master branch of scikit-learn be part of the 0.14 release."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A More Complete Parallel Model Selection and Assessment Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1073b28d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Some nice default configuration for plots\n",
    "plt.rcParams['figure.figsize'] = 10, 7.5\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.gray();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lb_view = client.load_balanced_view()\n",
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "CompositeError",
     "evalue": "one or more exceptions from call to method: load_in_memory\n[2:apply]: IOError: [Errno 2] No such file or directory: '/Users/Alexander/DSCI6007-student/week1/1.4/digits_cv_000.pkl'\n[1:apply]: IOError: [Errno 2] No such file or directory: '/Users/Alexander/DSCI6007-student/week1/1.4/digits_cv_000.pkl'\n[0:apply]: IOError: [Errno 2] No such file or directory: '/Users/Alexander/DSCI6007-student/week1/1.4/digits_cv_000.pkl'",
     "output_type": "error",
     "traceback": [
      "[2:apply]: ",
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)\u001b[1;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m",
      "\u001b[1;32m/home/sgeadmin/mmap_utils.py\u001b[0m in \u001b[0;36mload_in_memory\u001b[1;34m(filenames)\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/numpy_pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m",
      "\u001b[0;32m    407\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0mwas\u001b[0m \u001b[0msaved\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0marrays\u001b[0m \u001b[0mcannot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mmemmaped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m",
      "\u001b[0;32m    408\u001b[0m     \"\"\"",
      "\u001b[1;32m--> 409\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile_handle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m",
      "\u001b[0m\u001b[0;32m    410\u001b[0m         \u001b[1;31m# We are careful to open the file handle early and keep it open to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m",
      "\u001b[0;32m    411\u001b[0m         \u001b[1;31m# avoid race-conditions on renames. That said, if data are stored in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/Users/Alexander/DSCI6007-student/week1/1.4/digits_cv_000.pkl'",
      "",
      "[1:apply]: ",
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)\u001b[1;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m",
      "\u001b[1;32m/home/sgeadmin/mmap_utils.py\u001b[0m in \u001b[0;36mload_in_memory\u001b[1;34m(filenames)\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/numpy_pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m",
      "\u001b[0;32m    407\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0mwas\u001b[0m \u001b[0msaved\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0marrays\u001b[0m \u001b[0mcannot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mmemmaped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m",
      "\u001b[0;32m    408\u001b[0m     \"\"\"",
      "\u001b[1;32m--> 409\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile_handle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m",
      "\u001b[0m\u001b[0;32m    410\u001b[0m         \u001b[1;31m# We are careful to open the file handle early and keep it open to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m",
      "\u001b[0;32m    411\u001b[0m         \u001b[1;31m# avoid race-conditions on renames. That said, if data are stored in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/Users/Alexander/DSCI6007-student/week1/1.4/digits_cv_000.pkl'",
      "",
      "[0:apply]: ",
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)\u001b[1;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m",
      "\u001b[1;32m/home/sgeadmin/mmap_utils.py\u001b[0m in \u001b[0;36mload_in_memory\u001b[1;34m(filenames)\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/numpy_pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m",
      "\u001b[0;32m    407\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0mwas\u001b[0m \u001b[0msaved\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0marrays\u001b[0m \u001b[0mcannot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mmemmaped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m",
      "\u001b[0;32m    408\u001b[0m     \"\"\"",
      "\u001b[1;32m--> 409\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile_handle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m",
      "\u001b[0m\u001b[0;32m    410\u001b[0m         \u001b[1;31m# We are careful to open the file handle early and keep it open to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m",
      "\u001b[0;32m    411\u001b[0m         \u001b[1;31m# avoid race-conditions on renames. That said, if data are stored in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/Users/Alexander/DSCI6007-student/week1/1.4/digits_cv_000.pkl'",
      ""
     ]
    }
   ],
   "source": [
    "import sys, imp\n",
    "from collections import OrderedDict\n",
    "sys.path.append('..')\n",
    "import model_selection, mmap_utils\n",
    "imp.reload(model_selection), imp.reload(mmap_utils)\n",
    "\n",
    "lb_view.abort()\n",
    "\n",
    "svc_params = OrderedDict([\n",
    "    ('gamma', np.logspace(-4, 0, 5)),\n",
    "    ('C', np.logspace(-1, 2, 4)),\n",
    "])\n",
    "\n",
    "search = model_selection.RandomizedGridSeach(lb_view)\n",
    "search.launch_for_splits(model, svc_params, digits_split_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#I cant go any further without first import (or installing) sklearn!!!\n",
    "# but starcluster doesn't allow me to do either!?!?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
