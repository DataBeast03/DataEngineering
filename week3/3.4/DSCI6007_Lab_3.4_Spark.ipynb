{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Intro to Pair RDDs\n",
    "--------------------------\n",
    "\n",
    "Spark operations conforms to the functional programming paradigm.\n",
    "Objects (RDDs) are immutable and mapping a function to an RDD returns\n",
    "another RDD. A lot of Spark's functionalities assume the items in an\n",
    "RDD to be tuples of `(key, value)`. Structure your RDDs to be `(key,\n",
    "value)` whenever possible.\n",
    "\n",
    "Also beware of [**lazy\n",
    "evaluation**](http://en.wikipedia.org/wiki/Lazy_evaluation) where\n",
    "operations are not executed until a `.collect()`, `.first()`,\n",
    "`.take()` or `.count()` is call to retrieve items in the RDD. \n",
    "\n",
    "**So if you are doing a lot transformations in a row, call `first()`\n",
    "in between to ensure your transformations are running properly.**\n",
    "\n",
    "**If you are not sure what RDD transformations/actions there are,\n",
    "check out\n",
    "[http://spark.apache.org/docs/0.7.3/api/pyspark/pyspark.rdd.RDD-class.html](http://spark.apache.org/docs/0.7.3/api/pyspark/pyspark.rdd.RDD-class.html)**\n",
    "\n",
    "<br>\n",
    "\n",
    "1. Turn the items in `file_rdd` into `(key, value)` pairs using\n",
    "   `map()` and a `lambda` function. Map each item into    a json\n",
    "   object (use `json.loads`) and then map to the `(key, value)` pairs.\n",
    "   **Remember to cast value as type**  `int`.  Use `collect()` to see\n",
    "   your results. Using `collect()` is fine here since the data is\n",
    "   small.\n",
    "   \n",
    "   - **The key is the name of the person**\n",
    "   - **The value is how many chocolate chip cookies bought**\n",
    "    \n",
    "2. Now use `filter()` to look for entries with more than `5` chocolate\n",
    "   chip cookies.\n",
    "\n",
    "3. For each name, return the entry with the max number of cookies. \n",
    "   \n",
    "   **Hint:** \n",
    "   - Use `reduceByKey()` instead of `groupByKey()`. See why\n",
    "     [here](https://github.com/databricks/spark-knowledgebase/blob/master/best_practices/prefer_reducebykey_over_groupbykey.md)\n",
    "\n",
    "4. Calculate the total revenue from people buying cookies.\n",
    "\n",
    "   **Hint:**\n",
    "   - `rdd.values()` returns another RDD of all the values\n",
    "   - Use `reduce()` to return the sum of all the values\n",
    "\n",
    "<br>\n",
    "   \n",
    "Part 2: Starting a Local Cluster\n",
    "--------------------------------\n",
    "\n",
    "Here we will simulate starting a master/worker cluster locally. That\n",
    "allows us to develop code on a local cluster before deployment. We\n",
    "will be using [`tmux`](http://tmux.sourceforge.net/) to run our\n",
    "scripts in the background . `tmux` lets us *multiplex* your terminal,\n",
    "create terminal sessions, and attach/detach different programs in the\n",
    "terminal (somewhat like running processes in hidden terminals).\n",
    "**Below is just a quick guide to tmux for you to skim through.**\n",
    "\n",
    "<br>\n",
    "\n",
    "1. To get tmux run:\n",
    "   \n",
    "   ```bash\n",
    "   brew install tmux\n",
    "   ```\n",
    "\n",
    "2. To start a new tmux session run:\n",
    "   ```bash\n",
    "   tmux new -s [session_name]\n",
    "   ```\n",
    "\n",
    "3. To detach a tmux session use:\n",
    "   ```bash\n",
    "   ctrl+b, d\n",
    "   ```\n",
    "\n",
    "4. To get a list of your current tmux sessions run:\n",
    "   ```bash\n",
    "   tmux ls\n",
    "   ```\n",
    "\n",
    "5. To attach an existing session run:\n",
    "   ```bash\n",
    "   tmux attach -t [session_name]\n",
    "   ```\n",
    "\n",
    "<br>\n",
    "\n",
    "Now we can use `tmux` to create a local cluster (master and workers)\n",
    "which will be running in terminals in the background. \n",
    "\n",
    "<br>\n",
    "\n",
    "1. Start a tmux session which will host your master node:\n",
    "\n",
    "   ```bash\n",
    "   tmux new -s master\n",
    "   ```\n",
    "2. Run the following command to set up the Spark master to listen on local IP. The Master class in \n",
    "   `org.apache.spark.deploy.master` accepts the following parameters\n",
    "   \n",
    "   - `h` : host (which on our case is local host `127.0.0.1`) \n",
    "   - `p`: The port on which the master is listening in (`7077`)\n",
    "   - `webui-port`: The port on which the webui is reachable (`8080`)\n",
    "   \n",
    "   <br>\n",
    "\n",
    "   ```bash\n",
    "   ${SPARK_HOME}/bin/spark-class org.apache.spark.deploy.master.Master \\\n",
    "   -h 127.0.0.1 \\\n",
    "   -p 7077 \\\n",
    "   --webui-port 8080\n",
    "   ```\n",
    "3. You should get some output in your terminal similar to the following:\n",
    "   ![master_term](images/master_term.png)\n",
    "\n",
    "4. Detach from your master session(`crtl+b, d`). Start a new tmux session:\n",
    "   \n",
    "   ```bash\n",
    "   tmux new -s worker1\n",
    "   ```\n",
    "\n",
    "5. Start a worker by running the following:\n",
    "\n",
    "   ```bash\n",
    "   ${SPARK_HOME}/bin/spark-class org.apache.spark.deploy.worker.Worker \\\n",
    "   -c 1 \\\n",
    "   -m 1G \\\n",
    "   spark://127.0.0.1:7077\n",
    "   ```\n",
    "   \n",
    "   This will start a worker with 1GB memory and 1 core and attach it to the previously created Spark master. \n",
    "   The output in your terminal should be:\n",
    "   \n",
    "   ![worker_term](images/worker_term.png)\n",
    "\n",
    "   Detach the current session and create a new session and run the\n",
    "   same command to create a second worker. \n",
    "\n",
    "6. You have set up a master with 2 workers locally. Spark also\n",
    "   provides us with a web UI that lets us track the Spark jobs and see\n",
    "   other stats about any Spark related tasks and workers. \n",
    "\n",
    "   <h3 style=\"color:red\">Your web UI is at: <code>localhost:8080</code></h3>\n",
    "\n",
    "   <br>\n",
    "\n",
    "   ![sparkui_first](images/sparkui_first.png)\n",
    "\n",
    "7. We are not running any applications with our local Spark cluster\n",
    "   yet. We can attach an IPython notebook to the master and start\n",
    "   `pyspark` by running the following command which starts the\n",
    "   notebook in the browser and assigns 1G of RAM per executor and 1G\n",
    "   of RAM to the master in pyspark application.\n",
    "   \n",
    "   ```bash\n",
    "   IPYTHON_OPTS=\"notebook\"  ${SPARK_HOME}/bin/pyspark \\\n",
    "   --master spark://127.0.0.1:7077 \\\n",
    "   --executor-memory 1G \\\n",
    "   --driver-memory 1G\n",
    "   ```\n",
    "\n",
    "8. A SparkContext is already loaded into IPython. Access the\n",
    "   SparkContext with the variable `sc`.\n",
    "\n",
    "   You will see an output like below:\n",
    "\n",
    "   ```python\n",
    "   sc\n",
    "   pyspark.context.SparkContext at 0x104318250\n",
    "   ```\n",
    "\n",
    "9. Now if you refresh your spark web UI, you should see\n",
    "   **`PySparkShell`** running in the list of applications. \n",
    "   \n",
    "  ![running_application](images/running_application.png)\n",
    "\n",
    " \n",
    "<br>\n",
    "\n",
    "Part 3: Spark for Data Processing\n",
    "---------------------------------\n",
    "\n",
    "Using the cluster we have set up in the previous part, we will be\n",
    "dealing with airport data and we would want to identify airports with\n",
    "the worst / least delay.\n",
    " \n",
    "**2 types of delays:**\n",
    "\n",
    "- Arrival delays (`ARR_DELAY`) and departure delays (`DEP_DELAY`)\n",
    "- All delays are in terms of **minutes**\n",
    "- `ARR_DELAY` is associated with the destination airport (`DEST_AIRPORT_ID`)\n",
    "- `DEP_DELAY` is associated with the destination airport (`ORIGIN_AIRPORT_ID`)\n",
    "\n",
    "<br>\n",
    "\n",
    "1. Read [**sparkui.md**](sparkui.md) for further guide as to how to\n",
    "   use the UI. The guide will bring you through `2.` and `3.`.\n",
    "\n",
    "2. Load the file in as follow.\n",
    "\n",
    "   ```python\n",
    "   # DON'T INCLUDE THE '[' AND ']'\n",
    "   link = 's3n://[YOUR_AWS_ACCESS_KEY_ID]:[YOUR_AWS_SECRET_ACCESS_KEY]@mortar-example-data/airline-data'\n",
    "   airline = sc.textFile(link)\n",
    "   ```\n",
    "\n",
    "3. Note: If your Amazon AWS keys have a slash in them (like `/`) then\n",
    "   the Hadoop S3 input format will not be able to parse your URL\n",
    "   correctly. See <https://issues.apache.org/jira/browse/HADOOP-3733>\n",
    "   for more details.\n",
    "\n",
    "   If there is a `/` in your keys then regenerate the keys until you\n",
    "   get one that does not contain a `/`.\n",
    "   \n",
    "4. Print the first 2 entries. The first line is the column names and\n",
    "   starting from the second line is the corresponding data. Also run a\n",
    "   `.count()` on the RDD. This will **take a while** as the data set\n",
    "   is a few million rows. \n",
    "\n",
    "5. As you can see `.count()` takes a long time to run. It's a common\n",
    "   practice to sub-sample your data when writing your code so you\n",
    "   don't have to wait for different commands to run. You can use\n",
    "   `.take(100)` to sample out the first 100 rows and assign it to a\n",
    "   new RDD using `sc.parallelize`.\n",
    "\n",
    "6. Let's do some preprocessing. Remove the `'`, `\"` and the trailing\n",
    "   `,` for each line. Print the first 2 lines to confirm. The first 2\n",
    "   lines should look like the following.\n",
    "   \n",
    "   ```\n",
    "   YEAR,MONTH,UNIQUE_CARRIER,ORIGIN_AIRPORT_ID,DEST_AIRPORT_ID,DEP_DELAY,DEP_DELAY_NEW,ARR_DELAY,ARR_DELAY_NEW,CANCELLED\n",
    "   2012,4,AA,12478,12892,-4.00,0.00,-21.00,0.00,0.00\n",
    "   ```\n",
    "  \n",
    "7. Use `filter` to filter out the line containing the column names. \n",
    "\n",
    "8. Make a function, `make_rows()`, that takes a line as an argument and return a dictionary\n",
    "   where the keys are the column names and the values are the values for the column. \n",
    "   \n",
    "   - The output is a dictionary with only these columns:\n",
    "     `['DEST_AIRPORT_ID', 'ORIGIN_AIRPORT_ID', 'DEP_DELAY', 'ARR_DELAY']`\n",
    "   - Cast `DEP_DELAY` and `ARR_DELAY` as a float. These are minutes that are delayed.\n",
    "   - Subtract `DEP_DELAY` from `ARR_DELAY` to get the actual `ARR_DELYAY`\n",
    "   - If a flight is `CANCELLED`, add 5 hours to `DEP_DELAY`\n",
    "   - There are missing values in `DEP_DELAY` and `ARR_DELAY` (i.e. `''`) and you would want\n",
    "     to replace those with `0`.\n",
    "     \n",
    "   Map `make_rows()` to the RDD and you should have an RDD where each item is a dictionary.\n",
    "   \n",
    "9. Instead of dictionaries, make 2 RDDs where the items are tuples.\n",
    "   The first RDD will contain tuples `(DEST_AIRPORT_ID, ARR_DELAY)`.\n",
    "   The other RDD will contain `(ORIGIN_AIRPORT_ID, DEP_DELAY)`. Run a\n",
    "   `.first()` or `.take()` to confirm your results.\n",
    "\n",
    "10. Make 2 RDDs for the mean delay time for origin airports and\n",
    "    destination airports. You will need to `reduceByKey()` and then\n",
    "    take the mean of the delay times for the particular airport. \n",
    "\n",
    "11. Run `rdd.persist()` on the RDDs you made in in `8.`. Remember to\n",
    "    set the name of the RDD using `.setName()` before running\n",
    "    `persist()` (e.g. `rdd.setName('airline_rdd').persist()`). Setting\n",
    "    the name will allow you to identify the RDD in the Spark UI. That\n",
    "    will cache the RDDs so they do not need to be reproduced every\n",
    "    time they are called upon. Use `persist()` for RDDs that you are\n",
    "    going to repeatedly use.\n",
    "\n",
    "12. Use `rdd.sortBy()` to sort the RDDs by the mean delay time to\n",
    "    answer the following questions.\n",
    "\n",
    "    - Top 10 departing airport that has least avgerage delay in minutes\n",
    "    - Top 10 departing airport that has most avgerage delay in minutes\n",
    "    - Top 10 arriving airport that has least avgerage delay in minutes\n",
    "    - Top 10 arriving airport that has most avgerage delay in minutes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
