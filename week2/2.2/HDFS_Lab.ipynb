{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS Lab 1\n",
    "\n",
    "HDFS Lab 1 consists of the following tasks:\n",
    "\n",
    "- [Set up 4-node HDFS cluster on EMR](#Setup)\n",
    "- [HDFS Load Data and Run WordCount](#LoadData)\n",
    "- [Running Applications using Hortonworks Sandbox](#Application)\n",
    "\n",
    "<a name=\"Setup\"></a>\n",
    "\n",
    "## Set up 4-node HDFS cluster on EMR\n",
    "\n",
    "The first lab will be to create a 4-node cluster on Amazon's Elastic MapReduce\n",
    "environment. This assumes that you have already created an account with Amazon\n",
    "Web Services (http://aws.amazon.com/)\n",
    "\n",
    "### Amazon Key Pairs\n",
    "\n",
    "After logging into AWS, you need to create a key pair (a `pem` file)\n",
    "to log into the cluster. \n",
    "\n",
    "**NOTE:** Use your `pem` file name instead of `emr_training` as you\n",
    "follow this lab.\n",
    "\n",
    "### Create EMR Cluster\n",
    "\n",
    "Go to the main AWS page, click on the **EMR** item under the **Analytics**\n",
    "section as shown below:\n",
    "\n",
    "<img src=\"images/AWS_EMR.png\">\n",
    "\n",
    "The next step is to create an Elastic MapReduce cluster by clicking on **Create\n",
    "cluster**.\n",
    "\n",
    "<img src=\"images/Create_EMR.png\">\n",
    "\n",
    "On the **Quick cluster configuration** screen enter the following:\n",
    "\n",
    "Item                 |Value\n",
    "----                 |-----\n",
    "Cluster name         |`HDFS_Training`\n",
    "Applications         |Core Hadoop: Hadoop 2.6.0, Hive 1.0.0, and Pig 0.14.0\n",
    "Number of Instances  |`4`\n",
    "EC2 key pair         |`emr_training` (or a key you have previously created)\n",
    "\n",
    "Press **Create cluster** to continue\n",
    "\n",
    "<img src=\"images/EMR_Building.png\">\n",
    "\n",
    "The screen will refresh periodically as the cluster is being built.\n",
    "Once the cluster has been built, you should be able to ssh into the cluster.\n",
    "Click on the **SSH** link beside **Master public DNS:** as shown below:\n",
    "\n",
    "<img src=\"images/EMR_SSH_Link.png\">\n",
    "\n",
    "A screen will appear providing the details on how to connect to the cluster:\n",
    "<img src=\"images/EMR_SSH_Details.png\">\n",
    "Copy the *ssh* line as follows:\n",
    "\n",
    "    ssh hadoop@ec2-54-186-203-34.us-west-2.compute.amazonaws.com -i ~/emr_training.pem\n",
    "\n",
    "***NOTE:*** Replace the path to `emr_training.pem` with the path to your `pem` file.\n",
    "\n",
    "Here is the output you should see.\n",
    "\n",
    "    $ ssh hadoop@ec2-54-186-203-34.us-west-2.compute.amazonaws.com -i emr_training.pem\n",
    "    Last login: Tue Aug 11 18:01:49 2015\n",
    "\n",
    "           __|  __|_  )\n",
    "           _|  (     /   Amazon Linux AMI\n",
    "          ___|\\___|___|\n",
    "\n",
    "    https://aws.amazon.com/amazon-linux-ami/2015.03-release-notes/\n",
    "    14 package(s) needed for security, out of 26 available\n",
    "    Run \"sudo yum update\" to apply all updates.\n",
    "\n",
    "    EEEEEEEEEEEEEEEEEEEE MMMMMMMM           MMMMMMMM RRRRRRRRRRRRRRR\n",
    "    E::::::::::::::::::E M:::::::M         M:::::::M R::::::::::::::R\n",
    "    EE:::::EEEEEEEEE:::E M::::::::M       M::::::::M R:::::RRRRRR:::::R\n",
    "      E::::E       EEEEE M:::::::::M     M:::::::::M RR::::R      R::::R\n",
    "      E::::E             M::::::M:::M   M:::M::::::M   R:::R      R::::R\n",
    "      E:::::EEEEEEEEEE   M:::::M M:::M M:::M M:::::M   R:::RRRRRR:::::R\n",
    "      E::::::::::::::E   M:::::M  M:::M:::M  M:::::M   R:::::::::::RR\n",
    "      E:::::EEEEEEEEEE   M:::::M   M:::::M   M:::::M   R:::RRRRRR::::R\n",
    "      E::::E             M:::::M    M:::M    M:::::M   R:::R      R::::R\n",
    "      E::::E       EEEEE M:::::M     MMM     M:::::M   R:::R      R::::R\n",
    "    EE:::::EEEEEEEE::::E M:::::M             M:::::M   R:::R      R::::R\n",
    "    E::::::::::::::::::E M:::::M             M:::::M RR::::R      R::::R\n",
    "    EEEEEEEEEEEEEEEEEEEE MMMMMMM             MMMMMMM RRRRRRR      RRRRRR\n",
    "\n",
    "    [hadoop@ip-172-31-18-92 ~]$\n",
    "\n",
    "\n",
    "You have now successfully connected to your EMR cluster.\n",
    "\n",
    "We can test that we can list the files in the HDFS filesystem as follows:\n",
    "\n",
    "    [hadoop@ip-172-31-18-92 ~]$ hadoop fs -ls /\n",
    "    Found 7 items\n",
    "    drwxrwxrwx   - hdfs   hadoop          0 2015-08-11 17:22 /benchmarks\n",
    "    drwxr-xr-x   - hbase  hbase           0 2015-08-11 17:22 /hbase\n",
    "    drwxr-xr-x   - hadoop hadoop          0 2015-08-11 17:22 /mnt\n",
    "    drwxr-xr-x   - solr   solr            0 2015-08-11 17:22 /solr\n",
    "    drwxrwxrwt   - hdfs   hadoop          0 2015-08-11 17:23 /tmp\n",
    "    drwxr-xr-x   - hdfs   hadoop          0 2015-08-11 17:22 /user\n",
    "    drwxr-xr-x   - hdfs   hadoop          0 2015-08-11 17:22 /var\n",
    "    [hadoop@ip-172-31-18-92 ~]$\n",
    "\n",
    "\n",
    "<a name=\"LoadData\"></a>\n",
    "\n",
    "## HDFS Load Data and Run WordCount\n",
    "\n",
    "- Create directories\n",
    "- Upload sample file\n",
    "- Run WordCount on const.txt\n",
    "- Terminal EMR Cluster\n",
    "\n",
    "### Hadoop FS command\n",
    "\n",
    "**Step 1:** Type `hadoop fs` to get a list of all the sub-commands\n",
    "this has. \n",
    "\n",
    "**Step 2:** Type `hadoop fs -help` to get detailed information on the\n",
    "sub-commands.\n",
    "\n",
    "We are going to use the `hadoop fs` sub-commands extensively in this\n",
    "exercise.\n",
    "\n",
    "### Create directories\n",
    "\n",
    "Create a directory in HDFS called `/data/movielens`. What sub-command\n",
    "will do this for you?\n",
    "\n",
    "####Sub-command: hadoop fs -mkdir -p /data/movielens\n",
    "\n",
    "\n",
    "Check that the directory exists using `hadoop fs -ls -R /data`\n",
    " \n",
    "### Upload sample file\n",
    "\n",
    "We will download some sample data to run a WordCount test against this. We will\n",
    "download the U.S. Constitution text as the sample file to use.\n",
    "\n",
    "**Step 1:** Use `wget` to download the Constitution from\n",
    "<https://www.usconstitution.net/const.txt> on your EMR instance.\n",
    "\n",
    "**Step 2:** Run `ls -l` to verify that it is in the current directory.\n",
    "\n",
    "Note that this is not in HDFS yet. It is on your local file system.\n",
    "\n",
    "**Step 3:** Put `const.txt` into `/data/movielens/const.txt` on HDFS.\n",
    "What sub-command will do this for you?\n",
    "\n",
    "####Sub-command: hadoop fs -put const.txt /data/movielens/const.txt\n",
    "\n",
    "**Step 4:** Run `hadoop fs -ls /data/movielens` to verify that `const.txt`\n",
    "has been uploaded. It should appear in the output.\n",
    "\n",
    "### Run WordCount on const.txt\n",
    "\n",
    "**Step 1:** Run word count on this file in HDFS.\n",
    "\n",
    "    hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar \\\n",
    "        wordcount /data/movielens/const.txt wordcount_output`\n",
    "\n",
    "**Step 2:** View the output of the job in `wordcount_output` in HDFS.\n",
    "What sub-command will do this for you?\n",
    "\n",
    "####Sub-command: hadoop fs -cat /user/hadoop/wordcount_output/part-r-00000\n",
    "\n",
    "\n",
    "**Step 3:** View the first few lines of the output. What sub-command will\n",
    "do this for you?\n",
    "\n",
    "####Sub-command: hadoop fs -cat /user/hadoop/wordcount_output/part-r-00000 | head -n 3\n",
    "\n",
    "\n",
    "### Use distcp to copy directories from S3 to HDFS\n",
    "\n",
    "#### Download the dataset 'Movielens Latest'\n",
    "\n",
    "- URL [http://grouplens.org/datasets/movielens/](http://grouplens.org/datasets/m\n",
    "ovielens/)\n",
    "\n",
    "#### Upload the file to S3\n",
    "\n",
    "- Follow the instrutions under 'Create and Configure an Amazon S3\n",
    "  Bucket' at this URL:\n",
    "  <http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-plan-upload-s3.html>\n",
    "\n",
    "####Sub-command:  hadoop fs -put index.html s3://hdfs-training/index.html\n",
    "\n",
    "- Click your new bucket and upload the Movielens zip file.\n",
    "\n",
    "#### Copy file from S3 to HDFS using distcp\n",
    "\n",
    "Use the DistCp tool to copy the MovieLens file from S3 to HDFS. \n",
    "\n",
    "    hadoop distcp s3://AWS_ACCESS_KEY:AWS_SECRET_KEY@BUCKET_NAME/ml-latest.zip /training\n",
    "\n",
    "###NOTE: command ran successfually, but I can't locate 'training' folder!\n",
    "\n",
    "\n",
    "Replace the `AWS_ACCESS_KEY`, `AWS_SECRET_KEY`, and `BUCKET_NAME`\n",
    "appropriately.\n",
    "\n",
    "#### Bug: Keys Must Not Contain Forward Slash\n",
    "\n",
    "If your AWS access key or secret key has a `/` character in it\n",
    "`distcp` will not parse the s3 path correctly. \n",
    "\n",
    "The bug for this issue is still open at\n",
    "<https://issues.apache.org/jira/browse/HADOOP-3733>. \n",
    "\n",
    "The workaround is to regenerated AWS keys until you get keys without\n",
    "`/`.\n",
    "\n",
    "#### A note on distcp:\n",
    "\n",
    "\"The distcp tool is useful for quickly prepping S3 for MapReduce jobs that use\n",
    "S3 for input or for backing up the content of hdfs.\"\n",
    "\n",
    "Apache DistCp is an open-source tool you can use to copy large amounts of data.\n",
    "DistCp uses MapReduce to copy in a distributed manner\n",
    "S3DistCp is an extension of DistCp that is optimized to work with AWS,\n",
    "particularly Amazon S3.\n",
    "More info: <http://wiki.apache.org/hadoop/AmazonS3>\n",
    "\n",
    "\n",
    "### Terminate EMR Cluster\n",
    "\n",
    "After completing this exercise, go into Amazon AWS and click on **Terminate** to\n",
    "remove the cluster we have just built.\n",
    "\n",
    "<a name=\"Application\"></a>\n",
    "\n",
    "## Running Applications using Hortonworks Sandbox\n",
    "\n",
    "- Why use the sandbox instead of EMR\n",
    "- Downloading and Installing Hortonworks Sandbox\n",
    "- Use hadoop examples jar's grep, word-count, and wordmean\n",
    "- Use use these to split movie reviews into positive and negative\n",
    "- Run word count on the two sets of files\n",
    "- See what words are most common in each set\n",
    "\n",
    "### Why use the sandbox instead of EMR\n",
    "\n",
    "Q: What is the Hortonworks Sandbox?\n",
    "\n",
    "- Instead of running Hadoop on a cluster the Hortonworks Sandbox lets\n",
    "  you run it locally in a virtual machine.\n",
    "\n",
    "Q: Why would I need the sandbox? \n",
    "\n",
    "- While in production you will run a cluster on Amazon, for\n",
    "  development and testing it is a lot easier to spin up a one-machine\n",
    "  cluster locally. \n",
    "\n",
    "- It is a great training and learning environment for experimenting\n",
    "  with new technologies.\n",
    "\n",
    "- It is a great development environment where you can use small data\n",
    "  sets to verify that your application logic is correct.\n",
    "\n",
    "- Finally once everything is tested you can run your analysis on real\n",
    "  data on EMR.\n",
    "  \n",
    "- Also this is (a lot) cheaper.\n",
    "\n",
    "### Downloading and Installing Hortonworks Sandbox\n",
    "\n",
    "The remainder of this lab will use a local copy of the Hortonworks Sandbox.\n",
    "Please ensure you have **terminated** the EMR Cluster in Amazon AWS prior to\n",
    "continuing this lab.\n",
    "\n",
    "Go to the Hortonworks Sandbox download page - [http://hortonworks.com/products\n",
    "/hortonworks-sandbox/#install](http://hortonworks.com/products/hortonworks-\n",
    "sandbox/#install)\n",
    "\n",
    "Download the latest version. This assumes that you have a local environment for\n",
    "VirtualBox (free download from\n",
    "[https://www.virtualbox.org/](https://www.virtualbox.org/)), VMware (Workstation\n",
    "or Fusion), or Hyper-V to run the Virtual Machine. Download the appropriate VM\n",
    "for your local environment (which is about 7GB). Instructions for importing the\n",
    "Virtual Appliance are available on the Hortonworks Sandbox page. Once the\n",
    "Sandbox has started, the screen should look like the following.\n",
    "\n",
    "<img src=\"images/Sandbox.png\">\n",
    "\n",
    "Login to the Sandbox using the instructions for ssh on the screen.\n",
    "\n",
    "### Use hadoop examples jar's grep,  word-count, and wordmean\n",
    "\n",
    "**Step 1:** Download and uncompress Shakespeare's works.\n",
    "\n",
    "Download the dataset 'Complete Works of William Shakespeare' from\n",
    "<http://www.it.usyd.edu.au/~matty/Shakespeare/shakespeare.tar.gz> in\n",
    "the sandbox instance.\n",
    "\n",
    "    [root@sandbox ~]# wget http://www.it.usyd.edu.au/~matty/Shakespeare/shakespeare.tar.gz\n",
    "\n",
    "Unzip the William Shakespeare data\n",
    "\n",
    "    mkdir shakespeare\n",
    "    cd shakespeare\n",
    "    tar xvfz ../shakespeare.tar.gz\n",
    "\n",
    "Make sure you have all the files.\n",
    "\n",
    "    ls -l\n",
    "\n",
    "**Step 2:** Create a direcgtory in HDFS called `/data/shakespeare`\n",
    "\n",
    "####Sub-command: hadoop fs -mkdir -p /data/shakespeare\n",
    "\n",
    "**Step 3:** Verify using `hadoop fs -ls -R /data/shakespeare` that your upload worked.\n",
    "\n",
    "**Note:** For the next three exercises we will be running the grep,\n",
    "wordcount, and wordmean from the *hadoop-mapreduce-examples.jar* file.\n",
    "This is located in the **/usr/hdp/<version>/hadoop-mapreduce**\n",
    "directory. Please replace the `current` in the paths below with the\n",
    "current Sandbox version you are running with.\n",
    "\n",
    "#### Grep\n",
    "\n",
    "Use this command to find the number of times that `the` occurs in\n",
    "`/data/shakespeare/comedies`.\n",
    "\n",
    "    yarn jar /usr/hdp/current/hadoop-mapreduce-client \\\n",
    "        /hadoop-mapreduce-examples.jar grep /data/shakespeare/comedies /data/grep_out \\\n",
    "        the\n",
    "\n",
    "Look at the contents of `/data/grep_out` in HDFS to find out how many\n",
    "times `the` occurred.\n",
    "\n",
    "#### Exploring Examples\n",
    "\n",
    "Run this command to get a list of all the sub-programs under\n",
    "`hadoop-mapreduce-examples.jar`\n",
    "\n",
    "    yarn jar /usr/hdp/current/hadoop-mapreduce-client \\\n",
    "        /hadoop-mapreduce-examples.jar\n",
    "\n",
    "#### Word-Count\n",
    "\n",
    "Run `wordcount` against `/data/shakespeare/comedies`. This calculates\n",
    "the frequency of all the words.\n",
    "\n",
    "Look at the output directory to verify that the command worked and\n",
    "produced results.\n",
    "\n",
    "#### Wordmean\n",
    "\n",
    "Run `wordmean` against `/data/shakespeare/comedies`. This calculates\n",
    "the average word length across all the words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
