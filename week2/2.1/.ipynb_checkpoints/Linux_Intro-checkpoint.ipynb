{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linux Intro\n",
    "===========\n",
    "\n",
    "Objectives\n",
    "----------\n",
    "\n",
    "By the end of this morning, you will be able to:\n",
    "\n",
    "- `ssh` into a remote Linux server\n",
    "\n",
    "- Examine log files using `head` and `vi`\n",
    "\n",
    "- Create a `pipe` for interprocess communication\n",
    "\n",
    "- Find patterns using `grep`\n",
    "\n",
    "- `cut` out selected portions of each line of a file\n",
    "\n",
    "- `sort` lines of text files\n",
    "\n",
    "- report or filter out repeated lines in a file using `uniq`\n",
    "\n",
    "Connecting to a remote computer\n",
    "-------------------------------\n",
    "\n",
    "#### ~~`telnet`~~  \n",
    "\n",
    "#### `ssh`\n",
    "\n",
    "SSH\n",
    "---\n",
    "\n",
    "> Secure Shell, or SSH, is a cryptographic (encrypted) network\n",
    "> protocol for initiating text-based shell sessions on remote machines\n",
    "> in a secure way.\n",
    " \n",
    "![Ssh_binary_packet](https://upload.wikimedia.org/wikipedia/commons/0/0f/Ssh_binary_packet_alt.svg)\n",
    "\n",
    "\n",
    "    ssh -i ~/.ssh/galvanize-DEI.pem ec2-user@ec2-52-27-60-213.us-west-2.compute.amazonaws.com ls\n",
    "\n",
    "Unix Shell Walkthrough\n",
    "----------------------\n",
    "\n",
    "#### Grab `shakespeare-sonnets.txt`.\n",
    "\n",
    "    curl -LO http://dsci6007.s3.amazonaws.com/data/shakespeare-sonnets.txt\n",
    "\n",
    "#### Let's look at the first few lines.\n",
    "    \n",
    "    head shakespeare-sonnets.txt \n",
    "\n",
    "#### Let's skip the first two lines:\n",
    "\n",
    "    tail -n +3 shakespeare-sonnets.txt | head\n",
    "    \n",
    "####NOTE: tail -n +3 shakespeare-sonnets.txt\n",
    "\n",
    "    skips first 2 lines and displays the remainder of the file\n",
    "    as the tail end, which in this case is nearly the entire file\n",
    "    \n",
    "    This command is desirable because it filters out the title \n",
    "    and the name of the author, leave behind the actual sonnets\n",
    "\n",
    "#### Let's cut out the chapter : verse part:\n",
    "\n",
    "    tail -n +3 shakespeare-sonnets.txt | cut -d' ' -f2- | head\n",
    "    \n",
    "####NOTE:    cut -d' ' -f2- \n",
    "    \n",
    "    From input remove field 1 with space as field separator.\n",
    "    Here, 'field 1' is the first word on every line.\n",
    "\n",
    "#### Let's translate the characters to lower case:\n",
    "\n",
    "    tail -n +3 shakespeare-sonnets.txt | cut -d' ' -f2- | tr 'A-Z' 'a-z' | head\n",
    "\n",
    "####NOTE:  tr 'A-Z' 'a-z' \n",
    "\n",
    "    Replace A-Z with a-z (lowercase the words)\n",
    "\n",
    "\n",
    "#### Let's tokenize our words:\n",
    "\n",
    "\n",
    "    tail -n +3 shakespeare-sonnets.txt | \n",
    "        cut -d' ' -f2- | \n",
    "        tr 'A-Z' 'a-z' | \n",
    "        tr -cs 'a-z' '\\012' | \n",
    "        head\n",
    "\n",
    "####NOTE: tr -cs 'a-z' '\\012'\t\n",
    "\n",
    "    Replace sequences of non-a-z with newlines (split lines into words)\n",
    "    Spaces, commas, periods, etc are indicators that a string is a word\n",
    "    ALSO commands '-cs' and '-sc' are equivalent \n",
    "    \n",
    "#### Let's sort them:\n",
    "\n",
    "\n",
    "    tail -n +3 shakespeare-sonnets.txt |\n",
    "        cut -d' ' -f2- |\n",
    "        tr 'A-Z' 'a-z' |\n",
    "        tr -sc 'a-z' '\\012' |\n",
    "        sort |\n",
    "        head\n",
    "\n",
    "####NOTE:  sort, no head\n",
    "    \n",
    "    Returns a long list of sorted tokens\n",
    "    \n",
    "####NOTE: sort, with head\n",
    "\n",
    "    Returns a list of the first 10 sorted tokens\n",
    "\n",
    "\n",
    "#### What is our vocabulary?\n",
    "\n",
    "    tail -n +3 shakespeare-sonnets.txt |\n",
    "        cut -d' ' -f2- |\n",
    "        tr 'A-Z' 'a-z' |\n",
    "        tr -sc 'a-z' '\\012' |\n",
    "        sort |\n",
    "        uniq |\n",
    "        head\n",
    "        \n",
    "####NOTE: uniq\n",
    "    \n",
    "    Filters list by unique tokens\n",
    "    \n",
    "\n",
    "#### How big is our vocabulary?\n",
    "\n",
    "    tail -n +3 shakespeare-sonnets.txt | \n",
    "        cut -d' ' -f2- | \n",
    "        tr 'A-Z' 'a-z' | \n",
    "        tr -sc 'a-z' '\\012' | \n",
    "        sort | \n",
    "        uniq | \n",
    "        wc -w\n",
    "\n",
    "####NOTE: wc -w\n",
    "    \n",
    "    $\\textbf{wc}$ Count words, lines, characters\n",
    "    $\\textbf{-w}$ Specifies words\n",
    "\n",
    "#### How many times does each word occur?\n",
    "\n",
    "    tail -n +3 shakespeare-sonnets.txt | cut -d' ' -f2- | \n",
    "        tr 'A-Z' 'a-z' | tr -sc 'a-z' '\\012' | \n",
    "        sort | uniq -c | head\n",
    "\n",
    "#### How might we construct a rhyming dictionary?\n",
    "\n",
    "    tail -n +3 shakespeare-sonnets.txt | cut -d' ' -f2- | \n",
    "        tr 'A-Z' 'a-z' | tr -sc 'a-z' '\\012' | \n",
    "        sort | uniq | rev | sort | rev | head\n",
    "\n",
    "####NOTE: sort | uniq | rev \n",
    "\n",
    "    Sorts unique tokens then reverse them\n",
    "\n",
    "####NOTE: sort | uniq | rev | sort \n",
    "    \n",
    "    Takes reversed tokens (backward words) and sorts them.\n",
    "    This has the effect of ordering the end of words\n",
    "    by the same letters:\n",
    "    \n",
    "    (i.e. words that end with the same letters tend to rhyme!)\n",
    "\n",
    "\n",
    "#### What was the penultimate word of each sentence?\n",
    "\n",
    "    tail -n +3 shakespeare-sonnets.txt | cut -d' ' -f2- | awk '{print   $(NF-1)}' | head\n",
    "    \n",
    "####NOTES:  NF    \n",
    "\n",
    "    number of fields in the current record\n",
    "\n",
    "#### NOTES: awk '{print   $(NF-1)}' THROWS ERROR\n",
    "\n",
    "    awk: trying to access out of range field -1\n",
    "    \n",
    "    May be because the romen numbers exist in a line with one token\n",
    "    This command assumes that there are at least two tokens per line.\n",
    "    \n",
    "    commands with awk '{print   $(NF-1)}' will not work until I filter\n",
    "    out all the roman numerals (NOT PART OF THE EXERCISE, BUT I SHOULD DO IT ANYWAYS!!!!)\n",
    "\n",
    "\n",
    "#### What was the antepenultimate word of each sentence?\n",
    "\n",
    "    tail -n +3 shakespeare-sonnets.txt | cut -d' ' -f2- | awk '{print $(NF-2)}' | head\n",
    "    \n",
    "#### NOTES: THROWS ERROR\n",
    "\n",
    "    awk: trying to access out of range field -2\n",
    "    \n",
    "    May be because the romen numbers exist in a line with one token\n",
    "    This command assumes that there are at least two tokens per line.\n",
    "\n",
    "\n",
    "#### What's the word count of those words?\n",
    "\n",
    "    tail -n +3 shakespeare-sonnets.txt |\n",
    "        cut -d' ' -f2- |\n",
    "        awk '{print $(NF-2)}' |\n",
    "        sort |\n",
    "        uniq -c |\n",
    "        head\n",
    "\n",
    "#### Let's delete punctuation:\n",
    "\n",
    "    tail -n +3 shakespeare-sonnets.txt |\n",
    "        cut -d' ' -f2- |\n",
    "        awk '{print $(NF-2)}' |\n",
    "        sort |\n",
    "        tr -d '[:punct:]' |\n",
    "        uniq -c |\n",
    "        head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing Web Log Files\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) \n",
    "Look at the file `data/NASA_access_log_Jul95.gz` without\n",
    "saving the uncompressed version using `gunzip -c`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gzip -cd  NASA_access_log_Jul95.gz | head\n",
    "\n",
    "$\\textbf{gzip}$ is used to minipulate .gz files \n",
    "\n",
    "$\\textbf{c}$ writes output on standard output\n",
    "\n",
    "$\\textbf{d}$ decompress file\n",
    "\n",
    "#### Note: \n",
    "\n",
    "    if 'd' is used without 'c' then the .gz file is decompressed \n",
    "    \n",
    "    if 'd' is used with 'c' then the decompressed content is display\n",
    "    however, the original .gz file is not decompressed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Find the total number lines in the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gzip -cd  NASA_access_log_Jul95.gz | wc -l\n",
    "\n",
    "There are 1891714 lines in this document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Using `gunzip -c` find the total number of 400 errors in the file.\n",
    "This includes errors such as 401, 404, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### gzip -cd  NASA_access_log_Jul95.gz | awk '{print $(NF-1)}' | grep '^4' |sort |uniq -c\n",
    " \n",
    "        5 400\n",
    "       54 403\n",
    "    10845 404"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Find the total number of 500 errors in the file. Again include all\n",
    "errors from 500-599."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gzip -cd  NASA_access_log_Jul95.gz | awk '{print $(NF-1)}' | grep '^5' |sort |uniq -c\n",
    "\n",
    "      62 500\n",
    "      14 501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Find the total count of all the different status codes in the\n",
    "`NASA_access_log_Jul95.gz` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gzip -cd  NASA_access_log_Jul95.gz | awk '{print $(NF-1)}'|sort |uniq -c\n",
    "\n",
    "    1701534 200\n",
    "      46573 302\n",
    "     132627 304\n",
    "          5 400\n",
    "         54 403\n",
    "      10845 404\n",
    "         62 500\n",
    "         14 501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note\n",
    "----\n",
    "\n",
    "- Using `export LC_CTYPE=C` sets the locale to the default locale\n",
    "  instead of ASCII.\n",
    "\n",
    "- Setting this enables commands like `rev` and `tr` to not produce\n",
    "  `Illegal byte sequence` warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python2]",
   "language": "python",
   "name": "conda-env-python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
