{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Formats\n",
    "============\n",
    "\n",
    "Why Data Formats\n",
    "----------------\n",
    "\n",
    "I want to store my data in a more compact binary form and not as text.\n",
    "How can I convert my sales table into Avro?\n",
    "\n",
    "- A common use case for Hive is to convert data between different\n",
    "  formats.\n",
    "\n",
    "- A data format or storage format is a part of the file's metadata.\n",
    "\n",
    "<details><summary>\n",
    "What command can we use to define the data format for a table?\n",
    "</summary>\n",
    "`CREATE TABLE`\n",
    "</details>\n",
    "\n",
    "Defining Table Data Formats\n",
    "---------------------------\n",
    "\n",
    "How can I convert the sales table to Avro?\n",
    "\n",
    "```sql\n",
    "-- Drop table if it exists.\n",
    "DROP TABLE IF EXISTS sales_avro;\n",
    "\n",
    "-- Create external table.\n",
    "CREATE TABLE sales_avro(\n",
    "  id INT,\n",
    "  sale_date STRING,\n",
    "  store INT,\n",
    "  state STRING,\n",
    "  product INT,\n",
    "  amount DOUBLE\n",
    ")\n",
    "STORED AS AVRO;\n",
    "\n",
    "-- Insert data.\n",
    "INSERT OVERWRITE TABLE sales_avro\n",
    "SELECT * \n",
    "FROM sales;\n",
    "\n",
    "-- Select table to test.\n",
    "SELECT * FROM sales_avro;\n",
    "```\n",
    "\n",
    "Storage Formats\n",
    "---------------\n",
    "\n",
    "What storage formats can I define for my tables?\n",
    "\n",
    "In `CREATE TABLE` or `CREATE EXTERNAL TABLE` you can specify the\n",
    "format for your data.\n",
    "\n",
    "Storage Format             |Meaning\n",
    "--------------             |-------\n",
    "`STORED AS TEXTFILE`       |Stored as text (default)\n",
    "`STORED AS PARQUET`        |Stored as Parquet\n",
    "`STORED AS AVRO`           |Stored as Avro\n",
    "`STORED AS SEQUENCEFILE`   |Stored as SequenceFile\n",
    "`STORED AS ORC`            |Stored as ORC\n",
    "\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "What is an easy way to convert CSV files to Parquet?\n",
    "</summary>\n",
    "1. Define table 1 using `STORED AS TEXTFILE`.<br>\n",
    "2. Define table 2 using `STORED AS PARQUET`.<br>\n",
    "3. Load the data into table 1.<br>\n",
    "4. Insert/select the data from table 1 to table 2.<br>\n",
    "5. Hive is commonly used for data conversion.<br>\n",
    "</details>\n",
    "\n",
    "\n",
    "Text vs Binary\n",
    "--------------\n",
    "\n",
    "Should I use a text or a binary format?\n",
    "\n",
    "- Text is human readable but inefficient at scale.\n",
    "\n",
    "Avro vs Parquet vs ORC\n",
    "----------------------\n",
    "\n",
    "Which binary format should I use?\n",
    "\n",
    "- The optimal data format depends on nature of your data.\n",
    "\n",
    "- Parquet and ORC (Optimized Row Columnar) are columnar, and are good\n",
    "  for \"narrow\" data.\n",
    "\n",
    "- Narrow means the data has few columns and the rows are repetitive. \n",
    "\n",
    "- Avro is non-columnar and is good for unstructured \"wide\" data.\n",
    "\n",
    "- Wide means the data has lots of columns and each row is different.\n",
    "\n",
    "Parquet vs ORC\n",
    "--------------\n",
    "\n",
    "Between ORC and Parquet which one should I use?\n",
    "\n",
    "- ORC and Parquet are competing industry standards. Like Betamax and\n",
    "  VHS.\n",
    "\n",
    "- Parquet is more popular overall.\n",
    "\n",
    "- According to [research][ibm-parquet] from IBM in September 2014 ORC\n",
    "  uses less storage but Parquet has faster query times.\n",
    "\n",
    "[ibm-parquet]: https://developer.ibm.com/hadoop/blog/2014/09/19/big-sql-3-0-file-formats-usage-performance/\n",
    "\n",
    "![](images/ibm-data-format-comparison.jpg)\n",
    "\n",
    "\n",
    "RC File\n",
    "-------\n",
    "\n",
    "What is the RC File format?\n",
    "\n",
    "- RC (Record Columnar) is an older version of ORC (Optimized Row Columnar).\n",
    "\n",
    "SequenceFiles\n",
    "-------------\n",
    "\n",
    "When should I use SequenceFiles?\n",
    "\n",
    "- SequenceFiles are a binary format used by MapReduce that stores data\n",
    "  as key-value pairs.\n",
    "\n",
    "- Useful for storing intermediate data between a chain of MapReduce\n",
    "  jobs.\n",
    "\n",
    "- Easy to use in programs.\n",
    "\n",
    "- Unlike other binary formats, SequenceFiles only work with Java.\n",
    "\n",
    "SerDes\n",
    "------\n",
    "\n",
    "What is a `SerDe`?\n",
    "\n",
    "- `SerDe` is a short name for *Serializer and Deserializer*.\n",
    "\n",
    "- Using these Hive can read and write to any data source can read and\n",
    "  write a sequence of records.\n",
    "\n",
    "- There are many 3rd party SerDes such as Amazon's JSON SerDe.\n",
    "\n",
    "- Using Hive's developer API you can write your own custom SerDes.\n",
    "\n",
    "Joins\n",
    "=====\n",
    "\n",
    "Joining Tables\n",
    "--------------\n",
    "\n",
    "How can expand the state IDs to get full state names?\n",
    "\n",
    "### Bash: Upload data to HDFS\n",
    "\n",
    "```sh\n",
    "# Create states.csv.\n",
    "cat <<'END_OF_DATA' > states.csv\n",
    "#State,Name\n",
    "CA,California\n",
    "WA,Washington\n",
    "NV,Nevada\n",
    "END_OF_DATA\n",
    "\n",
    "# Upload it to HDFS.\n",
    "hadoop fs -rm -r /user/root/states\n",
    "hadoop fs -mkdir /user/root/states\n",
    "hadoop fs -put   states.csv /user/root/states/part1.csv\n",
    "\n",
    "# Test it was uploaded to HDFS.\n",
    "hadoop fs -ls -R /user/root/states\n",
    "hadoop fs -cat \"/user/root/states/*\"\n",
    "```\n",
    "\n",
    "### Hive: Create Hive table on data\n",
    "\n",
    "```sql\n",
    "-- Drop table if it exists.\n",
    "DROP TABLE IF EXISTS  states;\n",
    "\n",
    "-- Create external table.\n",
    "CREATE EXTERNAL TABLE states(\n",
    "  state STRING,\n",
    "  name STRING\n",
    ")\n",
    "ROW FORMAT DELIMITED \n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/user/root/states'\n",
    "TBLPROPERTIES(\"skip.header.line.count\"=\"1\");\n",
    "\n",
    "-- Select table to test.\n",
    "SELECT * FROM states;\n",
    "```\n",
    "\n",
    "Joining Tables\n",
    "--------------\n",
    "\n",
    "Join the sales and states tables.\n",
    "\n",
    "### Hive: Join tables\n",
    "\n",
    "```sql\n",
    "SELECT * FROM states JOIN sales ON sales.state = states.state;\n",
    "```\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "By default did `JOIN` do an inner join or an outer join?\n",
    "</summary>\n",
    "1. This was an inner join.<br>\n",
    "2. Notice that Nevada was not in the output.<br>\n",
    "3. Also Oregon was not in the output.<br>\n",
    "4. Why were these two states missing?<br>\n",
    "</details>\n",
    "\n",
    "Join Details\n",
    "------------\n",
    "\n",
    "Why are joins important?\n",
    "\n",
    "- Joins are indispensible for most interesting data analysis.\n",
    "\n",
    "- Incoming data has references to lookup tables.\n",
    "\n",
    "- Lookup tables map keys to detailed information.\n",
    "\n",
    "- To denormalize the data you join the incoming data with the lookup tables on the keys.\n",
    "\n",
    "Star Schema\n",
    "-----------\n",
    "\n",
    "- Star schema separates business process data into fact tables and\n",
    "  dimension tables.\n",
    "\n",
    "- Facts are measurable, quantitative data about a business For\n",
    "  example, sales transactions. \n",
    " \n",
    "- Dimensions are descriptive attributes related to fact data. For\n",
    "  example, product models, colors, sizes, and salesperson names.\n",
    "\n",
    "- Fact tables are connected to dimension tables through foreign keys.\n",
    "\n",
    "Pop Quiz: Star Schema\n",
    "---------------------\n",
    "\n",
    "<details><summary>\n",
    "How can we combine fact tables with dimension tables?\n",
    "</summary>\n",
    "Using join.<br>\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Between `sales` and `states` which is the fact table and which is the dimension table? \n",
    "</summary>\n",
    "1. `sales` is the fact table.<br>\n",
    "2. `states` is the dimension table.<br>\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Between fact and dimension tables, which is likely to be larger?\n",
    "</summary>\n",
    "1. Fact tables are larger and have more churn.<br>\n",
    "2. Dimension tables are smaller and are more stable.<br>\n",
    "3. `sales` will be larger than `states`.<br>\n",
    "</details>\n",
    "\n",
    "Join Optimization\n",
    "-----------------\n",
    "\n",
    "My joins are taking too long. What can I do?\n",
    "\n",
    "- The simplest heuristic is to put the largest table last.\n",
    "\n",
    "- We will talk about other techniques in the next lecture.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Which order should I join `sales` and `states`?\n",
    "</summary>\n",
    "1. `states` should be first.<br>\n",
    "2. `sales` should be last.<br>\n",
    "3. Mneumonic: *LTL* (Largest Table Last).<br>\n",
    "</details>\n",
    "\n",
    "Join Types\n",
    "----------\n",
    "\n",
    "What are the different types of join in Hive?\n",
    "\n",
    "Join Type              |Includes Rows That\n",
    "---------              |------------------\n",
    "`JOIN`                 |Matches found in both table\n",
    "`LEFT OUTER JOIN`      |Match in left table but no match in right\n",
    "`RIGHT OUTER JOIN`     |Match in right table but no match in left\n",
    "`FULL OUTER JOIN`      |Match in either left or right table \n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "Consider this query:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM states JOIN sales ON sales.state = states.state\n",
    "```\n",
    "\n",
    "<details><summary>\n",
    "Which join should I use to get sales transactions with invalid states in this\n",
    "query? \n",
    "</summary>\n",
    "`RIGHT OUTER JOIN`\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Which join should I use to include states with no sales transactions in this\n",
    "query? \n",
    "</summary>\n",
    "`LEFT OUTER JOIN`\n",
    "</details>\n",
    "\n",
    "\n",
    "Storage Layout\n",
    "==============\n",
    "\n",
    "Optimizing Storage Layout\n",
    "-------------------------\n",
    "\n",
    "By default Hive does full table scans on queries.\n",
    "\n",
    "Is it possible to do better than this?\n",
    "\n",
    "- If the data is laid out in a way so we know which HDFS file or\n",
    "  directory to scan we can avoid full-table scans.\n",
    "\n",
    "- *Partitioned* and *Bucketed* tables are some ways of laying out HDFS\n",
    "  directories and files to speed up queries.\n",
    "\n",
    "\n",
    "Analogy\n",
    "-------\n",
    "\n",
    "What are some ways of keeping your socks organized?\n",
    "\n",
    "- There is a tradeoff between storage and retrieval optimization.\n",
    "\n",
    "\n",
    "Hive Default\n",
    "------------\n",
    "\n",
    "<img src=\"images/hive-socks-default.jpg\">\n",
    "\n",
    "Solution 1: Dump all sock in big drawer.\n",
    "\n",
    "- This is Hive's default behavior.\n",
    "\n",
    "- Optimizes storage, but not retrieval.\n",
    "\n",
    "Partitioning\n",
    "------------\n",
    "\n",
    "<img src=\"images/hive-socks-partitioned.jpg\">\n",
    "\n",
    "\n",
    "Solution 2: Have separate bin for each color.\n",
    "\n",
    "- This is partitioning.\n",
    "\n",
    "- Optimizes retrieval, but not storage.\n",
    "\n",
    "Bucketing\n",
    "---------\n",
    "\n",
    "<img src=\"images/hive-socks-bucketed.jpg\">\n",
    "\n",
    "Solution 3: Have 2 bins. Black and white go in right bin. Other colors\n",
    "go in left bin.\n",
    "\n",
    "- This is bucketing.\n",
    "\n",
    "- Balances both storage and retrieval.\n",
    "\n",
    "Partitioned Tables\n",
    "==================\n",
    "\n",
    "Why Partitioned Tables\n",
    "----------------------\n",
    "\n",
    "<details><summary>\n",
    "Suppose we always query the data with a `state` value in the `WHERE`\n",
    "clause. How can we do better than scanning the full table (i.e.\n",
    "scanning all the HDFS files under `sales`) for each query?\n",
    "</summary>\n",
    "1. If we store the records in separate sub-directories, one per state.<br>\n",
    "2. Then if the query was for `'CA'` we could just scan the directory for `'CA'`.<br>\n",
    "3. This is called *partitioning*.<br>\n",
    "</details>\n",
    "\n",
    "Partitioning Example\n",
    "--------------------\n",
    "\n",
    "Suppose we partition this table based on state and save it into\n",
    "`sales_part`.\n",
    "\n",
    "ID   |Date        |Store  |State  |Product  |Amount\n",
    "--   |----        |-----  |-----  |-------  |------\n",
    "101  |2014-11-13  |100    |WA     |331      |300.00\n",
    "104  |2014-11-18  |700    |OR     |329      |450.00\n",
    "102  |2014-11-15  |203    |CA     |321      |200.00\n",
    "106  |2014-11-19  |202    |CA     |331      |330.00\n",
    "103  |2014-11-17  |101    |WA     |373      |750.00\n",
    "105  |2014-11-19  |202    |CA     |321      |200.00\n",
    "\n",
    "\n",
    "<details><summary>\n",
    "How many sub-directories will be created under `/app/hive/warehouse/sales_part`?\n",
    "</summary>\n",
    "3 sub-directories:\n",
    "`/apps/hive/warehouse/sales_part/state=CA/`<br>\n",
    "`/apps/hive/warehouse/sales_part/state=OR/`<br>\n",
    "`/apps/hive/warehouse/sales_part/state=WA/`<br>\n",
    "</details>\n",
    "\n",
    "Partitioning Pros and Cons\n",
    "--------------------------\n",
    "\n",
    "What are the pros and cons of partitioning?\n",
    "\n",
    "- Pro: It speeds up queries as long as they meet our constraints.\n",
    "\n",
    "- Con: It slows down queries that do not fix the partitioned column\n",
    "  (`state` in this case).\n",
    "\n",
    "- It imposes a structure on the data. It optimizes specific queries at\n",
    "  the expense of ad hoc queries.\n",
    "\n",
    "Ingesting Into Partitioned Tables\n",
    "---------------------------------\n",
    "\n",
    "- Hive does not touch the directory structure of external tables.\n",
    "\n",
    "- Therefore partitioned tables have to be internal.\n",
    "\n",
    "- The steps are: (1) load data into HDFS, (2) insert-select into a\n",
    "  partitioned table.\n",
    "\n",
    "Ingesting Into Partitioned Tables\n",
    "---------------------------------\n",
    "\n",
    "### Bash: Upload data to HDFS\n",
    "\n",
    "```sh\n",
    "# Create sales.csv.\n",
    "cat <<'END_OF_DATA' > sales.csv\n",
    "#ID,Date,Store,State,Product,Amount\n",
    "101,2014-11-13,100,WA,331,300.00\n",
    "104,2014-11-18,700,OR,329,450.00\n",
    "102,2014-11-15,203,CA,321,200.00\n",
    "106,2014-11-19,202,CA,331,330.00\n",
    "103,2014-11-17,101,WA,373,750.00\n",
    "105,2014-11-19,202,CA,321,200.00\n",
    "END_OF_DATA\n",
    "\n",
    "# Upload it to HDFS.\n",
    "hadoop fs -rm -r /user/root/sales\n",
    "hadoop fs -mkdir /user/root/sales\n",
    "hadoop fs -put   sales.csv /user/root/sales/part1.csv\n",
    "\n",
    "# Test it was uploaded to HDFS.\n",
    "hadoop fs -ls -R /user/root/sales\n",
    "hadoop fs -cat \"/user/root/sales/*\"\n",
    "```\n",
    "\n",
    "### Hive: Create partitioned table\n",
    "\n",
    "```sql\n",
    "-- Create external table.\n",
    "DROP TABLE IF EXISTS  sales;\n",
    "CREATE EXTERNAL TABLE sales(\n",
    "  id INT,\n",
    "  sale_date STRING,\n",
    "  store INT,\n",
    "  state STRING,\n",
    "  product INT,\n",
    "  amount DOUBLE\n",
    ")\n",
    "ROW FORMAT DELIMITED \n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/user/root/sales'\n",
    "TBLPROPERTIES(\"skip.header.line.count\"=\"1\");\n",
    "\n",
    "-- Create partitioned table.\n",
    "DROP TABLE IF EXISTS  sales_part;\n",
    "CREATE TABLE sales_part(\n",
    "  id INT,\n",
    "  sale_date STRING,\n",
    "  store INT,\n",
    "  product INT,\n",
    "  amount DOUBLE\n",
    ")\n",
    "PARTITIONED BY (state STRING)\n",
    "ROW FORMAT DELIMITED \n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE;\n",
    "\n",
    "-- Insert select into partitioned table.\n",
    "FROM sales \n",
    "INSERT INTO TABLE sales_part PARTITION(state)\n",
    "SELECT id,sale_date,store,product,amount,state;\n",
    "\n",
    "-- Select table to test.\n",
    "SELECT * FROM sales_part;\n",
    "```\n",
    "\n",
    "Notes\n",
    "-----\n",
    "\n",
    "- Instead of `INSERT INTO` you can use `INSERT OVERWRITE` to overwrite\n",
    "  the table.\n",
    "\n",
    "- You have to partition on the last columns.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "How many directories do you expect to see in HDFS under `sales_part`?\n",
    "</summary>\n",
    "1. The partition count will equal the number of states which is 3.<br>\n",
    "2. So there should be 3 directories, one per state.<br>\n",
    "</details>\n",
    "\n",
    "Lets check.\n",
    "\n",
    "### Bash: Verify data split between 4 files\n",
    "\n",
    "```bash\n",
    "hadoop fs -ls -R /apps/hive/warehouse/sales_part\n",
    "```\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Does `FROM sales INSERT INTO TABLE sales_part ...` delete the data\n",
    "from `sales`?\n",
    "</summary>\n",
    "1. It does not.<br>\n",
    "2. Only `LOAD DATA` removes the source HDFS file.<br>\n",
    "3. Insert-select does not remove the source.<br>\n",
    "</details>\n",
    "\n",
    "\n",
    "Bucketed Tables\n",
    "===============\n",
    "\n",
    "Bucketing Intro\n",
    "---------------\n",
    "\n",
    "Partitioning is one way to avoid full table scans. Another is\n",
    "*bucketing*.\n",
    "\n",
    "Instead of putting rows with a specific key in a separate directory,\n",
    "rows whose keys compute the same hash value bucketed into the same\n",
    "file.\n",
    "\n",
    "Bucketing\n",
    "---------\n",
    "\n",
    "What is bucketing?\n",
    "\n",
    "- For each row Hive calculates a hash value based on the bucketed\n",
    "  columns.\n",
    "\n",
    "- All rows with the same hash value are written to the same file.\n",
    "\n",
    "Bucketing Example\n",
    "-----------------\n",
    "\n",
    "Suppose we split this table into 4 buckets by Store.\n",
    "\n",
    "ID   |Date        |Store  |State  |Product  |Amount\n",
    "--   |----        |-----  |-----  |-------  |------\n",
    "101  |2014-11-13  |100    |WA     |331      |300.00\n",
    "104  |2014-11-18  |700    |OR     |329      |450.00\n",
    "102  |2014-11-15  |203    |CA     |321      |200.00\n",
    "106  |2014-11-19  |202    |CA     |331      |330.00\n",
    "103  |2014-11-17  |101    |WA     |373      |750.00\n",
    "105  |2014-11-19  |202    |CA     |321      |200.00\n",
    "\n",
    "- All rows with same store ID will be in same file.\n",
    "\n",
    "- All rows will be in one of 4 files.\n",
    "\n",
    "- Each file represents a random sample of the data.\n",
    "\n",
    "Why Bucketing\n",
    "-------------\n",
    "\n",
    "Why do this?\n",
    "\n",
    "- The data is relatively evenly spread out between files.\n",
    "\n",
    "- Buckets are useful for map-side joins across large tables.\n",
    "\n",
    "- Sampling is faster.\n",
    "\n",
    "When To Bucket\n",
    "--------------\n",
    "\n",
    "When should you bucket?\n",
    "\n",
    "- If you are planning to join on store bucketing can speed up the\n",
    "  join.\n",
    "\n",
    "- If you are running queries on specific stores.\n",
    "\n",
    "- Bucketing reorganizes the data without creating directory\n",
    "  hierarchies, as opposed to partitioning.\n",
    "\n",
    "Bucketing\n",
    "---------\n",
    "\n",
    "### Hive: Create bucketed table\n",
    "\n",
    "```sql\n",
    "-- Create bucketed table.\n",
    "SET hive.enforce.bucketing=true;\n",
    "DROP TABLE IF EXISTS  sales_bucket;\n",
    "CREATE TABLE sales_bucket(\n",
    "  id INT,\n",
    "  sale_date STRING,\n",
    "  store INT,\n",
    "  state STRING,\n",
    "  product INT,\n",
    "  amount DOUBLE\n",
    ")\n",
    "CLUSTERED BY (store) INTO 4 BUCKETS\n",
    "ROW FORMAT DELIMITED \n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE;\n",
    "\n",
    "-- Insert select into bucketed table.\n",
    "FROM sales \n",
    "INSERT INTO TABLE sales_bucket \n",
    "SELECT *;\n",
    "\n",
    "-- Select table to test.\n",
    "SELECT * FROM sales_bucket;\n",
    "```\n",
    "\n",
    "### Bash: Verify data split between 4 files\n",
    "\n",
    "```bash\n",
    "hadoop fs -ls -R /apps/hive/warehouse/sales_bucket\n",
    "```\n",
    "\n",
    "Partitioning and Bucketing\n",
    "==========================\n",
    "\n",
    "Partitioning and Bucketing Overview\n",
    "-----------------------------------\n",
    "\n",
    "You can do both partitioning and bucketing on the same table.\n",
    "\n",
    "- You partition on the main columns that have a few values and that\n",
    "  divide the data.\n",
    "\n",
    "- You bucket on the columns that are likely to be join keys or that\n",
    "  will be used for queries.\n",
    "\n",
    "Partitioning and Bucketing Demo\n",
    "-------------------------------\n",
    "\n",
    "### Hive: Create partitioned bucketed table\n",
    "\n",
    "```sql\n",
    "-- Create partitioned bucketed table.\n",
    "DROP TABLE IF EXISTS  sales_part_bucket;\n",
    "CREATE TABLE sales_part_bucket(\n",
    "  id INT,\n",
    "  sale_date STRING,\n",
    "  store INT,\n",
    "  product INT,\n",
    "  amount DOUBLE\n",
    ")\n",
    "PARTITIONED BY (state STRING)\n",
    "CLUSTERED BY (store) INTO 4 BUCKETS\n",
    "ROW FORMAT DELIMITED \n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE;\n",
    "\n",
    "-- Insert select into partitioned bucketed table.\n",
    "FROM sales \n",
    "INSERT INTO TABLE sales_part_bucket PARTITION(state)\n",
    "SELECT id,sale_date,store,product,amount,state;\n",
    "\n",
    "-- Select table to test.\n",
    "SELECT * FROM sales_part_bucket;\n",
    "```\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "How many files do you expect to see in HDFS?\n",
    "</summary>\n",
    "1. The partition count will equal the number of states which is 3.<br>\n",
    "2. Within each partition there will be 4 buckets.<br>\n",
    "3. So there will be a total of $3 * 4 = 12$ files.<br>\n",
    "</details>\n",
    "\n",
    "Lets check.\n",
    "\n",
    "### Bash: Verify data split between 4 files\n",
    "\n",
    "```bash\n",
    "hadoop fs -ls -R /apps/hive/warehouse/sales_part_bucket\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
