{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install fastavro:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash \n",
    "sudo pip install fastavro\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from cStringIO import StringIO\n",
    "import fastavro\n",
    "import boto\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import boto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data, and use avro schema to map to JSON\n",
    "We must firts acquire data. In this case, we will load a series of .avro files from AWS S3. Note:\n",
    "- in our binaryFiles() call, we use a \"?\" character as a regular expression to indicate either zero or one character.\n",
    "- for access tot the S3 bucket, you need to set your AWS credentials as environment variables\n",
    "\n",
    "In the few lines below, we:\n",
    "- read the files from disk into a JavaPairRDD,\n",
    "- map each binary data value in the RDD to a string using StringIO,\n",
    "- read each string and combines (flatMap) them into a json RDD.\n",
    "\n",
    "**An important note on distributed processing**: Imagine we're dealing with files that are ~500TB put together. We can't process that locally, which is where Spark's distributed framework comes in. By loading the data into an RDD, we ensure we're using Spark's core strength to process these large data sets at scale.\n",
    "\n",
    "**An important note on lazy evaluation in Spark**: As noted during the Spark lecture, it applies _lazy evaluation_, which is to say that for example transformations like map() and flatMap() are only evaluated when their results are explicitly requested through a function like <code>.take()</code> or <code>.collect()</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# connect to AWS\n",
    "s3 = boto.connect_s3(aws_access_key_id =\"AKIAIBZEDBZIIV7PUW5Q\", \n",
    "                     aws_secret_access_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# access relevent bucket for lab\n",
    "dsci = s3.get_bucket(\"dsci\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dsci = s3.get_bucket(\"alexander-graphlab-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_key() takes at least 2 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-1bf16ca68d08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdsci\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: get_key() takes at least 2 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "dsci.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all keys that reference the files in bucket called \"dsci\" \n",
    "# NOTE: for testing purposes, only use data2 file\n",
    "file_keys = dsci.get_all_keys(prefix = \"6007/data/SuperWebAnalytics/new_data/data2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keys = sc.parallelize(file_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Key: dsci,6007/data/SuperWebAnalytics/new_data/data2.avro>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avro_data = keys.map(lambda key: StringIO(key.get_contents_as_string()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I can't open this RDD to expore the contents. Because that is serialized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_data = avro_data.flatMap(fastavro.reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "Before working with a data set, it is useful to explore it a bit and see what we are working with. In this case, our data is based on an avro schema for a graph schema we have worked with before. The data consists of records, each describing either a property of a node, or an edge.\n",
    "\n",
    "Let's use .take() to grab the first few records: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'dataunit': {u'person_property': {u'property': {u'location': {u'city': None, u'state': None, u'country': u'US'}}, u'id': {u'user_id': 9999}}}, u'pedigree': {u'true_as_of_secs': 1438381448}}]\n"
     ]
    }
   ],
   "source": [
    "print json_data.top(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning the data\n",
    "Now that we have the data, we need to divide it into pieces according to the partitioning scheme outlined in the Lab specs. Our data is stored as a json array of objects. We'll take each record and map it to a 2-tuple contaning the datatype and the actual datum. By dynamically generating the partition name, our code will be able to handle any new node properties or edge types that might be added later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datum = json_data.top(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'dataunit': {u'person_property': {u'id': {u'user_id': 9999},\n",
       "   u'property': {u'location': {u'city': None,\n",
       "     u'country': u'US',\n",
       "     u'state': None}}}},\n",
       " u'pedigree': {u'true_as_of_secs': 1438381448}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the map function will automatically pass in the contents of the list\n",
    "# and not the brackets of the list \n",
    "datum[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'person_property': {u'id': {u'user_id': 9999},\n",
       "  u'property': {u'location': {u'city': None,\n",
       "    u'country': u'US',\n",
       "    u'state': None}}}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pulls up the contents of 'dataunit'\n",
    "datum[0]['dataunit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'person_property'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identifies the first key\n",
    "# in this case, the first key is 'person_property'\n",
    "datum[0]['dataunit'].keys()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datatype = datum[0]['dataunit'].keys()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'location'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identifies the name of the actual property under the meta label of 'property'\n",
    "# in this case it's 'location\n",
    "# NOTE: these operations assume that the dataunit is, in fact, atomic!\n",
    "# If that is NOT ATOMIC, I forsee errors!\n",
    "datum[0]['dataunit'][datatype]['property'].keys()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'person_property/location'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, we arrive at the virtical partitioning of the data\n",
    "'/'.join((datatype, datum[0]['dataunit'][datatype]['property'].keys()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datatype' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4ec84e836779>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Output is a tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# (file partition path, original datum)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;34m'/'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataunit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'property'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'datatype' is not defined"
     ]
    }
   ],
   "source": [
    "# Output is a tuple\n",
    "# (file partition path, original datum)\n",
    "'/'.join((datatype, datum[0]['dataunit'][datatype]['property'].keys()[0])), datum[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The else return statement is  there so that the function \n",
    "# can pass through edge dataunits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partition_data(datum):\n",
    "    print datum\n",
    "    datatype = datum['dataunit'].keys()[0]\n",
    "    if datatype.endswith('property'):\n",
    "        return '/'.join((datatype, datum['dataunit'][datatype]['property'].keys()[0])), datum\n",
    "    else:\n",
    "        return datatype, datum # Edge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "partitioned_json = json_data.map(partition_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'page_property/page_views', {u'dataunit': {u'page_property': {u'property': {u'page_views': 3}, u'id': {u'url': u'http://mysite.com/'}}}, u'pedigree': {u'true_as_of_secs': 1438381257}}), (u'person_property/location', {u'dataunit': {u'person_property': {u'property': {u'location': {u'city': None, u'state': None, u'country': u'US'}}, u'id': {u'user_id': 2528}}}, u'pedigree': {u'true_as_of_secs': 1438381257}}), (u'person_property/location', {u'dataunit': {u'person_property': {u'property': {u'location': {u'city': None, u'state': None, u'country': u'US'}}, u'id': {u'cookie': u'FGHIJ'}}}, u'pedigree': {u'true_as_of_secs': 1438381257}})]\n"
     ]
    }
   ],
   "source": [
    "print partitioned_json.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[26] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache data into RAM for quick access\n",
    "# will be performing as many transformations as there are properties + edges \n",
    "partitioned_json.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# appears to be a sanity check \n",
    "# to make sure that only the desired partitions were created\n",
    "partition_names = partitioned_json.map(lambda t: t[0]).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {u'person_property/location': 149929, u'page_property/page_views': 125713, u'page_view': 599271, u'equiv': 125087})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count on each partition \n",
    "partitioned_json.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'page_property/page_views',\n",
       " u'person_property/location',\n",
       " u'page_view',\n",
       " u'equiv']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Â "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: need to gracefully handle when dir/file already exists\n",
    "\n",
    "# This function creates a new folder for each partition name. \n",
    "# For this data set, there are 4 partition names. \n",
    "# NOTE: each folder will have as many files as there are reducers (MapReduce reducers)\n",
    "# Recall that large distributed files are broken up into blocks and each block is passed to a reducer \n",
    "\n",
    "\n",
    "\n",
    "for p in partition_names:\n",
    "    path = \"../SuperWebAnalytics/master/{}\".format(p)\n",
    "    if os.path.exists(path):\n",
    "        print \"{} exists\".format(path)\n",
    "    else:\n",
    "        partitioned_json.filter(lambda t: t[0] == p).values().saveAsPickleFile(path)\n",
    "#         #  line below does avro:\n",
    "#         partitioned_json.filter(lambda t: t[0] == p).values().mapPartitions(avro_writer).saveAsTextFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*_property [error opening dir]\r\n",
      "\r\n",
      "0 directories, 0 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree *_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
